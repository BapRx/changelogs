{"0": {
    "doc": "alertmanager-snmp-notifier",
    "title": "alertmanager-snmp-notifier",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/alertmanager-snmp-notifier",
    
    "relUrl": "/prometheus-community/helm-charts/alertmanager-snmp-notifier"
  },"1": {
    "doc": "alertmanager-snmp-notifier",
    "title": "0.1.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . image: repository: maxwo/snmp-notifier pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" imagePullSecrets: [] nameOverride: \"\" fullnameOverride: \"\" snmpNotifier: # extraArgs allows to pass SNMP notifier configurations, as described on https://github.com/maxwo/snmp_notifier#snmp-notifier-configuration extraArgs: [] # - --alert.severity-label=severity # snmpDestinations is the list of SNMP servers to send the traps to snmpDestinations: - snmp-server:162 # SNMP authentication secrets, that may be instanciated by the chart, or may use an already created secret # snmpCommunity: public # snmpAuthenticationUsername: my_authentication_username # snmpAuthenticationPassword: my_authentication_password # snmpPrivatePassword: my_private_password # snmpCommunitySecret: # name: existingsecret # key: community # snmpAuthenticationUsernameSecret: # name: existingsecret # key: authenticationUsername # snmpAuthenticationPasswordSecret: # name: existingsecret # key: authenticationPassword # snmpPrivatePasswordSecret: # name: existingsecret # key: privatePassword # snmpTemplates allows to customize the description of the traps, and add extra trap fields snmpTemplates: {} # description: | # {{- if .Alerts -}} # {{ len .Alerts }}/{{ len .DeclaredAlerts }} alerts are firing: # {{ range $severity, $alerts := (groupAlertsByLabel .Alerts \"severity\") -}} # Status: {{ $severity }} # {{- range $index, $alert := $alerts }} # - Alert: {{ $alert.Labels.alertname }} # Summary: {{ $alert.Annotations.summary }} # Description: {{ $alert.Annotations.description }} # {{ end }} # {{ end }} # {{ else -}} # Status: OK # {{- end -}} # extraFields: # - subId: 4 # template: | # {{- if .Alerts -}} # Status: NOK # {{- else -}} # Status: OK # {{- end -}} # - subId: 5 # template: | # This is a constant serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 replicaCount: 1 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 9464 ingress: enabled: false className: \"\" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: snmp-notifier.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: snmp-notifier-tls # hosts: # - snmp-notifier.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 80 # targetMemoryUtilizationPercentage: 80 nodeSelector: {} tolerations: [] affinity: {} # Enable this if you're using https://github.com/coreos/prometheus-operator serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed namespace: monitoring # Set how frequently Prometheus should scrape interval: 30s # Set path to cloudwatch-exporter telemtery-path telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator labels: # Set timeout for scrape timeout: 10s # Set of labels to transfer from the Kubernetes Service onto the target targetLabels: [] # MetricRelabelConfigs to apply to samples before ingestion metricRelabelings: [] # Set relabel_configs as per https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config relabelings: [] . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/alertmanager-snmp-notifier#010",
    
    "relUrl": "/prometheus-community/helm-charts/alertmanager-snmp-notifier#010"
  },"2": {
    "doc": "alertmanager",
    "title": "alertmanager",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/alertmanager",
    
    "relUrl": "/prometheus-community/helm-charts/alertmanager"
  },"3": {
    "doc": "alertmanager",
    "title": "0.26.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for alertmanager. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: quay.io/prometheus/alertmanager pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" extraArgs: {} ## Additional Alertmanager Secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # subPath: \"\" # secretName: alertmanager-secret-files # readOnly: true imagePullSecrets: [] nameOverride: \"\" fullnameOverride: \"\" serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: # Sets priorityClassName in alertmanager pod priorityClassName: \"\" podSecurityContext: fsGroup: 65534 dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 securityContext: # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true runAsUser: 65534 runAsNonRoot: true runAsGroup: 65534 additionalPeers: [] ## Additional InitContainers to initialize the pod ## extraInitContainers: [] livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http service: annotations: {} type: ClusterIP port: 9093 clusterPort: 9094 loadBalancerIP: \"\" # Assign ext IP when Service type is LoadBalancer loadBalancerSourceRanges: [] # Only allow access to loadBalancerIP from these IPs # if you want to force a specific nodePort. Must be use with service.type=NodePort # nodePort: ingress: enabled: false className: \"\" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: alertmanager.domain.com paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - alertmanager.domain.com resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 10m # memory: 32Mi nodeSelector: {} tolerations: [] affinity: {} ## Pod anti-affinity can prevent the scheduler from placing Alertmanager replicas on the same node. ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided. ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node. ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured. ## podAntiAffinity: \"\" ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity. ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone ## podAntiAffinityTopologyKey: kubernetes.io/hostname ## Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ topologySpreadConstraints: [] # - maxSkew: 1 # topologyKey: failure-domain.beta.kubernetes.io/zone # whenUnsatisfiable: DoNotSchedule # labelSelector: # matchLabels: # app.kubernetes.io/instance: alertmanager statefulSet: annotations: {} podAnnotations: {} podLabels: {} # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/ podDisruptionBudget: {} # maxUnavailable: 1 # minAvailable: 1 command: [] persistence: enabled: true ## Persistent Volume Storage Class ## If defined, storageClassName: &lt;storageClass&gt; ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. ## # storageClass: \"-\" accessModes: - ReadWriteOnce size: 50Mi config: global: {} # slack_api_url: '' templates: - '/etc/alertmanager/*.tmpl' receivers: - name: default-receiver # slack_configs: # - channel: '@you' # send_resolved: true route: group_wait: 10s group_interval: 5m receiver: default-receiver repeat_interval: 3h ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: ## If false, the configmap-reload container will not be deployed ## enabled: false ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.8.0 pullPolicy: IfNotPresent # containerPort: 9533 ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} templates: {} # alertmanager.tmpl: |- ## Optionally specify extra list of additional volumeMounts extraVolumeMounts: [] # - name: extras # mountPath: /usr/share/extras # readOnly: true ## Optionally specify extra list of additional volumes extraVolumes: [] # - name: extras # emptyDir: {} ## Optionally specify extra environment variables to add to alertmanager container extraEnv: [] # - name: FOO # value: BAR . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/alertmanager#0260",
    
    "relUrl": "/prometheus-community/helm-charts/alertmanager#0260"
  },"4": {
    "doc": "prometheus-community/helm-charts",
    "title": "prometheus-community/helm-charts",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts",
    
    "relUrl": "/prometheus-community/helm-charts"
  },"5": {
    "doc": "vmware-tanzu/helm-charts",
    "title": "vmware-tanzu/helm-charts",
    "content": " ",
    "url": "/changelogs/vmware-tanzu/helm-charts",
    
    "relUrl": "/vmware-tanzu/helm-charts"
  },"6": {
    "doc": "Home",
    "title": "Changelogs",
    "content": "This website periodically generate changelogs from hand picked repositories. You can navigate through these changelogs by using the navigation on left side or by searching directly at the top. ",
    "url": "/changelogs/#changelogs",
    
    "relUrl": "/#changelogs"
  },"7": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/changelogs/",
    
    "relUrl": "/"
  },"8": {
    "doc": "jiralert",
    "title": "jiralert",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/jiralert",
    
    "relUrl": "/prometheus-community/helm-charts/jiralert"
  },"9": {
    "doc": "jiralert",
    "title": "1.2.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . global: imagePullSecrets: [] # -- String to partially override jiralert.fullname template (will maintain the release name) nameOverride: \"\" # -- Override the namespace namespaceOverride: \"\" # -- String to fully override amazon-eks-pod-identity.fullname template fullnameOverride: \"\" image: pullPolicy: IfNotPresent # -- jiralert image registry repository: quay.io/jiralert/jiralert-linux-amd64 # -- jiralert image tag (immutable tags are recommended). # @default -- `.Chart.AppVersion` tag: \"\" extraArgs: - -log.level=debug # Number of pod replicas replicaCount: 1 # Container resources resources: # -- The resources limits for the jiralert container ## Example: ## limits: ## cpu: 100m ## memory: 128Mi limits: cpu: 200m memory: 128Mi # -- The requested resources for the jiralert container ## Examples: ## requests: ## cpu: 100m ## memory: 128Mi requests: cpu: 100m memory: 64Mi # -- Annotations for jiralert deployment annotations: {} # -- Annotations for jiralert pods podAnnotations: {} # -- jiralert pods' Security Context. podSecurityContext: {} # fsGroup: 2000 # Container security context securityContext: runAsUser: 1001 runAsGroup: 1001 runAsNonRoot: true readOnlyRootFilesystem: true livenessProbe: httpGet: path: /healthz port: http readinessProbe: httpGet: path: /healthz port: http serviceAccount: # -- Enable creation of ServiceAccount for nginx pod create: true # -- The name of the ServiceAccount to use. # @default -- A name is generated using the `jiralert.fullname` template name: '' # -- Annotations for service account. Evaluated as a template. annotations: {} # -- Labels for service account. Evaluated as a template. labels: {} existingConfigSecret: ~ ## Pass the jiralert configuration directives through Helm's templating ## engine. If the jiralert configuration contains Alertmanager templates, ## they'll need to be properly escaped so that they are not interpreted by ## Helm ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function tplConfig: false config: # File containing template definitions. Required. template: jiralert.tmpl # Example: https://github.com/prometheus-community/jiralert/blob/master/examples/jiralert.yml defaults: # API access fields. api_url: \"https://example.atlassian.net\" # user: {{ .config.jiraUser }} # password: '{{ .config.jiraToken }}' summary: '{{`{{ template \"jira.summary\" . }}`}}' description: '{{`{{ template \"jira.description\" . }}`}}' issue_type: Bug reopen_state: \"To Do\" reopen_duration: 0h # Receiver definitions. At least one must be defined. receivers: [] # - name: 'default' # # JIRA project to create the issue in. Required. # project: # # Copy all Prometheus labels into separate JIRA labels. Optional (default: false). # add_group_labels: false # fields: # # Set the Environment field # \"customfield_10078\": { \"value\": { { .config.receiver.environment | quote } } } # # Set the Organization field # \"customfield_10002\": [ { { .config.receiver.organizationID } } ] # Alternative to config. Must be used with tplConfig=true. Could be used for complex configurations. configString: \"\" # -- Affinity for pod assignment affinity: {} # -- Node labels for pod assignment. Evaluated as a template. nodeSelector: {} # -- Tolerations for pod assignment. Evaluated as a template. tolerations: [] ingress: enabled: false className: \"\" labels: {} annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local issueTemplate: | {{`{{ define \"jira.summary\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join \" \" }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join \" \" }}{{ end }}){{ end }}{{ end }} {{ define \"jira.description\" }}{{ range .Alerts.Firing }}Labels: {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }} Annotations: {{ range .Annotations.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }} Source: {{ .GeneratorURL }} {{ end }}{{ end }}`}} # Enable this if you're using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: false # namespace: monitoring # Fallback to the prometheus default unless specified # interval: 10s ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. # scheme: \"\" ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig # tlsConfig: {} # bearerTokenFile: # Fallback to the prometheus default unless specified # scrapeTimeout: 30s ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec additionalLabels: {} # Retain the job and instance labels of the metrics pushed to the Pushgateway # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape) honorLabels: true ## Metric relabel configs to apply to samples before ingestion. ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs) metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## Relabel configs to apply to samples before ingestion. ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/jiralert#120",
    
    "relUrl": "/prometheus-community/helm-charts/jiralert#120"
  },"10": {
    "doc": "kube-prometheus-stack",
    "title": "kube-prometheus-stack",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/kube-prometheus-stack",
    
    "relUrl": "/prometheus-community/helm-charts/kube-prometheus-stack"
  },"11": {
    "doc": "kube-prometheus-stack",
    "title": "45.5.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for kube-prometheus-stack. # This is a YAML-formatted file. # Declare variables to be passed into your templates. ## Provide a name in place of kube-prometheus-stack for `app:` labels ## nameOverride: \"\" ## Override the deployment namespace ## namespaceOverride: \"\" ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6 ## kubeTargetVersionOverride: \"\" ## Allow kubeVersion to be overridden while creating the ingress ## kubeVersionOverride: \"\" ## Provide a name to substitute for the full names of resources ## fullnameOverride: \"\" ## Labels to apply to all resources ## commonLabels: {} # scmhash: abc123 # myLabel: aakkmd ## Create default rules for monitoring the cluster ## defaultRules: create: true rules: alertmanager: true etcd: true configReloaders: true general: true k8s: true kubeApiserverAvailability: true kubeApiserverBurnrate: true kubeApiserverHistogram: true kubeApiserverSlos: true kubeControllerManager: true kubelet: true kubeProxy: true kubePrometheusGeneral: true kubePrometheusNodeRecording: true kubernetesApps: true kubernetesResources: true kubernetesStorage: true kubernetesSystem: true kubeSchedulerAlerting: true kubeSchedulerRecording: true kubeStateMetrics: true network: true node: true nodeExporterAlerting: true nodeExporterRecording: true prometheus: true prometheusOperator: true ## Reduce app namespace alert scope appNamespacesTarget: \".*\" ## Labels for default rules labels: {} ## Annotations for default rules annotations: {} ## Additional labels for PrometheusRule alerts additionalRuleLabels: {} ## Additional annotations for PrometheusRule alerts additionalRuleAnnotations: {} ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules. runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\" ## Disabled PrometheusRule alerts disabled: {} # KubeAPIDown: true # NodeRAIDDegraded: true ## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster. ## # additionalPrometheusRules: [] # - name: my-rule-file # groups: # - name: my_group # rules: # - record: my_record # expr: 100 * my_record ## Provide custom recording or alerting rules to be deployed into the cluster. ## additionalPrometheusRulesMap: {} # rule-name: # groups: # - name: my_group # rules: # - record: my_record # expr: 100 * my_record ## global: rbac: create: true ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles createAggregateClusterRoles: false pspEnabled: false pspAnnotations: {} ## Specify pod annotations ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl ## # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*' # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...) ## imageRegistry: \"\" ## Reference to one or more secrets to be used when pulling images ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## imagePullSecrets: [] # - name: \"image-pull-secret\" # or # - \"image-pull-secret\" ## Configuration for alertmanager ## ref: https://prometheus.io/docs/alerting/alertmanager/ ## alertmanager: ## Deploy alertmanager ## enabled: true ## Annotations for Alertmanager ## annotations: {} ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2 ## apiVersion: v2 ## Service account for Alertmanager to use. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ ## serviceAccount: create: true name: \"\" annotations: {} ## Configure pod disruption budgets for Alertmanager ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget ## This configuration is immutable once created and will require the PDB to be deleted to be changed ## https://github.com/kubernetes/kubernetes/issues/45398 ## podDisruptionBudget: enabled: false minAvailable: 1 maxUnavailable: \"\" ## Alertmanager configuration directives ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file ## https://prometheus.io/webtools/alerting/routing-tree-editor/ ## config: global: resolve_timeout: 5m inhibit_rules: - source_matchers: - 'severity = critical' target_matchers: - 'severity =~ warning|info' equal: - 'namespace' - 'alertname' - source_matchers: - 'severity = warning' target_matchers: - 'severity = info' equal: - 'namespace' - 'alertname' - source_matchers: - 'alertname = InfoInhibitor' target_matchers: - 'severity = info' equal: - 'namespace' route: group_by: ['namespace'] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: 'null' routes: - receiver: 'null' matchers: - alertname =~ \"InfoInhibitor|Watchdog\" receivers: - name: 'null' templates: - '/etc/alertmanager/config/*.tmpl' ## Alertmanager configuration directives (as string type, preferred over the config hash map) ## stringConfig will be used only, if tplConfig is true ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file ## https://prometheus.io/webtools/alerting/routing-tree-editor/ ## stringConfig: \"\" ## Pass the Alertmanager configuration directives through Helm's templating ## engine. If the Alertmanager configuration contains Alertmanager templates, ## they'll need to be properly escaped so that they are not interpreted by ## Helm ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function ## https://prometheus.io/docs/alerting/configuration/#tmpl_string ## https://prometheus.io/docs/alerting/notifications/ ## https://prometheus.io/docs/alerting/notification_examples/ tplConfig: false ## Alertmanager template files to format alerts ## By default, templateFiles are placed in /etc/alertmanager/config/ and if ## they have a .tmpl file suffix will be loaded. See config.templates above ## to change, add other suffixes. If adding other suffixes, be sure to update ## config.templates above to include those suffixes. ## ref: https://prometheus.io/docs/alerting/notifications/ ## https://prometheus.io/docs/alerting/notification_examples/ ## templateFiles: {} # ## An example template: # template_1.tmpl: |- # {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }} # # {{ define \"slack.myorg.text\" }} # {{- $root := . -}} # {{ range .Alerts }} # *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}` # *Cluster:* {{ template \"cluster\" $root }} # *Description:* {{ .Annotations.description }} # *Graph:* &lt;{{ .GeneratorURL }}|:chart_with_upwards_trend:&gt; # *Runbook:* &lt;{{ .Annotations.runbook }}|:spiral_note_pad:&gt; # *Details:* # {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}` # {{ end }} # {{ end }} # {{ end }} ingress: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} ## Redirect ingress to an additional defined port on the service # servicePort: 8081 ## Hosts must be provided if Ingress is enabled. ## hosts: [] # - alertmanager.domain.com ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## TLS configuration for Alertmanager Ingress ## Secret must be manually created in the namespace ## tls: [] # - secretName: alertmanager-general-tls # hosts: # - alertmanager.example.com ## Configuration for Alertmanager secret ## secret: annotations: {} ## Configuration for creating an Ingress that will map to each Alertmanager replica service ## alertmanager.servicePerReplica must be enabled ## ingressPerReplica: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} ## Final form of the hostname for each per replica ingress is ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }} ## ## Prefix for the per replica ingress that will have `-$replicaNumber` ## appended to the end hostPrefix: \"\" ## Domain that will be used for the per replica ingress hostDomain: \"\" ## Paths to use for ingress rules ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## Secret name containing the TLS certificate for alertmanager per replica ingress ## Secret must be manually created in the namespace tlsSecretName: \"\" ## Separated secret for each per replica Ingress. Can be used together with cert-manager ## tlsSecretPerReplica: enabled: false ## Final form of the secret for each per replica ingress is ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }} ## prefix: \"alertmanager\" ## Configuration for Alertmanager service ## service: annotations: {} labels: {} clusterIP: \"\" ## Port for Alertmanager Service to listen on ## port: 9093 ## To be used with a proxy extraContainer port ## targetPort: 9093 ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30903 ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## ## Additional ports to open for Alertmanager service additionalPorts: [] # additionalPorts: # - name: authenticated # port: 8081 # targetPort: 8081 externalIPs: [] loadBalancerIP: \"\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## Configuration for creating a separate Service for each statefulset Alertmanager replica ## servicePerReplica: enabled: false annotations: {} ## Port for Alertmanager Service per replica to listen on ## port: 9093 ## To be used with a proxy extraContainer port targetPort: 9093 ## Port to expose on each node ## Only used if servicePerReplica.type is 'NodePort' ## nodePort: 30904 ## Loadbalancer source IP ranges ## Only used if servicePerReplica.type is \"LoadBalancer\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## If true, create a serviceMonitor for alertmanager ## serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" selfMonitor: true ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. scheme: \"\" ## enableHttp2: Whether to enable HTTP2. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint enableHttp2: true ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig tlsConfig: {} bearerTokenFile: ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Settings affecting alertmanagerSpec ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec ## alertmanagerSpec: ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata ## Metadata Labels and Annotations gets propagated to the Alertmanager pods. ## podMetadata: {} ## Image of Alertmanager ## image: registry: quay.io repository: prometheus/alertmanager tag: v0.25.0 sha: \"\" ## If true then the user will be responsible to provide a secret with alertmanager configuration ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used ## useExistingSecret: false ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/. ## secrets: [] ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods. ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/. ## configMaps: [] ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config. ## # configSecret: ## WebTLSConfig defines the TLS parameters for HTTPS ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec web: {} ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with. ## alertmanagerConfigSelector: {} ## Example which selects all alertmanagerConfig resources ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\" # alertmanagerConfigSelector: # matchExpressions: # - key: alertconfig # operator: In # values: # - example-config # - example-config-2 # ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\" # alertmanagerConfigSelector: # matchLabels: # role: example-config ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace. ## alertmanagerConfigNamespaceSelector: {} ## Example which selects all namespaces ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\" # alertmanagerConfigNamespaceSelector: # matchExpressions: # - key: alertmanagerconfig # operator: In # values: # - example-namespace # - example-namespace-2 ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\" # alertmanagerConfigNamespaceSelector: # matchLabels: # alertmanagerconfig: enabled ## AlermanagerConfig to be used as top level configuration ## alertmanagerConfiguration: {} ## Example with select a global alertmanagerconfig # alertmanagerConfiguration: # name: global-alertmanager-Configuration ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg: ## alertmanagerConfigMatcherStrategy: {} ## Example with use OnNamespace strategy # alertmanagerConfigMatcherStrategy: # type: OnNamespace ## Define Log Format # Use logfmt (default) or json logging logFormat: logfmt ## Log level for Alertmanager to be configured with. ## logLevel: info ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the ## running cluster equal to the expected size. replicas: 1 ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours). ## retention: 120h ## Storage is the definition of how storage will be used by the Alertmanager instances. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md ## storage: {} # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\"ReadWriteOnce\"] # resources: # requests: # storage: 50Gi # selector: {} ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string false ## externalUrl: ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true, ## but the server serves requests under a different route prefix. For example for use with kubectl proxy. ## routePrefix: / ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions. ## paused: false ## Define which Nodes the Pods are scheduled on. ## ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Define resources requests and limits for single Pods. ## ref: https://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # requests: # memory: 400Mi ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node. ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided. ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node. ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured. ## podAntiAffinity: \"\" ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity. ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone ## podAntiAffinityTopologyKey: kubernetes.io/hostname ## Assign custom affinity rules to the alertmanager instance ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## affinity: {} # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: kubernetes.io/e2e-az-name # operator: In # values: # - e2e-az1 # - e2e-az2 ## If specified, the pod's tolerations. ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] # - key: \"key\" # operator: \"Equal\" # value: \"value\" # effect: \"NoSchedule\" ## If specified, the pod's topology spread constraints. ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ ## topologySpreadConstraints: [] # - maxSkew: 1 # topologyKey: topology.kubernetes.io/zone # whenUnsatisfiable: DoNotSchedule # labelSelector: # matchLabels: # app: alertmanager ## SecurityContext holds pod-level security attributes and common container settings. ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext false ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ ## securityContext: runAsGroup: 2000 runAsNonRoot: true runAsUser: 1000 fsGroup: 2000 ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP. ## Note this is only for the Alertmanager UI, not the gossip communication. ## listenLocal: false ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod. ## containers: [] # containers: # - name: oauth-proxy # image: quay.io/oauth2-proxy/oauth2-proxy:v7.3.0 # args: # - --upstream=http://127.0.0.1:9093 # - --http-address=0.0.0.0:8081 # - ... # ports: # - containerPort: 8081 # name: oauth-proxy # protocol: TCP # resources: {} # Additional volumes on the output StatefulSet definition. volumes: [] # Additional VolumeMounts on the output StatefulSet definition. volumeMounts: [] ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes ## (permissions, dir tree) on mounted volumes before starting prometheus initContainers: [] ## Priority class assigned to the Pods ## priorityClassName: \"\" ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster. ## additionalPeers: [] ## PortName to use for Alert Manager. ## portName: \"http-web\" ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918 ## clusterAdvertiseAddress: false ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica. ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each. forceEnableClusterMode: false ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready). minReadySeconds: 0 ## ExtraSecret can be used to store various data in an extra secret ## (use it for example to store hashed basic auth credentials) extraSecret: ## if not set, name will be auto generated # name: \"\" annotations: {} data: {} # auth: | # foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0 # someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c. ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml ## grafana: enabled: true namespaceOverride: \"\" ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled ## forceDeployDatasources: false ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled ## forceDeployDashboards: false ## Deploy default dashboards ## defaultDashboardsEnabled: true ## Timezone for the default dashboards ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg ## defaultDashboardsTimezone: utc adminPassword: prom-operator rbac: ## If true, Grafana PSPs will be created ## pspEnabled: false ingress: ## If true, Grafana Ingress will be created ## enabled: false ## IngressClassName for Grafana Ingress. ## Should be provided if Ingress is enable. ## # ingressClassName: nginx ## Annotations for Grafana Ingress ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" ## Labels to be added to the Ingress ## labels: {} ## Hostnames. ## Must be provided if Ingress is enable. ## # hosts: # - grafana.domain.com hosts: [] ## Path for grafana ingress path: / ## TLS configuration for grafana Ingress ## Secret must be manually created in the namespace ## tls: [] # - secretName: grafana-general-tls # hosts: # - grafana.example.com sidecar: dashboards: enabled: true label: grafana_dashboard labelValue: \"1\" ## Annotations for Grafana dashboard configmaps ## annotations: {} multicluster: global: enabled: false etcd: enabled: false provider: allowUiUpdates: false datasources: enabled: true defaultDatasourceEnabled: true isDefaultDatasource: true uid: prometheus ## URL of prometheus datasource ## # url: http://prometheus-stack-prometheus:9090/ # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default # defaultDatasourceScrapeInterval: 15s ## Annotations for Grafana datasource configmaps ## annotations: {} ## Create datasource for each Pod of Prometheus StatefulSet; ## this uses headless service `prometheus-operated` which is ## created by Prometheus Operator ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286 createPrometheusReplicasDatasources: false label: grafana_datasource labelValue: \"1\" ## Field with internal link pointing to existing data source in Grafana. ## Can be provisioned via additionalDataSources exemplarTraceIdDestinations: {} # datasourceUid: Jaeger # traceIdLabelName: trace_id extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /etc/grafana/ssl/ # configMap: certs-configmap # readOnly: true deleteDatasources: [] # - name: example-datasource # orgId: 1 ## Configure additional grafana datasources (passed through tpl) ## ref: http://docs.grafana.org/administration/provisioning/#datasources additionalDataSources: [] # - name: prometheus-sample # access: proxy # basicAuth: true # basicAuthPassword: pass # basicAuthUser: daco # editable: false # jsonData: # tlsSkipVerify: true # orgId: 1 # type: prometheus # url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090 # version: 1 ## Passed to grafana subchart and used by servicemonitor below ## service: portName: http-web serviceMonitor: # If true, a ServiceMonitor CRD is created for a prometheus operator # https://github.com/coreos/prometheus-operator # enabled: true # Path to use for scraping metrics. Might be different if server.root_url is set # in grafana.ini path: \"/metrics\" # namespace: monitoring (defaults to use the namespace this chart is deployed to) # labels for the ServiceMonitor labels: {} # Scrape interval. If not set, the Prometheus default scrape interval is used. # interval: \"\" scheme: http tlsConfig: {} scrapeTimeout: 30s ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Component scraping the kube api server ## kubeApiServer: enabled: true tlsConfig: serverName: kubernetes insecureSkipVerify: false serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" jobLabel: component selector: matchLabels: component: apiserver provider: kubernetes ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: # Drop excessively noisy apiserver buckets. - action: drop regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50) sourceLabels: - __name__ - le # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: # - __meta_kubernetes_namespace # - __meta_kubernetes_service_name # - __meta_kubernetes_endpoint_port_name # action: keep # regex: default;kubernetes;https # - targetLabel: __address__ # replacement: kubernetes.default.svc:443 ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping the kubelet and kubelet-hosted cAdvisor ## kubelet: enabled: true namespace: kube-system serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## Enable scraping the kubelet over https. For requirements to enable this see ## https://github.com/prometheus-operator/prometheus-operator/issues/926 ## https: true ## Enable scraping /metrics/cadvisor from kubelet's service ## cAdvisor: true ## Enable scraping /metrics/probes from kubelet's service ## probes: true ## Enable scraping /metrics/resource from kubelet's service ## This is disabled by default because container metrics are already exposed by cAdvisor ## resource: false # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource resourcePath: \"/metrics/resource/v1alpha1\" ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## cAdvisorMetricRelabelings: # Drop less useful container CPU metrics. - sourceLabels: [__name__] action: drop regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)' # Drop less useful container / always zero filesystem metrics. - sourceLabels: [__name__] action: drop regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)' # Drop less useful / always zero container memory metrics. - sourceLabels: [__name__] action: drop regex: 'container_memory_(mapped_file|swap)' # Drop less useful container process metrics. - sourceLabels: [__name__] action: drop regex: 'container_(file_descriptors|tasks_state|threads_max)' # Drop container spec metrics that overlap with kube-state-metrics. - sourceLabels: [__name__] action: drop regex: 'container_spec.*' # Drop cgroup metrics with no pod. - sourceLabels: [id, pod] action: drop regex: '.+;' # - sourceLabels: [__name__, image] # separator: ; # regex: container_([a-z_]+); # replacement: $1 # action: drop # - sourceLabels: [__name__] # separator: ; # regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s) # replacement: $1 # action: drop ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## probesMetricRelabelings: [] # - sourceLabels: [__name__, image] # separator: ; # regex: container_([a-z_]+); # replacement: $1 # action: drop # - sourceLabels: [__name__] # separator: ; # regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s) # replacement: $1 # action: drop ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## ## metrics_path is required to match upstream rules and charts cAdvisorRelabelings: - action: replace sourceLabels: [__metrics_path__] targetLabel: metrics_path # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## probesRelabelings: - action: replace sourceLabels: [__metrics_path__] targetLabel: metrics_path # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## resourceRelabelings: - action: replace sourceLabels: [__metrics_path__] targetLabel: metrics_path # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - sourceLabels: [__name__, image] # separator: ; # regex: container_([a-z_]+); # replacement: $1 # action: drop # - sourceLabels: [__name__] # separator: ; # regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s) # replacement: $1 # action: drop ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## ## metrics_path is required to match upstream rules and charts relabelings: - action: replace sourceLabels: [__metrics_path__] targetLabel: metrics_path # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping the kube controller manager ## kubeControllerManager: enabled: true ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on ## endpoints: [] # - 10.141.4.22 # - 10.141.4.23 # - 10.141.4.24 ## If using kubeControllerManager.endpoints only the port and targetPort are used ## service: enabled: true ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change ## of default port in Kubernetes 1.22. ## port: null targetPort: null # selector: # component: kube-controller-manager serviceMonitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## Enable scraping kube-controller-manager over https. ## Requires proper certs (not self-signed) and delegated authentication/authorization checks. ## If null or unset, the value is determined dynamically based on target Kubernetes version. ## https: null # Skip TLS certificate validation when scraping insecureSkipVerify: null # Name of the server to use when validating TLS certificate serverName: null ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping coreDns. Use either this or kubeDns ## coreDns: enabled: true service: port: 9153 targetPort: 9153 # selector: # k8s-app: kube-dns serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping kubeDns. Use either this or coreDns ## kubeDns: enabled: false service: dnsmasq: port: 10054 targetPort: 10054 skydns: port: 10055 targetPort: 10055 # selector: # k8s-app: kube-dns serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## dnsmasqMetricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## dnsmasqRelabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping etcd ## kubeEtcd: enabled: true ## If your etcd is not deployed as a pod, specify IPs it can be found on ## endpoints: [] # - 10.141.4.22 # - 10.141.4.23 # - 10.141.4.24 ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used ## service: enabled: true port: 2381 targetPort: 2381 # selector: # component: etcd ## Configure secure access to the etcd cluster by loading a secret into prometheus and ## specifying security configuration below. For example, with a secret named etcd-client-cert ## ## serviceMonitor: ## scheme: https ## insecureSkipVerify: false ## serverName: localhost ## caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca ## certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client ## keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key ## serviceMonitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" scheme: http insecureSkipVerify: false serverName: \"\" caFile: \"\" certFile: \"\" keyFile: \"\" ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping kube scheduler ## kubeScheduler: enabled: true ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on ## endpoints: [] # - 10.141.4.22 # - 10.141.4.23 # - 10.141.4.24 ## If using kubeScheduler.endpoints only the port and targetPort are used ## service: enabled: true ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change ## of default port in Kubernetes 1.23. ## port: null targetPort: null # selector: # component: kube-scheduler serviceMonitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## Enable scraping kube-scheduler over https. ## Requires proper certs (not self-signed) and delegated authentication/authorization checks. ## If null or unset, the value is determined dynamically based on target Kubernetes version. ## https: null ## Skip TLS certificate validation when scraping insecureSkipVerify: null ## Name of the server to use when validating TLS certificate serverName: null ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping kube proxy ## kubeProxy: enabled: true ## If your kube proxy is not deployed as a pod, specify IPs it can be found on ## endpoints: [] # - 10.141.4.22 # - 10.141.4.23 # - 10.141.4.24 service: enabled: true port: 10249 targetPort: 10249 # selector: # k8s-app: kube-proxy serviceMonitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## Enable scraping kube-proxy over https. ## Requires proper certs (not self-signed) and delegated authentication/authorization checks ## https: false ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## Additional labels ## additionalLabels: {} # foo: bar ## Component scraping kube state metrics ## kubeStateMetrics: enabled: true ## Configuration for kube-state-metrics subchart ## kube-state-metrics: namespaceOverride: \"\" rbac: create: true releaseLabel: true prometheus: monitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used. ## scrapeTimeout: \"\" ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" # Keep labels from scraped data, overriding server-side labels ## honorLabels: true ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace selfMonitor: enabled: false ## Deploy node exporter as a daemonset to all nodes ## nodeExporter: enabled: true ## Configuration for prometheus-node-exporter subchart ## prometheus-node-exporter: namespaceOverride: \"\" podLabels: ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards ## jobLabel: node-exporter releaseLabel: true extraArgs: - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/) - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$ service: portName: http-metrics prometheus: monitor: enabled: true jobLabel: jobLabel ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used. ## scrapeTimeout: \"\" ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - sourceLabels: [__name__] # separator: ; # regex: ^node_mountstats_nfs_(event|operations|transport)_.+ # replacement: $1 # action: drop ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace rbac: ## If true, create PSPs for node-exporter ## pspEnabled: false ## Manages Prometheus and Alertmanager components ## prometheusOperator: enabled: true ## Prometheus-Operator v0.39.0 and later support TLS natively. ## tls: enabled: true # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants tlsMinVersion: VersionTLS13 # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules. internalPort: 10250 ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted ## rules from making their way into prometheus and potentially preventing the container from starting admissionWebhooks: failurePolicy: ## The default timeoutSeconds is 10 and the maximum value is 30. timeoutSeconds: 10 enabled: true ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate. ## If unspecified, system trust roots on the apiserver are used. caBundle: \"\" ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data. ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own ## certs ahead of time if you wish. ## annotations: {} # argocd.argoproj.io/hook: PreSync # argocd.argoproj.io/hook-delete-policy: HookSucceeded patch: enabled: true image: registry: registry.k8s.io repository: ingress-nginx/kube-webhook-certgen tag: v20221220-controller-v1.5.1-58-g787ea74b6 sha: \"\" pullPolicy: IfNotPresent resources: {} ## Provide a priority class name to the webhook patching job ## priorityClassName: \"\" annotations: {} # argocd.argoproj.io/hook: PreSync # argocd.argoproj.io/hook-delete-policy: HookSucceeded podAnnotations: {} nodeSelector: {} affinity: {} tolerations: [] ## SecurityContext holds pod-level security attributes and common container settings. ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext false ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ ## securityContext: runAsGroup: 2000 runAsNonRoot: true runAsUser: 2000 # Security context for create job container createSecretJob: securityContext: {} # Security context for patch job container patchWebhookJob: securityContext: {} # Use certmanager to generate webhook certs certManager: enabled: false # self-signed root certificate rootCert: duration: \"\" # default to be 5y admissionCert: duration: \"\" # default to be 1y # issuerRef: # name: \"issuer\" # kind: \"ClusterIssuer\" ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list). ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration ## namespaces: {} # releaseNamespace: true # additional: # - kube-system ## Namespaces not to scope the interaction of the Prometheus Operator (deny list). ## denyNamespaces: [] ## Filter namespaces to look for prometheus-operator custom resources ## alertmanagerInstanceNamespaces: [] alertmanagerConfigNamespaces: [] prometheusInstanceNamespaces: [] thanosRulerInstanceNamespaces: [] ## The clusterDomain value will be added to the cluster.peer option of the alertmanager. ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value) ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094 ## # clusterDomain: \"cluster.local\" networkPolicy: ## Enable creation of NetworkPolicy resources. ## enabled: false ## Service account for Alertmanager to use. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ ## serviceAccount: create: true name: \"\" ## Configuration for Prometheus operator service ## service: annotations: {} labels: {} clusterIP: \"\" ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30080 nodePortTls: 30443 ## Additional ports to open for Prometheus service ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services ## additionalPorts: [] ## Loadbalancer IP ## Only use if service.type is \"LoadBalancer\" ## loadBalancerIP: \"\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## NodePort, ClusterIP, LoadBalancer ## type: ClusterIP ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] # ## Labels to add to the operator deployment # ## labels: {} ## Annotations to add to the operator deployment ## annotations: {} ## Labels to add to the operator pod ## podLabels: {} ## Annotations to add to the operator pod ## podAnnotations: {} ## Assign a PriorityClassName to pods if set # priorityClassName: \"\" ## Define Log Format # Use logfmt (default) or json logging # logFormat: logfmt ## Decrease log verbosity to errors only # logLevel: error ## If true, the operator will create and maintain a service for scraping kubelets ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md ## kubeletService: enabled: true namespace: kube-system ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default name: \"\" ## Create a servicemonitor for the operator ## serviceMonitor: ## Labels for ServiceMonitor additionalLabels: {} ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## Scrape timeout. If not set, the Prometheus default scrape timeout is used. scrapeTimeout: \"\" selfMonitor: true ## Metric relabel configs to apply to samples before ingestion. ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] # relabel configs to apply to samples before ingestion. ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Resource limits &amp; requests ## resources: {} # limits: # cpu: 200m # memory: 200Mi # requests: # cpu: 100m # memory: 100Mi # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico), # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working ## hostNetwork: false ## Define which Nodes the Pods are scheduled on. ## ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Tolerations for use with node taints ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] # - key: \"key\" # operator: \"Equal\" # value: \"value\" # effect: \"NoSchedule\" ## Assign custom affinity rules to the prometheus operator ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## affinity: {} # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: kubernetes.io/e2e-az-name # operator: In # values: # - e2e-az1 # - e2e-az2 dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 securityContext: fsGroup: 65534 runAsGroup: 65534 runAsNonRoot: true runAsUser: 65534 ## Container-specific security context configuration ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ ## containerSecurityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true # Enable vertical pod autoscaler support for prometheus-operator verticalPodAutoscaler: enabled: false # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory controlledResources: [] # Define the max allowed resources for the pod maxAllowed: {} # cpu: 200m # memory: 100Mi # Define the min allowed resources for the pod minAllowed: {} # cpu: 200m # memory: 100Mi updatePolicy: # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\". updateMode: Auto ## Prometheus-operator image ## image: registry: quay.io repository: prometheus-operator/prometheus-operator # if not set appVersion field from Chart.yaml is used tag: \"\" sha: \"\" pullPolicy: IfNotPresent ## Prometheus image to use for prometheuses managed by the operator ## # prometheusDefaultBaseImage: prometheus/prometheus ## Prometheus image registry to use for prometheuses managed by the operator ## # prometheusDefaultBaseImageRegistry: quay.io ## Alertmanager image to use for alertmanagers managed by the operator ## # alertmanagerDefaultBaseImage: prometheus/alertmanager ## Alertmanager image registry to use for alertmanagers managed by the operator ## # alertmanagerDefaultBaseImageRegistry: quay.io ## Prometheus-config-reloader ## prometheusConfigReloader: image: registry: quay.io repository: prometheus-operator/prometheus-config-reloader # if not set appVersion field from Chart.yaml is used tag: \"\" sha: \"\" # resource config for prometheusConfigReloader resources: requests: cpu: 200m memory: 50Mi limits: cpu: 200m memory: 50Mi ## Thanos side-car image when configured ## thanosImage: registry: quay.io repository: thanos/thanos tag: v0.30.2 sha: \"\" ## Set a Field Selector to filter watched secrets ## secretFieldSelector: \"\" ## Deploy a Prometheus instance ## prometheus: enabled: true ## Annotations for Prometheus ## annotations: {} ## Service account for Prometheuses to use. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ ## serviceAccount: create: true name: \"\" annotations: {} # Service for thanos service discovery on sidecar # Enable this can make Thanos Query can use # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery # Thanos sidecar on prometheus nodes # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!) thanosService: enabled: false annotations: {} labels: {} ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## gRPC port config portName: grpc port: 10901 targetPort: \"grpc\" ## HTTP port config (for metrics) httpPortName: http httpPort: 10902 targetHttpPort: \"http\" ## ClusterIP to assign # Default is to make this a headless service (\"None\") clusterIP: \"None\" ## Port to expose on each node, if service type is NodePort ## nodePort: 30901 httpNodePort: 30902 # ServiceMonitor to scrape Sidecar metrics # Needs thanosService to be enabled as well thanosServiceMonitor: enabled: false interval: \"\" ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. scheme: \"\" ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig tlsConfig: {} bearerTokenFile: ## Metric relabel configs to apply to samples before ingestion. metricRelabelings: [] ## relabel configs to apply to samples before ingestion. relabelings: [] # Service for external access to sidecar # Enabling this creates a service to expose thanos-sidecar outside the cluster. thanosServiceExternal: enabled: false annotations: {} labels: {} loadBalancerIP: \"\" loadBalancerSourceRanges: [] ## gRPC port config portName: grpc port: 10901 targetPort: \"grpc\" ## HTTP port config (for metrics) httpPortName: http httpPort: 10902 targetHttpPort: \"http\" ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: LoadBalancer ## Port to expose on each node ## nodePort: 30901 httpNodePort: 30902 ## Configuration for Prometheus service ## service: annotations: {} labels: {} clusterIP: \"\" ## Port for Prometheus Service to listen on ## port: 9090 ## To be used with a proxy extraContainer port targetPort: 9090 ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30090 ## Loadbalancer IP ## Only use if service.type is \"LoadBalancer\" loadBalancerIP: \"\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## Additional port to define in the Service additionalPorts: [] # additionalPorts: # - name: authenticated # port: 8081 # targetPort: 8081 ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec publishNotReadyAddresses: false sessionAffinity: \"\" ## Configuration for creating a separate Service for each statefulset Prometheus replica ## servicePerReplica: enabled: false annotations: {} ## Port for Prometheus Service per replica to listen on ## port: 9090 ## To be used with a proxy extraContainer port targetPort: 9090 ## Port to expose on each node ## Only used if servicePerReplica.type is 'NodePort' ## nodePort: 30091 ## Loadbalancer source IP ranges ## Only used if servicePerReplica.type is \"LoadBalancer\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## Configure pod disruption budgets for Prometheus ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget ## This configuration is immutable once created and will require the PDB to be deleted to be changed ## https://github.com/kubernetes/kubernetes/issues/45398 ## podDisruptionBudget: enabled: false minAvailable: 1 maxUnavailable: \"\" # Ingress exposes thanos sidecar outside the cluster thanosIngress: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} servicePort: 10901 ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30901 ## Hosts must be provided if Ingress is enabled. ## hosts: [] # - thanos-gateway.domain.com ## Paths to use for ingress rules ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## TLS configuration for Thanos Ingress ## Secret must be manually created in the namespace ## tls: [] # - secretName: thanos-gateway-tls # hosts: # - thanos-gateway.domain.com # ## ExtraSecret can be used to store various data in an extra secret ## (use it for example to store hashed basic auth credentials) extraSecret: ## if not set, name will be auto generated # name: \"\" annotations: {} data: {} # auth: | # foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0 # someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c. ingress: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} ## Redirect ingress to an additional defined port on the service # servicePort: 8081 ## Hostnames. ## Must be provided if Ingress is enabled. ## # hosts: # - prometheus.domain.com hosts: [] ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## TLS configuration for Prometheus Ingress ## Secret must be manually created in the namespace ## tls: [] # - secretName: prometheus-general-tls # hosts: # - prometheus.example.com ## Configuration for creating an Ingress that will map to each Prometheus replica service ## prometheus.servicePerReplica must be enabled ## ingressPerReplica: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} ## Final form of the hostname for each per replica ingress is ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }} ## ## Prefix for the per replica ingress that will have `-$replicaNumber` ## appended to the end hostPrefix: \"\" ## Domain that will be used for the per replica ingress hostDomain: \"\" ## Paths to use for ingress rules ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## Secret name containing the TLS certificate for Prometheus per replica ingress ## Secret must be manually created in the namespace tlsSecretName: \"\" ## Separated secret for each per replica Ingress. Can be used together with cert-manager ## tlsSecretPerReplica: enabled: false ## Final form of the secret for each per replica ingress is ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }} ## prefix: \"prometheus\" ## Configure additional options for default pod security policy for Prometheus ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ podSecurityPolicy: allowedCapabilities: [] allowedHostPaths: [] volumes: [] serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" selfMonitor: true ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. scheme: \"\" ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig tlsConfig: {} bearerTokenFile: ## Metric relabel configs to apply to samples before ingestion. ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] # relabel configs to apply to samples before ingestion. ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Settings affecting prometheusSpec ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec ## prometheusSpec: ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos ## disableCompaction: false ## APIServerConfig ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig ## apiserverConfig: {} ## Allows setting additional arguments for the Prometheus container ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus additionalArgs: [] ## Interval between consecutive scrapes. ## Defaults to 30s. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183 ## scrapeInterval: \"\" ## Number of seconds to wait for target to respond before erroring ## scrapeTimeout: \"\" ## Interval between consecutive evaluations. ## evaluationInterval: \"\" ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP. ## listenLocal: false ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series. ## This is disabled by default. ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis ## enableAdminAPI: false ## WebTLSConfig defines the TLS parameters for HTTPS ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig web: {} ## Exemplars related settings that are runtime reloadable. ## It requires to enable the exemplar storage feature to be effective. exemplars: \"\" ## Maximum number of exemplars stored in memory for all series. ## If not set, Prometheus uses its default value. ## A value of zero or less than zero disables the storage. # maxSize: 100000 # EnableFeatures API enables access to Prometheus disabled features. # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/ enableFeatures: [] # - exemplar-storage ## Image of Prometheus. ## image: registry: quay.io repository: prometheus/prometheus tag: v2.42.0 sha: \"\" ## Tolerations for use with node taints ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] # - key: \"key\" # operator: \"Equal\" # value: \"value\" # effect: \"NoSchedule\" ## If specified, the pod's topology spread constraints. ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ ## topologySpreadConstraints: [] # - maxSkew: 1 # topologyKey: topology.kubernetes.io/zone # whenUnsatisfiable: DoNotSchedule # labelSelector: # matchLabels: # app: prometheus ## Alertmanagers to which alerts will be sent ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints ## ## Default configuration will connect to the alertmanager deployed as part of this release ## alertingEndpoints: [] # - name: \"\" # namespace: \"\" # port: http # scheme: http # pathPrefix: \"\" # tlsConfig: {} # bearerTokenFile: \"\" # apiVersion: v2 ## External labels to add to any time series or alerts when communicating with external systems ## externalLabels: {} ## enable --web.enable-remote-write-receiver flag on prometheus-server ## enableRemoteWriteReceiver: false ## Name of the external label used to denote replica name ## replicaExternalLabelName: \"\" ## If true, the Operator won't add the external label used to denote replica name ## replicaExternalLabelNameClear: false ## Name of the external label used to denote Prometheus instance name ## prometheusExternalLabelName: \"\" ## If true, the Operator won't add the external label used to denote Prometheus instance name ## prometheusExternalLabelNameClear: false ## External URL at which Prometheus will be reachable. ## externalUrl: \"\" ## Define which Nodes the Pods are scheduled on. ## ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods. ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated ## with the new list of secrets. ## secrets: [] ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods. ## The ConfigMaps are mounted into /etc/prometheus/configmaps/. ## configMaps: [] ## QuerySpec defines the query command line flags when starting Prometheus. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec ## query: {} ## Namespaces to be selected for PrometheusRules discovery. ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage ## ruleNamespaceSelector: {} ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the ## prometheus resource to be created with selectors based on values in the helm deployment, ## which will also match the PrometheusRule resources created ## ruleSelectorNilUsesHelmValues: true ## PrometheusRules to be selected for target discovery. ## If {}, select all PrometheusRules ## ruleSelector: {} ## Example which select all PrometheusRules resources ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\" # ruleSelector: # matchExpressions: # - key: prometheus # operator: In # values: # - example-rules # - example-rules-2 # ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\" # ruleSelector: # matchLabels: # role: example-rules ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the ## prometheus resource to be created with selectors based on values in the helm deployment, ## which will also match the servicemonitors created ## serviceMonitorSelectorNilUsesHelmValues: true ## ServiceMonitors to be selected for target discovery. ## If {}, select all ServiceMonitors ## serviceMonitorSelector: {} ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\" # serviceMonitorSelector: # matchLabels: # prometheus: somelabel ## Namespaces to be selected for ServiceMonitor discovery. ## serviceMonitorNamespaceSelector: {} ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\" # serviceMonitorNamespaceSelector: # matchLabels: # prometheus: somelabel ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the ## prometheus resource to be created with selectors based on values in the helm deployment, ## which will also match the podmonitors created ## podMonitorSelectorNilUsesHelmValues: true ## PodMonitors to be selected for target discovery. ## If {}, select all PodMonitors ## podMonitorSelector: {} ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\" # podMonitorSelector: # matchLabels: # prometheus: somelabel ## Namespaces to be selected for PodMonitor discovery. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage ## podMonitorNamespaceSelector: {} ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the ## prometheus resource to be created with selectors based on values in the helm deployment, ## which will also match the probes created ## probeSelectorNilUsesHelmValues: true ## Probes to be selected for target discovery. ## If {}, select all Probes ## probeSelector: {} ## Example which selects Probes with label \"prometheus\" set to \"somelabel\" # probeSelector: # matchLabels: # prometheus: somelabel ## Namespaces to be selected for Probe discovery. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage ## probeNamespaceSelector: {} ## How long to retain metrics ## retention: 10d ## Maximum size of metrics ## retentionSize: \"\" ## Enable compression of the write-ahead log using Snappy. ## walCompression: true ## If true, the Operator won't process any Prometheus configuration changes ## paused: false ## Number of replicas of each shard to deploy for a Prometheus deployment. ## Number of replicas multiplied by shards is the total number of Pods created. ## replicas: 1 ## EXPERIMENTAL: Number of shards to distribute targets onto. ## Number of replicas multiplied by shards is the total number of Pods created. ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved. ## Increasing shards will not reshard data either but it will continue to be available from the same instances. ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location. ## Sharding is done on the content of the `__address__` target meta-label. ## shards: 1 ## Log level for Prometheus be configured in ## logLevel: info ## Log format for Prometheus be configured in ## logFormat: logfmt ## Prefix used to register routes, overriding externalUrl route. ## Useful for proxies that rewrite URLs. ## routePrefix: / ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata ## Metadata Labels and Annotations gets propagated to the prometheus pods. ## podMetadata: {} # labels: # app: prometheus # k8s-app: prometheus ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node. ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided. ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node. ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured. podAntiAffinity: \"\" ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity. ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone ## podAntiAffinityTopologyKey: kubernetes.io/hostname ## Assign custom affinity rules to the prometheus instance ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## affinity: {} # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: kubernetes.io/e2e-az-name # operator: In # values: # - e2e-az1 # - e2e-az2 ## The remote_read spec configuration for Prometheus. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec remoteRead: [] # - url: http://remote1/read ## additionalRemoteRead is appended to remoteRead additionalRemoteRead: [] ## The remote_write spec configuration for Prometheus. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec remoteWrite: [] # - url: http://remote1/push ## additionalRemoteWrite is appended to remoteWrite additionalRemoteWrite: [] ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature remoteWriteDashboards: false ## Resource limits &amp; requests ## resources: {} # requests: # memory: 400Mi ## Prometheus StorageSpec for persistent data ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md ## storageSpec: {} ## Using PersistentVolumeClaim ## # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\"ReadWriteOnce\"] # resources: # requests: # storage: 50Gi # selector: {} ## Using tmpfs volume ## # emptyDir: # medium: Memory # Additional volumes on the output StatefulSet definition. volumes: [] # Additional VolumeMounts on the output StatefulSet definition. volumeMounts: [] ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form ## as specified in the official Prometheus documentation: ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible ## scrape configs are going to break Prometheus after the upgrade. ## AdditionalScrapeConfigs can be defined as a list or as a templated string. ## ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes ## additionalScrapeConfigs: [] # - job_name: kube-etcd # kubernetes_sd_configs: # - role: node # scheme: https # tls_config: # ca_file: /etc/prometheus/secrets/etcd-client-cert/etcd-ca # cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client # key_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key # relabel_configs: # - action: labelmap # regex: __meta_kubernetes_node_label_(.+) # - source_labels: [__address__] # action: replace # targetLabel: __address__ # regex: ([^:;]+):(\\d+) # replacement: ${1}:2379 # - source_labels: [__meta_kubernetes_node_name] # action: keep # regex: .*mst.* # - source_labels: [__meta_kubernetes_node_name] # action: replace # targetLabel: node # regex: (.*) # replacement: ${1} # metric_relabel_configs: # - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone) # action: labeldrop # ## If scrape config contains a repetitive section, you may want to use a template. ## In the following example, you can see how to define `gce_sd_configs` for multiple zones # additionalScrapeConfigs: | # - job_name: \"node-exporter\" # gce_sd_configs: # {{range $zone := .Values.gcp_zones}} # - project: \"project1\" # zone: \"{{$zone}}\" # port: 9100 # {{end}} # relabel_configs: # ... ## If additional scrape configurations are already deployed in a single secret file you can use this section. ## Expected values are the secret name and key ## Cannot be used with additionalScrapeConfigs additionalScrapeConfigsSecret: {} # enabled: false # name: # key: ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false' additionalPrometheusSecretsAnnotations: {} ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#&lt;alertmanager_config&gt;. ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator. ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade. ## additionalAlertManagerConfigs: [] # - consul_sd_configs: # - server: consul.dev.test:8500 # scheme: http # datacenter: dev # tag_separator: ',' # services: # - metrics-prometheus-alertmanager ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage ## them separately from the helm deployment, you can use this section. ## Expected values are the secret name and key ## Cannot be used with additionalAlertManagerConfigs additionalAlertManagerConfigsSecret: {} # name: # key: # optional: false ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs. ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel ## configs are going to break Prometheus after the upgrade. ## additionalAlertRelabelConfigs: [] # - separator: ; # regex: prometheus_replica # replacement: $1 # action: labeldrop ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage ## them separately from the helm deployment, you can use this section. ## Expected values are the secret name and key ## Cannot be used with additionalAlertRelabelConfigs additionalAlertRelabelConfigsSecret: {} # name: # key: ## SecurityContext holds pod-level security attributes and common container settings. ## This defaults to non root user with uid 1000 and gid 2000. ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md ## securityContext: runAsGroup: 2000 runAsNonRoot: true runAsUser: 1000 fsGroup: 2000 ## Priority class assigned to the Pods ## priorityClassName: \"\" ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment. ## This section is experimental, it may change significantly without deprecation notice in any release. ## This is experimental and may change significantly without backward compatibility in any release. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec ## thanos: {} # secretProviderClass: # provider: gcp # parameters: # secrets: | # - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\" # fileName: \"objstore.yaml\" # objectStorageConfigFile: /var/secrets/object-store.yaml ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod. ## if using proxy extraContainer update targetPort with proxy container port containers: [] # containers: # - name: oauth-proxy # image: quay.io/oauth2-proxy/oauth2-proxy:v7.3.0 # args: # - --upstream=http://127.0.0.1:9093 # - --http-address=0.0.0.0:8081 # - ... # ports: # - containerPort: 8081 # name: oauth-proxy # protocol: TCP # resources: {} ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes ## (permissions, dir tree) on mounted volumes before starting prometheus initContainers: [] ## PortName to use for Prometheus. ## portName: \"http-web\" ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files ## on the file system of the Prometheus container e.g. bearer token files. arbitraryFSAccessThroughSMs: false ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor ## or PodMonitor to true, this overrides honor_labels to false. overrideHonorLabels: false ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs. overrideHonorTimestamps: false ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor ## configs, and they will only discover endpoints within their current namespace. Defaults to false. ignoreNamespaceSelectors: false ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created. ## The label value will always be the namespace of the object that is being created. ## Disabled by default enforcedNamespaceLabel: \"\" ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels. ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair ## Deprecated, use `excludedFromEnforcement` instead prometheusRulesExcludedFromEnforce: [] ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects ## to be excluded from enforcing a namespace label of origin. ## Works only if enforcedNamespaceLabel set to true. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference excludedFromEnforcement: [] ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable, ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions ## of Prometheus &gt;= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/) queryLogFile: false ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead. enforcedSampleLimit: false ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced. enforcedTargetLimit: false ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions ## 2.27.0 and newer. enforcedLabelLimit: false ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions ## 2.27.0 and newer. enforcedLabelNameLengthLimit: false ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus ## versions 2.27.0 and newer. enforcedLabelValueLengthLimit: false ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental ## in Prometheus so it may change in any upcoming release. allowOverlappingBlocks: false ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready). minReadySeconds: 0 # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico), # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it. # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically. hostNetwork: false # HostAlias holds the mapping between IP and hostnames that will be injected # as an entry in the pods hosts file. hostAliases: [] # - ip: 10.10.0.100 # hostnames: # - a1.app.local # - b1.app.local additionalRulesForClusterRole: [] # - apiGroups: [ \"\" ] # resources: # - nodes/proxy # verbs: [ \"get\", \"list\", \"watch\" ] additionalServiceMonitors: [] ## Name of the ServiceMonitor to create ## # - name: \"\" ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from ## the chart ## # additionalLabels: {} ## Service label for use in assembling a job name of the form &lt;label value&gt;-&lt;port&gt; ## If no label is specified, the service name is used. ## # jobLabel: \"\" ## labels to transfer from the kubernetes service to the target ## # targetLabels: [] ## labels to transfer from the kubernetes pods to the target ## # podTargetLabels: [] ## Label selector for services to which this ServiceMonitor applies ## # selector: {} ## Namespaces from which services are selected ## # namespaceSelector: ## Match any namespace ## # any: false ## Explicit list of namespace names to select ## # matchNames: [] ## Endpoints of the selected service to be monitored ## # endpoints: [] ## Name of the endpoint's service port ## Mutually exclusive with targetPort # - port: \"\" ## Name or number of the endpoint's target port ## Mutually exclusive with port # - targetPort: \"\" ## File containing bearer token to be used when scraping targets ## # bearerTokenFile: \"\" ## Interval at which metrics should be scraped ## # interval: 30s ## HTTP path to scrape for metrics ## # path: /metrics ## HTTP scheme to use for scraping ## # scheme: http ## TLS configuration to use when scraping the endpoint ## # tlsConfig: ## Path to the CA file ## # caFile: \"\" ## Path to client certificate file ## # certFile: \"\" ## Skip certificate verification ## # insecureSkipVerify: false ## Path to client key file ## # keyFile: \"\" ## Server name used to verify host name ## # serverName: \"\" additionalPodMonitors: [] ## Name of the PodMonitor to create ## # - name: \"\" ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from ## the chart ## # additionalLabels: {} ## Pod label for use in assembling a job name of the form &lt;label value&gt;-&lt;port&gt; ## If no label is specified, the pod endpoint name is used. ## # jobLabel: \"\" ## Label selector for pods to which this PodMonitor applies ## # selector: {} ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target. ## # podTargetLabels: {} ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## # sampleLimit: 0 ## Namespaces from which pods are selected ## # namespaceSelector: ## Match any namespace ## # any: false ## Explicit list of namespace names to select ## # matchNames: [] ## Endpoints of the selected pods to be monitored ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint ## # podMetricsEndpoints: [] ## Configuration for thanosRuler ## ref: https://thanos.io/tip/components/rule.md/ ## thanosRuler: ## Deploy thanosRuler ## enabled: false ## Annotations for ThanosRuler ## annotations: {} ## Service account for ThanosRuler to use. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ ## serviceAccount: create: true name: \"\" annotations: {} ## Configure pod disruption budgets for ThanosRuler ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget ## This configuration is immutable once created and will require the PDB to be deleted to be changed ## https://github.com/kubernetes/kubernetes/issues/45398 ## podDisruptionBudget: enabled: false minAvailable: 1 maxUnavailable: \"\" ingress: enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx annotations: {} labels: {} ## Hosts must be provided if Ingress is enabled. ## hosts: [] # - thanosruler.domain.com ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix ## paths: [] # - / ## For Kubernetes &gt;= 1.18 you should specify the pathType (determines how Ingress paths should be matched) ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types # pathType: ImplementationSpecific ## TLS configuration for ThanosRuler Ingress ## Secret must be manually created in the namespace ## tls: [] # - secretName: thanosruler-general-tls # hosts: # - thanosruler.example.com ## Configuration for ThanosRuler service ## service: annotations: {} labels: {} clusterIP: \"\" ## Port for ThanosRuler Service to listen on ## port: 10902 ## To be used with a proxy extraContainer port ## targetPort: 10902 ## Port to expose on each node ## Only used if service.type is 'NodePort' ## nodePort: 30905 ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## ## Additional ports to open for ThanosRuler service additionalPorts: [] externalIPs: [] loadBalancerIP: \"\" loadBalancerSourceRanges: [] ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints ## externalTrafficPolicy: Cluster ## Service type ## type: ClusterIP ## If true, create a serviceMonitor for thanosRuler ## serviceMonitor: ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \"\" selfMonitor: true ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. scheme: \"\" ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig tlsConfig: {} bearerTokenFile: ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## RelabelConfigs to apply to samples before scraping ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig ## relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace ## Settings affecting thanosRulerpec ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec ## thanosRulerSpec: ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods. ## podMetadata: {} ## Image of ThanosRuler ## image: registry: quay.io repository: thanos/thanos tag: v0.30.2 sha: \"\" ## Namespaces to be selected for PrometheusRules discovery. ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery. ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage ## ruleNamespaceSelector: {} ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the ## prometheus resource to be created with selectors based on values in the helm deployment, ## which will also match the PrometheusRule resources created ## ruleSelectorNilUsesHelmValues: true ## PrometheusRules to be selected for target discovery. ## If {}, select all PrometheusRules ## ruleSelector: {} ## Example which select all PrometheusRules resources ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\" # ruleSelector: # matchExpressions: # - key: prometheus # operator: In # values: # - example-rules # - example-rules-2 # ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\" # ruleSelector: # matchLabels: # role: example-rules ## Define Log Format # Use logfmt (default) or json logging logFormat: logfmt ## Log level for ThanosRuler to be configured with. ## logLevel: info ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the ## running cluster equal to the expected size. replicas: 1 ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours). ## retention: 24h ## Interval between consecutive evaluations. ## evaluationInterval: \"\" ## Storage is the definition of how storage will be used by the ThanosRuler instances. ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md ## storage: {} # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\"ReadWriteOnce\"] # resources: # requests: # storage: 50Gi # selector: {} ## AlertmanagerConfig define configuration for connecting to alertmanager. ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg. alertmanagersConfig: {} # - api_version: v2 # http_config: # basic_auth: # username: some_user # password: some_pass # static_configs: # - alertmanager.thanos.io # scheme: http # timeout: 10s ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead. ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg. # alertmanagersUrl: ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false ## externalPrefix: ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true, ## but the server serves requests under a different route prefix. For example for use with kubectl proxy. ## routePrefix: / ## ObjectStorageConfig configures object storage in Thanos. Alternative to ## ObjectStorageConfigFile, and lower order priority. objectStorageConfig: {} ## ObjectStorageConfigFile specifies the path of the object storage configuration file. ## When used alongside with ObjectStorageConfig, ObjectStorageConfigFile takes precedence. objectStorageConfigFile: \"\" ## QueryEndpoints defines Thanos querier endpoints from which to query metrics. ## Maps to the --query flag of thanos ruler. queryEndpoints: [] ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored. ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher. queryConfig: {} ## Labels configure the external label pairs to ThanosRuler. A default replica ## label `thanos_ruler_replica` will be always added as a label with the value ## of the pod's name and it will be dropped in the alerts. labels: {} ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions. ## paused: false ## Define which Nodes the Pods are scheduled on. ## ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Define resources requests and limits for single Pods. ## ref: https://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # requests: # memory: 400Mi ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node. ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided. ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node. ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured. ## podAntiAffinity: \"\" ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity. ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone ## podAntiAffinityTopologyKey: kubernetes.io/hostname ## Assign custom affinity rules to the thanosRuler instance ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## affinity: {} # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: kubernetes.io/e2e-az-name # operator: In # values: # - e2e-az1 # - e2e-az2 ## If specified, the pod's tolerations. ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] # - key: \"key\" # operator: \"Equal\" # value: \"value\" # effect: \"NoSchedule\" ## If specified, the pod's topology spread constraints. ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ ## topologySpreadConstraints: [] # - maxSkew: 1 # topologyKey: topology.kubernetes.io/zone # whenUnsatisfiable: DoNotSchedule # labelSelector: # matchLabels: # app: thanos-ruler ## SecurityContext holds pod-level security attributes and common container settings. ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext false ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ ## securityContext: runAsGroup: 2000 runAsNonRoot: true runAsUser: 1000 fsGroup: 2000 ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP. ## Note this is only for the ThanosRuler UI, not the gossip communication. ## listenLocal: false ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod. ## containers: [] # Additional volumes on the output StatefulSet definition. volumes: [] # Additional VolumeMounts on the output StatefulSet definition. volumeMounts: [] ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes ## (permissions, dir tree) on mounted volumes before starting prometheus initContainers: [] ## Priority class assigned to the Pods ## priorityClassName: \"\" ## PortName to use for ThanosRuler. ## portName: \"web\" ## ExtraSecret can be used to store various data in an extra secret ## (use it for example to store hashed basic auth credentials) extraSecret: ## if not set, name will be auto generated # name: \"\" annotations: {} data: {} # auth: | # foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0 # someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c. ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation. ## cleanPrometheusOperatorObjectNames: false . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/kube-prometheus-stack#4550",
    
    "relUrl": "/prometheus-community/helm-charts/kube-prometheus-stack#4550"
  },"12": {
    "doc": "kube-state-metrics",
    "title": "kube-state-metrics",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/kube-state-metrics",
    
    "relUrl": "/prometheus-community/helm-charts/kube-state-metrics"
  },"13": {
    "doc": "kube-state-metrics",
    "title": "4.31.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for kube-state-metrics. prometheusScrape: true image: repository: registry.k8s.io/kube-state-metrics/kube-state-metrics # If unset use v + .Charts.appVersion tag: \"\" sha: \"\" pullPolicy: IfNotPresent imagePullSecrets: [] # - name: \"image-pull-secret\" global: # To help compatibility with other charts which use global.imagePullSecrets. # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style). # global: # imagePullSecrets: # - name: pullSecret1 # - name: pullSecret2 # or # global: # imagePullSecrets: # - pullSecret1 # - pullSecret2 imagePullSecrets: [] # # Allow parent charts to override registry hostname imageRegistry: \"\" # If set to true, this will deploy kube-state-metrics as a StatefulSet and the data # will be automatically sharded across &lt;.Values.replicas&gt; pods using the built-in # autodiscovery feature: https://github.com/kubernetes/kube-state-metrics#automated-sharding # This is an experimental feature and there are no stability guarantees. autosharding: enabled: false replicas: 1 # List of additional cli arguments to configure kube-state-metrics # for example: --enable-gzip-encoding, --log-file, etc. # all the possible args can be found here: https://github.com/kubernetes/kube-state-metrics/blob/master/docs/cli-arguments.md extraArgs: [] service: port: 8080 # Default to clusterIP for backward compatibility type: ClusterIP nodePort: 0 loadBalancerIP: \"\" # Only allow access to the loadBalancerIP from these IPs loadBalancerSourceRanges: [] clusterIP: \"\" annotations: {} ## Additional labels to add to all resources customLabels: {} # app: kube-state-metrics ## Override selector labels selectorOverride: {} ## set to true to add the release label so scraping of the servicemonitor with kube-prometheus-stack works out of the box releaseLabel: false hostNetwork: false rbac: # If true, create &amp; use RBAC resources create: true # Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to it, rolename set here. # useExistingRole: your-existing-role # If set to false - Run without Cluteradmin privs needed - ONLY works if namespace is also set (if useExistingRole is set this name is used as ClusterRole or Role to bind to) useClusterRole: true # Add permissions for CustomResources' apiGroups in Role/ClusterRole. Should be used in conjunction with Custom Resource State Metrics configuration # Example: # - apiGroups: [\"monitoring.coreos.com\"] # resources: [\"prometheuses\"] # verbs: [\"list\", \"watch\"] extraRules: [] # Configure kube-rbac-proxy. When enabled, creates one kube-rbac-proxy container per exposed HTTP endpoint (metrics and telemetry if enabled). # The requests are served through the same service but requests are then HTTPS. kubeRBACProxy: enabled: false image: repository: quay.io/brancz/kube-rbac-proxy tag: v0.14.0 sha: \"\" pullPolicy: IfNotPresent # List of additional cli arguments to configure kube-rbac-prxy # for example: --tls-cipher-suites, --log-file, etc. # all the possible args can be found here: https://github.com/brancz/kube-rbac-proxy#usage extraArgs: [] ## Specify security settings for a Container ## Allows overrides and additional options compared to (Pod) securityContext ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 64Mi # requests: # cpu: 10m # memory: 32Mi serviceAccount: # Specifies whether a ServiceAccount should be created, require rbac true create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: # Reference to one or more secrets to be used when pulling images # ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ imagePullSecrets: [] # ServiceAccount annotations. # Use case: AWS EKS IAM roles for service accounts # ref: https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html annotations: {} prometheus: monitor: enabled: false additionalLabels: {} namespace: \"\" jobLabel: \"\" targetLabels: [] podTargetLabels: [] interval: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 scrapeTimeout: \"\" proxyUrl: \"\" selectorOverride: {} honorLabels: false metricRelabelings: [] relabelings: [] scheme: \"\" tlsConfig: {} ## Specify if a Pod Security Policy for kube-state-metrics must be created ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: enabled: false annotations: {} ## Specify pod annotations ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl ## # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*' # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' additionalVolumes: [] securityContext: enabled: true runAsGroup: 65534 runAsUser: 65534 fsGroup: 65534 ## Specify security settings for a Container ## Allows overrides and additional options compared to (Pod) securityContext ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} ## Node labels for pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: {} ## Affinity settings for pod assignment ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ affinity: {} ## Tolerations for pod assignment ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] ## Topology spread constraints for pod assignment ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ topologySpreadConstraints: [] # Annotations to be added to the deployment/statefulset annotations: {} # Annotations to be added to the pod podAnnotations: {} ## Assign a PriorityClassName to pods if set # priorityClassName: \"\" # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/ podDisruptionBudget: {} # Comma-separated list of metrics to be exposed. # This list comprises of exact metric names and/or regex patterns. # The allowlist and denylist are mutually exclusive. metricAllowlist: [] # Comma-separated list of metrics not to be enabled. # This list comprises of exact metric names and/or regex patterns. # The allowlist and denylist are mutually exclusive. metricDenylist: [] # Comma-separated list of additional Kubernetes label keys that will be used in the resource's # labels metric. By default the metric contains only name and namespace labels. # To include additional labels, provide a list of resource names in their plural form and Kubernetes # label keys you would like to allow for them (Example: '=namespaces=[k8s-label-1,k8s-label-n,...],pods=[app],...)'. # A single '*' can be provided per resource instead to allow any labels, but that has # severe performance implications (Example: '=pods=[*]'). metricLabelsAllowlist: [] # - namespaces=[k8s-label-1,k8s-label-n] # Comma-separated list of Kubernetes annotations keys that will be used in the resource' # labels metric. By default the metric contains only name and namespace labels. # To include additional annotations provide a list of resource names in their plural form and Kubernetes # annotation keys you would like to allow for them (Example: '=namespaces=[kubernetes.io/team,...],pods=[kubernetes.io/team],...)'. # A single '*' can be provided per resource instead to allow any annotations, but that has # severe performance implications (Example: '=pods=[*]'). metricAnnotationsAllowList: [] # - pods=[k8s-annotation-1,k8s-annotation-n] # Available collectors for kube-state-metrics. # By default, all available resources are enabled, comment out to disable. collectors: - certificatesigningrequests - configmaps - cronjobs - daemonsets - deployments - endpoints - horizontalpodautoscalers - ingresses - jobs - leases - limitranges - mutatingwebhookconfigurations - namespaces - networkpolicies - nodes - persistentvolumeclaims - persistentvolumes - poddisruptionbudgets - pods - replicasets - replicationcontrollers - resourcequotas - secrets - services - statefulsets - storageclasses - validatingwebhookconfigurations - volumeattachments # - verticalpodautoscalers # not a default resource, see also: https://github.com/kubernetes/kube-state-metrics#enabling-verticalpodautoscalers # Enabling kubeconfig will pass the --kubeconfig argument to the container kubeconfig: enabled: false # base64 encoded kube-config file secret: # Enable only the release namespace for collecting resources. By default all namespaces are collected. # If releaseNamespace and namespaces are both set a merged list will be collected. releaseNamespace: false # Comma-separated list(string) or yaml list of namespaces to be enabled for collecting resources. By default all namespaces are collected. namespaces: \"\" # Comma-separated list of namespaces not to be enabled. If namespaces and namespaces-denylist are both set, # only namespaces that are excluded in namespaces-denylist will be used. namespacesDenylist: \"\" ## Override the deployment namespace ## namespaceOverride: \"\" resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 64Mi # requests: # cpu: 10m # memory: 32Mi ## Provide a k8s version to define apiGroups for podSecurityPolicy Cluster Role. ## For example: kubeTargetVersionOverride: 1.14.9 ## kubeTargetVersionOverride: \"\" # Enable self metrics configuration for service and Service Monitor # Default values for telemetry configuration can be overridden # If you set telemetryNodePort, you must also set service.type to NodePort selfMonitor: enabled: false # telemetryHost: 0.0.0.0 # telemetryPort: 8081 # telemetryNodePort: 0 # Enable vertical pod autoscaler support for kube-state-metrics verticalPodAutoscaler: enabled: false # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory controlledResources: [] # Define the max allowed resources for the pod maxAllowed: {} # cpu: 200m # memory: 100Mi # Define the min allowed resources for the pod minAllowed: {} # cpu: 200m # memory: 100Mi # updatePolicy: # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\". # updateMode: Auto # volumeMounts are used to add custom volume mounts to deployment. # See example below volumeMounts: [] # - mountPath: /etc/config # name: config-volume # volumes are used to add custom volumes to deployment # See example below volumes: [] # - configMap: # name: cm-for-volume # name: config-volume . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/kube-state-metrics#4310",
    
    "relUrl": "/prometheus-community/helm-charts/kube-state-metrics#4310"
  },"14": {
    "doc": "prom-label-proxy",
    "title": "prom-label-proxy",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prom-label-proxy",
    
    "relUrl": "/prometheus-community/helm-charts/prom-label-proxy"
  },"15": {
    "doc": "prom-label-proxy",
    "title": "0.2.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # -- Number of prom-label-proxy replicas to deploy replicaCount: 1 image: pullPolicy: IfNotPresent # -- prom-label-proxy image registry repository: quay.io/prometheuscommunity/prom-label-proxy # -- prom-label-proxy image tag (immutable tags are recommended). # @default -- `.Chart.AppVersion` tag: \"\" # -- registry secret names as an array imagePullSecrets: [] # -- String to partially override prom-label-proxy.fullname template (will maintain the release name) nameOverride: \"\" # -- Override the namespace namespaceOverride: \"\" # -- String to fully override amazon-eks-pod-identity.fullname template fullnameOverride: \"\" serviceAccount: # -- Enable creation of ServiceAccount for nginx pod create: true # -- The name of the ServiceAccount to use. # @default -- A name is generated using the `prom-label-proxy.fullname` template name: '' # -- Annotations for service account. Evaluated as a template. annotations: {} # -- Annotations for prom-label-proxy pods podAnnotations: {} # -- Labels for prom-label-proxy pods podLabels: {} # -- prom-label-proxy pods' Security Context. podSecurityContext: {} # fsGroup: 2000 securityContext: runAsUser: 65534 runAsGroup: 65534 runAsNonRoot: true readOnlyRootFilesystem: true service: port: 8080 # -- Service type type: ClusterIP # -- Service annotations annotations: {} livenessProbe: httpGet: # -- This is the liveness check endpoint path: /healthz port: http readinessProbe: httpGet: # -- This is the readiness check endpoint path: /healthz port: http resources: # -- The resources limits for the prom-label-proxy container ## Example: ## limits: ## cpu: 100m ## memory: 128Mi limits: cpu: 200m memory: 128Mi # -- The requested resources for the prom-label-proxy container ## Examples: ## requests: ## cpu: 100m ## memory: 128Mi requests: cpu: 100m memory: 64Mi # -- Affinity for pod assignment affinity: {} # -- Node labels for pod assignment. Evaluated as a template. nodeSelector: {} # -- Tolerations for pod assignment. Evaluated as a template. tolerations: [] ingress: enabled: false className: \"\" labels: {} annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local config: # -- listen address listenAddress: 0.0.0.0:8080 # -- The upstream URL to proxy to upstream: \"http://prometheus:9090\" # -- The label to enforce in all proxies PromQL queries. label: \"namespace\" # -- Additional arguments for prom-label-proxy extraArgs: - \"--enable-label-apis=true\" - \"--error-on-replace=true\" . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prom-label-proxy#020",
    
    "relUrl": "/prometheus-community/helm-charts/prom-label-proxy#020"
  },"16": {
    "doc": "prometheus-adapter",
    "title": "prometheus-adapter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-adapter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-adapter"
  },"17": {
    "doc": "prometheus-adapter",
    "title": "4.1.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . affinity: {} topologySpreadConstraints: [] image: repository: registry.k8s.io/prometheus-adapter/prometheus-adapter tag: v0.10.0 pullPolicy: IfNotPresent logLevel: 4 metricsRelistInterval: 1m listenPort: 6443 nodeSelector: {} priorityClassName: \"\" ## Override the release namespace (for multi-namespace deployments in combined charts) namespaceOverride: \"\" ## Additional annotations to add to all resources customAnnotations: {} # role: custom-metrics ## Additional labels to add to all resources customLabels: {} # monitoring: prometheus-adapter # Url to access prometheus prometheus: # Value is templated url: http://prometheus.default.svc port: 9090 path: \"\" replicas: 1 # k8s 1.21 needs fsGroup to be set for non root deployments # ref: https://github.com/kubernetes/kubernetes/issues/70679 podSecurityContext: fsGroup: 10001 # SecurityContext of the container # ref. https://kubernetes.io/docs/tasks/configure-pod-container/security-context securityContext: allowPrivilegeEscalation: false capabilities: drop: [\"all\"] readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 10001 seccompProfile: type: RuntimeDefault rbac: # Specifies whether RBAC resources should be created create: true psp: # Specifies whether PSP resources should be created create: false serviceAccount: # Specifies whether a service account should be created create: true # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: # ServiceAccount annotations. # Use case: AWS EKS IAM roles for service accounts # ref: https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html annotations: {} # Custom DNS configuration to be added to prometheus-adapter pods dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 resources: {} # requests: # cpu: 100m # memory: 128Mi # limits: # cpu: 100m # memory: 128Mi rules: default: true custom: [] # - seriesQuery: '{__name__=~\"^some_metric_count$\"}' # resources: # template: &lt;&lt;.Resource&gt;&gt; # name: # matches: \"\" # as: \"my_custom_metric\" # metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) # Mounts a configMap with pre-generated rules for use. Overrides the # default, custom, external and resource entries existing: external: [] # - seriesQuery: '{__name__=~\"^some_metric_count$\"}' # resources: # template: &lt;&lt;.Resource&gt;&gt; # name: # matches: \"\" # as: \"my_external_metric\" # metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) # resource: # cpu: # containerQuery: | # sum by (&lt;&lt;.GroupBy&gt;&gt;) ( # rate(container_cpu_usage_seconds_total{container!=\"\",&lt;&lt;.LabelMatchers&gt;&gt;}[3m]) # ) # nodeQuery: | # sum by (&lt;&lt;.GroupBy&gt;&gt;) ( # rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\",&lt;&lt;.LabelMatchers&gt;&gt;}[3m]) # ) # resources: # overrides: # node: # resource: node # namespace: # resource: namespace # pod: # resource: pod # containerLabel: container # memory: # containerQuery: | # sum by (&lt;&lt;.GroupBy&gt;&gt;) ( # avg_over_time(container_memory_working_set_bytes{container!=\"\",&lt;&lt;.LabelMatchers&gt;&gt;}[3m]) # ) # nodeQuery: | # sum by (&lt;&lt;.GroupBy&gt;&gt;) ( # avg_over_time(node_memory_MemTotal_bytes{&lt;&lt;.LabelMatchers&gt;&gt;}[3m]) # - # avg_over_time(node_memory_MemAvailable_bytes{&lt;&lt;.LabelMatchers&gt;&gt;}[3m]) # ) # resources: # overrides: # node: # resource: node # namespace: # resource: namespace # pod: # resource: pod # containerLabel: container # window: 3m service: annotations: {} port: 443 type: ClusterIP # clusterIP: 1.2.3.4 tls: enable: false ca: |- # Public CA file that signed the APIService key: |- # Private key of the APIService certificate: |- # Public key of the APIService # Set environment variables from secrets, configmaps or by setting them as name/value env: [] # - name: TMP_DIR # value: /tmp # - name: PASSWORD # valueFrom: # secretKeyRef: # name: mysecret # key: password # optional: false # Any extra arguments extraArguments: [] # - --tls-private-key-file=/etc/tls/tls.key # - --tls-cert-file=/etc/tls/tls.crt # Any extra volumes extraVolumes: [] # - name: example-name # hostPath: # path: /path/on/host # type: DirectoryOrCreate # - name: ssl-certs # hostPath: # path: /etc/ssl/certs/ca-bundle.crt # type: File # Any extra volume mounts extraVolumeMounts: [] # - name: example-name # mountPath: /path/in/container # - name: ssl-certs # mountPath: /etc/ssl/certs/ca-certificates.crt # readOnly: true tolerations: [] # Labels added to the pod podLabels: {} # Annotations added to the pod podAnnotations: {} # Annotations added to the deployment deploymentAnnotations: {} hostNetwork: # Specifies if prometheus-adapter should be started in hostNetwork mode. # # You would require this enabled if you use alternate overlay networking for pods and # API server unable to communicate with metrics-server. As an example, this is required # if you use Weave network on EKS. See also dnsPolicy enabled: false # When hostNetwork is enabled, you probably want to set this to ClusterFirstWithHostNet # dnsPolicy: ClusterFirstWithHostNet # Deployment strategy type strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% podDisruptionBudget: # Specifies if PodDisruptionBudget should be enabled # When enabled, minAvailable or maxUnavailable should also be defined. enabled: false minAvailable: maxUnavailable: 1 certManager: enabled: false caCertDuration: 43800h certDuration: 8760h . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-adapter#411",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-adapter#411"
  },"18": {
    "doc": "prometheus-blackbox-exporter",
    "title": "prometheus-blackbox-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-blackbox-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-blackbox-exporter"
  },"19": {
    "doc": "prometheus-blackbox-exporter",
    "title": "7.6.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . restartPolicy: Always kind: Deployment ## Override the namespace ## namespaceOverride: \"\" # Override Kubernetes version if your distribution does not follow semver v2 kubeVersionOverride: \"\" ## set to true to add the release label so scraping of the servicemonitor with kube-prometheus-stack works out of the box releaseLabel: false podDisruptionBudget: {} # maxUnavailable: 0 ## Allow automount the serviceaccount token for sidecar container (eg: oauthproxy) automountServiceAccountToken: false ## Additional blackbox-exporter container environment variables ## For instance to add a http_proxy ## ## extraEnv: ## HTTP_PROXY: \"http://superproxy.com:3128\" ## NO_PROXY: \"localhost,127.0.0.1\" extraEnv: {} extraVolumes: [] # - name: secret-blackbox-oauth-htpasswd # secret: # defaultMode: 420 # secretName: blackbox-oauth-htpasswd # - name: storage-volume # persistentVolumeClaim: # claimName: example ## Additional volumes that will be attached to the blackbox-exporter container extraVolumeMounts: # - name: ca-certs # mountPath: /etc/ssl/certs/ca-certificates.crt ## Additional InitContainers to initialize the pod ## extraInitContainers: [] extraContainers: [] # - name: oAuth2-proxy # args: # - -https-address=:9116 # - -upstream=http://localhost:9115 # - -skip-auth-regex=^/metrics # - -openshift-delegate-urls={\"/\":{\"group\":\"monitoring.coreos.com\",\"resource\":\"prometheuses\",\"verb\":\"get\"}} # image: openshift/oauth-proxy:v1.1.0 # ports: # - containerPort: 9116 # name: proxy # resources: # limits: # memory: 16Mi # requests: # memory: 4Mi # cpu: 20m # volumeMounts: # - mountPath: /etc/prometheus/secrets/blackbox-tls # name: secret-blackbox-tls ## Enable pod security policy pspEnabled: true hostNetwork: false strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate image: repository: prom/blackbox-exporter tag: v0.23.0 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName podSecurityContext: {} # fsGroup: 1000 ## User and Group to run blackbox-exporter container as securityContext: runAsUser: 1000 runAsGroup: 1000 readOnlyRootFilesystem: true runAsNonRoot: true allowPrivilegeEscalation: false capabilities: drop: [\"ALL\"] # Add NET_RAW to enable ICMP # add: [\"NET_RAW\"] livenessProbe: httpGet: path: /health port: http readinessProbe: httpGet: path: /health port: http nodeSelector: {} tolerations: [] affinity: {} # if the configuration is managed as secret outside the chart, using SealedSecret for example, # provide the name of the secret here. If secretConfig is set to true, configExistingSecretName will be ignored # in favor of the config value. configExistingSecretName: \"\" # Store the configuration as a `Secret` instead of a `ConfigMap`, useful in case it contains sensitive data secretConfig: false config: modules: http_2xx: prober: http timeout: 5s http: valid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"] follow_redirects: true preferred_ip_protocol: \"ip4\" # Set custom config path, other than default /config/blackbox.yaml. If let empty, path will be \"/config/blackbox.yaml\" # configPath: \"/foo/bar\" extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /etc/secrets/ssl/ # subPath: certificates.crt # (optional) # configMap: certs-configmap # readOnly: true # defaultMode: 420 ## Additional secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # secretName: blackbox-secret-files # readOnly: true # defaultMode: 420 resources: {} # limits: # memory: 300Mi # requests: # memory: 50Mi priorityClassName: \"\" service: annotations: {} labels: {} type: ClusterIP port: 9115 # Only changes container port. Application port can be changed with extraArgs (--web.listen-address=:9115) # https://github.com/prometheus/blackbox_exporter/blob/998037b5b40c1de5fee348ffdea8820509d85171/main.go#L55 containerPort: 9115 serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: annotations: {} ## An Ingress resource can provide name-based virtual hosting and TLS ## termination among other things for CouchDB deployments which are accessed ## from outside the Kubernetes cluster. ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/ ingress: enabled: false className: \"\" labels: {} annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local podAnnotations: {} # Hostaliases allow to add additional DNS entries to be injected directly into pods. # This will take precedence over your implemented DNS solution hostAliases: [] # - ip: 192.168.1.1 # hostNames: # - test.example.com # - another.example.net pod: labels: {} extraArgs: [] # - --history.limit=1000 replicas: 1 serviceMonitor: ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator for blackbox-exporter itself ## selfMonitor: enabled: false additionalMetricsRelabels: {} additionalRelabeling: [] labels: {} interval: 30s scrapeTimeout: 30s ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator for each target ## enabled: false # Default values that will be used for all ServiceMonitors created by `targets` defaults: additionalMetricsRelabels: {} additionalRelabeling: [] labels: {} interval: 30s scrapeTimeout: 30s module: http_2xx ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. scheme: http ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig tlsConfig: {} bearerTokenFile: targets: # - name: example # Human readable URL that will appear in Prometheus / AlertManager # url: http://example.com/healthz # The URL that blackbox will scrape # hostname: example.com # HTTP probes can accept an additional `hostname` parameter that will set `Host` header and TLS SNI # labels: {} # Map of labels for ServiceMonitor. Overrides value set in `defaults` # interval: 60s # Scraping interval. Overrides value set in `defaults` # scrapeTimeout: 60s # Scrape timeout. Overrides value set in `defaults` # module: http_2xx # Module used for scraping. Overrides value set in `defaults` # additionalMetricsRelabels: {} # Map of metric labels and values to add # additionalRelabeling: [] # List of metric relabeling actions to run ## Custom PrometheusRules to be defined ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule: enabled: false additionalLabels: {} namespace: \"\" rules: [] ## Network policy for chart networkPolicy: # Enable network policy and allow access from anywhere enabled: false # Limit access only from monitoring namespace # Before setting this value to true, you must add the name=monitoring label to the monitoring namespace # Network Policy uses label filtering allowMonitoringNamespace: false ## dnsPolicy and dnsConfig for Deployments and Daemonsets if you want non-default settings. ## These will be passed directly to the PodSpec of same. dnsPolicy: dnsConfig: . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-blackbox-exporter#761",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-blackbox-exporter#761"
  },"20": {
    "doc": "prometheus-cloudwatch-exporter",
    "title": "prometheus-cloudwatch-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-cloudwatch-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-cloudwatch-exporter"
  },"21": {
    "doc": "prometheus-cloudwatch-exporter",
    "title": "0.24.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-cloudwatch-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: prom/cloudwatch-exporter # if not set appVersion field from Chart.yaml is used tag: pullPolicy: IfNotPresent pullSecrets: # - name: \"image-pull-secret\" # Example proxy configuration: # command: # - 'java' # - '-Dhttp.proxyHost=proxy.example.com' # - '-Dhttp.proxyPort=3128' # - '-Dhttps.proxyHost=proxy.example.com' # - '-Dhttps.proxyPort=3128' # - '-jar' # - '/cloudwatch_exporter.jar' # - '9106' # - '/config/config.yml' command: [] containerPort: 9106 service: type: ClusterIP port: 9106 portName: http annotations: {} labels: {} pod: labels: {} annotations: {} # Labels and annotations to attach to the deployment resource deployment: labels: {} annotations: {} # Extra environment variables extraEnv: # - name: foo # value: baa resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi aws: role: # Enables usage of regional STS endpoints rather than global which is default stsRegional: enabled: false # The name of a pre-created secret in which AWS credentials are stored. When # set, aws_access_key_id is assumed to be in a field called access_key, # aws_secret_access_key is assumed to be in a field called secret_key, and the # session token, if it exists, is assumed to be in a field called # security_token secret: name: includesSessionToken: false # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before aws_access_key_id: aws_secret_access_key: serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: # annotations: # Will add the provided map to the annotations for the created serviceAccount # e.g. # annotations: # eks.amazonaws.com/role-arn: arn:aws:iam::1234567890:role/prom-cloudwatch-exporter-oidc # eks.amazonaws.com/sts-regional-endpoints: \"true\" # Specifies whether to automount API credentials for the ServiceAccount. automountServiceAccountToken: true rbac: # Specifies whether RBAC resources should be created create: true # Configuration is rendered with `tpl` function, therefore you can use any Helm variables and/or templates here config: |- # This is the default configuration for prometheus-cloudwatch-exporter region: eu-west-1 period_seconds: 240 metrics: - aws_namespace: AWS/ELB aws_metric_name: HealthyHostCount aws_dimensions: [AvailabilityZone, LoadBalancerName] aws_statistics: [Average] - aws_namespace: AWS/ELB aws_metric_name: UnHealthyHostCount aws_dimensions: [AvailabilityZone, LoadBalancerName] aws_statistics: [Average] - aws_namespace: AWS/ELB aws_metric_name: RequestCount aws_dimensions: [AvailabilityZone, LoadBalancerName] aws_statistics: [Sum] - aws_namespace: AWS/ELB aws_metric_name: Latency aws_dimensions: [AvailabilityZone, LoadBalancerName] aws_statistics: [Average] - aws_namespace: AWS/ELB aws_metric_name: SurgeQueueLength aws_dimensions: [AvailabilityZone, LoadBalancerName] aws_statistics: [Maximum, Sum] nodeSelector: {} tolerations: [] affinity: {} # Configurable health checks against the /healthy and /ready endpoints livenessProbe: path: /-/healthy initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 readinessProbe: path: /-/ready initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed # namespace: monitoring # Set how frequently Prometheus should scrape # interval: 30s # Set path to cloudwatch-exporter telemtery-path # telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator # labels: # Set timeout for scrape # timeout: 10s # Set relabelings for the ServiceMonitor, use to apply to samples before scraping # relabelings: [] # Set metricRelabelings for the ServiceMonitor, use to apply to samples for ingestion # metricRelabelings: [] # # Example - note the Kubernetes convention of camelCase instead of Prometheus' snake_case # metricRelabelings: # - sourceLabels: [dbinstance_identifier] # action: replace # replacement: mydbname # targetLabel: dbname prometheusRule: # Specifies whether a PrometheusRule should be created enabled: false # Set the namespace the PrometheusRule should be deployed # namespace: monitoring # Set labels for the PrometheusRule, use this to define your scrape label for Prometheus Operator # labels: # Example - note the Kubernetes convention of camelCase instead of Prometheus' # rules: # - alert: ELB-Low-BurstBalance # annotations: # message: The ELB BurstBalance during the last 10 minutes is lower than 80%. # expr: aws_ebs_burst_balance_average &lt; 80 # for: 10m # labels: # severity: warning # - alert: ELB-Low-BurstBalance # annotations: # message: The ELB BurstBalance during the last 10 minutes is lower than 50%. # expr: aws_ebs_burst_balance_average &lt; 50 # for: 10m # labels: # severity: warning # - alert: ELB-Low-BurstBalance # annotations: # message: The ELB BurstBalance during the last 10 minutes is lower than 30%. # expr: aws_ebs_burst_balance_average &lt; 30 # for: 10m # labels: # severity: critical ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx # pathType is only for k8s &gt;= 1.18 pathType: Prefix securityContext: runAsUser: 65534 # run as nobody user instead of root fsGroup: 65534 # necessary to be able to read the EKS IAM token containerSecurityContext: {} # allowPrivilegeEscalation: false # readOnlyRootFilesystem: true # Leverage a PriorityClass to ensure your pods survive resource shortages # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/ # priorityClassName: system-cluster-critical priorityClassName: \"\" . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-cloudwatch-exporter#0240",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-cloudwatch-exporter#0240"
  },"22": {
    "doc": "prometheus-conntrack-stats-exporter",
    "title": "prometheus-conntrack-stats-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-conntrack-stats-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-conntrack-stats-exporter"
  },"23": {
    "doc": "prometheus-conntrack-stats-exporter",
    "title": "0.5.5",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . fullnameOverride: \"\" image: repository: jwkohnen/conntrack-stats-exporter pullPolicy: IfNotPresent # if not set appVersion field from Chart.yaml is used tag: \"\" imagePullSecrets: [] nameOverride: \"\" nodeSelector: {} podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9371\" commonLabels: {} podLabels: {} podMonitor: enabled: false honorLabels: true interval: 10s labels: {} metricRelabelings: [] relabelings: [] selectorOverride: {} scrapeTimeout: 10s podSecurityContext: {} # fsGroup: 2000 resources: {} # requests: # cpu: 10m # memory: 8Mi # limits: # cpu: 20m # memory: 15Mi securityContext: privileged: true serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" tolerations: [] . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-conntrack-stats-exporter#055",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-conntrack-stats-exporter#055"
  },"24": {
    "doc": "prometheus-consul-exporter",
    "title": "prometheus-consul-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-consul-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-consul-exporter"
  },"25": {
    "doc": "prometheus-consul-exporter",
    "title": "1.0.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for consul-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 rbac: # Specifies whether RBAC resources should be created create: true pspEnabled: true pspUseAppArmor: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: image: repository: prom/consul-exporter tag: v0.4.0 pullPolicy: IfNotPresent nameOverride: \"\" fullnameOverride: \"\" # Add your consul server details here consulServer: host:port # Flags - for a list visit https://github.com/prometheus/consul_exporter#flags options: {} # no-consul.health-summary: # kv.filter=foobar service: type: ClusterIP port: 9107 annotations: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed # namespace: monitoring # Set how frequently Prometheus should scrape # interval: 30s # Set path to consul-exporter telemtery-path # telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator # labels: # Set timeout for scrape # timeout: 10s # Set of labels to transfer on the Kubernetes Service onto the target. # targetLabels: [] # metricRelabelings: [] nodeSelector: {} tolerations: [] affinity: {} # Extra environment variables extraEnv: [] # Annotations for the pods podAnnotations: {} # Init Containers for Exporter Pod initContainers: [] # Extra containers for the exporter pod extraContainers: [] # Extra Volumes for the pod extraVolumes: [] # - name: example # configMap: # name: example # Extra Volume Mounts for the exporter container extraVolumeMounts: [] # - name: example # mountPath: /example . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-consul-exporter#100",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-consul-exporter#100"
  },"26": {
    "doc": "prometheus-couchdb-exporter",
    "title": "prometheus-couchdb-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-couchdb-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-couchdb-exporter"
  },"27": {
    "doc": "prometheus-couchdb-exporter",
    "title": "0.2.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-couchdb-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. rbac: # Specifies whether RBAC resources should be created create: true # Specifies whether a PodSecurityPolicy should be created pspEnabled: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: replicaCount: 1 image: repository: gesellix/couchdb-prometheus-exporter tag: 16 pullPolicy: IfNotPresent service: type: ClusterIP port: 9984 ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" path: / hosts: # - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} tolerations: [] affinity: {} ## CouchDB exporter configurations couchdb: ## URI of the couchdb instance uri: http://couchdb.default.svc:5984 ## Specify the list of databases to get the disk usage stats as comma separates like \"db-1,db-2\" ## or to get stats for every database, please use \"_all_dbs\" databases: _all_dbs ## CouchDB username # username: ## CouchDB Password # password: . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-couchdb-exporter#021",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-couchdb-exporter#021"
  },"28": {
    "doc": "prometheus-druid-exporter",
    "title": "prometheus-druid-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-druid-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-druid-exporter"
  },"29": {
    "doc": "prometheus-druid-exporter",
    "title": "1.0.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . --- name: druid-exporter image: name: quay.io/opstree/druid-exporter tag: v0.11 pullPolicy: IfNotPresent annotations: {} podAnnotations: {} druidURL: http://druid.opstreelabs.in logLevel: info logFormat: json exporterPort: 8080 serviceAccount: create: true serviceType: ClusterIP serviceMonitor: enabled: false namespace: monitoring interval: 30s scrapeTimeout: 10s additionalLabels: {} targetLabels: [] securityContext: {} containerSecurityContext: {} nodeSelector: {} tolerations: [] affinity: {} . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-druid-exporter#100",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-druid-exporter#100"
  },"30": {
    "doc": "prometheus-elasticsearch-exporter",
    "title": "prometheus-elasticsearch-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-elasticsearch-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-elasticsearch-exporter"
  },"31": {
    "doc": "prometheus-elasticsearch-exporter",
    "title": "5.0.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . ## Global settings for all charts in a tree of dependent charts ## This allows to set the same values for the imagePullSecrets, obviating the need ## to set the same values for each chart (and image) separately global: imagePullSecrets: [] ## number of exporter instances ## replicaCount: 1 ## restart policy for all containers ## restartPolicy: Always image: repository: quay.io/prometheuscommunity/elasticsearch-exporter tag: v1.5.0 pullPolicy: IfNotPresent pullSecret: \"\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 seccompProfile: type: \"RuntimeDefault\" securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true # Custom DNS configuration to be added to prometheus-elasticsearch-exporter pods dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 log: format: logfmt level: info resources: {} # requests: # cpu: 100m # memory: 128Mi # limits: # cpu: 100m # memory: 128Mi priorityClassName: \"\" nodeSelector: {} tolerations: [] podAnnotations: {} podLabels: {} affinity: {} service: type: ClusterIP httpPort: 9108 metricsPort: name: http annotations: {} labels: {} deployment: annotations: {} labels: {} ## Extra environment variables that will be passed into the exporter pod ## example: ## env: ## KEY_1: value1 ## KEY_2: value2 env: {} ## Extra arguments to pass to the container's executable ## example: # extraArgs: # - --es.indices_mappings extraArgs: [] ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment ## This can be useful for auth tokens, etc envFromSecret: \"\" ## A list of environment variables from secret refs that will be passed into the exporter pod ## example: ## This will set ${ES_PASSWORD} to the 'password' key from the 'my-secret' secret ## extraEnvSecrets: ## ES_PASSWORD: ## secret: my-secret ## key: password extraEnvSecrets: {} # A list of secrets and their paths to mount inside the pod # This is useful for mounting certificates for security secretMounts: [] # - name: elastic-certs # secretName: elastic-certs # path: /ssl # A list of additional Volume to add to the deployment # this is useful if the volume you need is not a secret (csi volume etc.) extraVolumes: [] # - name: csi-volume # csi: # driver: secrets-store.csi.k8s.io # readOnly: true # volumeAttributes: # secretProviderClass: my-spc # A list of additional VolumeMounts to add to the deployment # this is useful for mounting any other needed resource into # the elasticsearch-exporter pod extraVolumeMounts: [] # - name: csi-volume # mountPath: /csi/volume # readOnly: true es: ## Address (host and port) of the Elasticsearch node we should connect to. ## This could be a local node (localhost:9200, for instance), or the address ## of a remote Elasticsearch server. When basic auth is needed, ## specify as: &lt;proto&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;. e.g., http://admin:pass@localhost:9200. ## uri: http://localhost:9200 ## If true, query stats for all nodes in the cluster, rather than just the ## node we connect to. ## all: true ## If true, query stats for all indices in the cluster. ## indices: true ## If true, query settings stats for all indices in the cluster. ## indices_settings: true ## If true, query mapping stats for all indices in the cluster. ## indices_mappings: true ## If true, query stats for shards in the cluster. ## shards: true ## If true, query stats for snapshots in the cluster. ## snapshots: true ## If true, query stats for cluster settings. ## cluster_settings: false ## Timeout for trying to get stats from Elasticsearch. (ex: 20s) ## timeout: 30s ## Skip SSL verification when connecting to Elasticsearch ## (only available if image.tag &gt;= 1.0.4rc1) ## sslSkipVerify: false ssl: ## If true, a secure connection to ES cluster is used ## enabled: false ## If true, certs from secretMounts will be need to be referenced instead of certs below ## useExistingSecrets: false ca: ## PEM that contains trusted CAs used for setting up secure Elasticsearch connection ## # pem: # Path of ca pem file which should match a secretMount path path: /ssl/ca.pem client: ## if true, client SSL certificate is used for authentication ## enabled: true ## PEM that contains the client cert to connect to Elasticsearch. ## # pem: # Path of client pem file which should match a secretMount path pemPath: /ssl/client.pem ## Private key for client auth when connecting to Elasticsearch ## # key: # Path of client key file which should match a secretMount path keyPath: /ssl/client.key web: ## Path under which to expose metrics. ## path: /metrics serviceMonitor: ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled: false # namespace: monitoring labels: {} interval: 10s jobLabel: \"\" scrapeTimeout: 10s scheme: http relabelings: [] targetLabels: [] metricRelabelings: [] sampleLimit: 0 prometheusRule: ## If true, a PrometheusRule CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## ## The rules will be processed as Helm template, allowing to set variables in them. enabled: false # namespace: monitoring labels: {} rules: [] # - record: elasticsearch_filesystem_data_used_percent # expr: | # 100 * (elasticsearch_filesystem_data_size_bytes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\"} - elasticsearch_filesystem_data_free_bytes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\"}) # / elasticsearch_filesystem_data_size_bytes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\"} # - record: elasticsearch_filesystem_data_free_percent # expr: 100 - elasticsearch_filesystem_data_used_percent{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\"} # - alert: ElasticsearchTooFewNodesRunning # expr: elasticsearch_cluster_health_number_of_nodes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\"} &lt; 3 # for: 5m # labels: # severity: critical # annotations: # description: There are only {{ \"{{ $value }}\" }} &lt; 3 ElasticSearch nodes running # summary: ElasticSearch running on less than 3 nodes # - alert: ElasticsearchHeapTooHigh # expr: | # elasticsearch_jvm_memory_used_bytes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\", area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{service=\"{{ template \"elasticsearch-exporter.fullname\" . }}\", area=\"heap\"} # &gt; 0.9 # for: 15m # labels: # severity: critical # annotations: # description: The heap usage is over 90% for 15m # summary: ElasticSearch node {{ \"{{ $labels.node }}\" }} heap usage is high # Create a service account # To use a service account not handled by the chart, set the name here # and set create to false serviceAccount: create: false name: default automountServiceAccountToken: true annotations: {} # Creates a PodSecurityPolicy and the role/rolebinding # allowing the serviceaccount to use it podSecurityPolicies: enabled: false . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-elasticsearch-exporter#500",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-elasticsearch-exporter#500"
  },"32": {
    "doc": "prometheus-fastly-exporter",
    "title": "prometheus-fastly-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-fastly-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-fastly-exporter"
  },"33": {
    "doc": "prometheus-fastly-exporter",
    "title": "0.1.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for fastly-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 rbac: # Specifies whether RBAC resources should be created create: true pspEnabled: true pspUseAppArmor: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: image: repository: ghcr.io/fastly/fastly-exporter tag: v7.2.4 pullPolicy: IfNotPresent # API token Fastly - https://docs.fastly.com/en/guides/using-api-tokens#creating-api-tokens fastly: token: \"token\" # Name of an externally managed secret (in the same namespace) containing the fastly token as key `fastly-api-token`. # If this is provided, the value token is ignored. existingSecret: name: \"\" key: \"fastly-api-token\" nameOverride: \"\" fullnameOverride: \"\" # Flags - for a list visit https://github.com/fastly/fastly-exporter#filtering-services options: {} # metric-allowlist='bytes_total$' # metric-blocklist=imgopto service: type: ClusterIP port: 8080 annotations: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed # namespace: monitoring # Set how frequently Prometheus should scrape # interval: 30s # Set path to fastly-exporter telemtery-path # telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator # labels: # Set timeout for scrape # timeout: 10s # Set of labels to transfer on the Kubernetes Service onto the target. # targetLabels: [] # metricRelabelings: [] nodeSelector: {} tolerations: [] affinity: {} # Extra environment variables extraEnv: [] # Annotations for the pods podAnnotations: {} # Init Containers for Exporter Pod initContainers: [] # Extra containers for the exporter pod extraContainers: [] # Extra Volumes for the pod extraVolumes: [] # - name: example # configMap: # name: example # Extra Volume Mounts for the exporter container extraVolumeMounts: [] # - name: example # mountPath: /example . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-fastly-exporter#011",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-fastly-exporter#011"
  },"34": {
    "doc": "prometheus-json-exporter",
    "title": "prometheus-json-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-json-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-json-exporter"
  },"35": {
    "doc": "prometheus-json-exporter",
    "title": "0.6.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-json-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: quay.io/prometheuscommunity/json-exporter pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" imagePullSecrets: [] nameOverride: \"\" fullnameOverride: \"\" serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: [] # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" podAnnotations: [] podSecurityContext: {} # fsGroup: 2000 # podLabels: # Custom labels for the pod securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 7979 targetPort: http name: http serviceMonitor: ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled: false namespace: \"\" scheme: http # Default values that will be used for all ServiceMonitors created by `targets` defaults: additionalMetricsRelabels: {} interval: 10s labels: {} scrapeTimeout: 30s targets: # - name: example # Human readable URL that will appear in Prometheus / AlertManager # url: http://example.com/healthz # The URL that json-exporter will scrape # labels: {} # Map of labels for ServiceMonitor. Overrides value set in `defaults` # interval: 60s # Scraping interval. Overrides value set in `defaults` # scrapeTimeout: 60s # Scrape timeout. Overrides value set in `defaults` # additionalMetricsRelabels: {} # Map of metric labels and values to add # module: example_module # Name of the module to pick up from `config.yaml` for scraping this target. Optional. Default is `default` provided by the exporter itself. ingress: enabled: false className: \"\" annotations: [] # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi environmentVariables: [] # - name: some-secret-variable # valueFrom: # secretKeyRef: # key: some-key # name: some-secret-name # - name: some-other-variable # value: some-value autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 80 # targetMemoryUtilizationPercentage: 80 nodeSelector: [] tolerations: [] affinity: [] configuration: config: | --- modules: default: metrics: - name: example_global_value path: \"{ .counter }\" help: Example of a top-level global value scrape in the json labels: environment: beta # static label location: \"planet-{.location}\" # dynamic label - name: example_value type: object help: Example of sub-level value scrapes from a json path: '{.values[?(@.state == \"ACTIVE\")]}' labels: environment: beta # static label id: '{.id}' # dynamic label values: active: 1 # static value count: '{.count}' # dynamic value boolean: '{.some_boolean}' headers: X-Dummy: my-test-header # If 'body' is set, it will be sent by the exporter as the body content in the scrape request. The HTTP method will also be set as 'POST' in this case. # body: # content: | # {\"time_diff\": \"1m25s\", \"anotherVar\": \"some value\"} # The body content can also be a Go Template (https://golang.org/pkg/text/template), with all the functions from the Sprig library (https://masterminds.github.io/sprig/) available. All the query parameters sent by prometheus in the scrape query to the exporter, are available in the template. # body: # content: | # {\"time_diff\": \"{{ duration `95` }}\",\"anotherVar\": \"{{ .myVal | first }}\"} # templatize: true # For full http client config parameters, ref: https://pkg.go.dev/github.com/prometheus/common/config?tab=doc#HTTPClientConfig # # http_client_config: # tls_config: # insecure_skip_verify: true # basic_auth: # username: myuser # #password: veryverysecret # password_file: /tmp/mysecret.txt ## Custom PrometheusRules to be defined ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule: enabled: false additionalLabels: {} namespace: \"\" rules: [] additionalVolumes: [] # - name: password-file # secret: # secretName: secret-name additionalVolumeMounts: [] # - name: password-file # mountPath: \"/tmp/mysecret.txt\" # subPath: mysecret.txt . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-json-exporter#061",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-json-exporter#061"
  },"36": {
    "doc": "prometheus-kafka-exporter",
    "title": "prometheus-kafka-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-kafka-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-kafka-exporter"
  },"37": {
    "doc": "prometheus-kafka-exporter",
    "title": "1.8.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . image: repository: danielqsj/kafka-exporter # Overrides the image tag whose default is the chart appVersion. tag: \"\" pullPolicy: IfNotPresent replicaCount: 1 kafkaServer: - kafka-server:9092 # Specifies broker version to use, leave empty for default kafkaBrokerVersion: # Specifies log verbosity verbosity: 0 # Sarama logging sarama: logEnabled: false rbac: # Specifies whether RBAC resources should be created create: true # Specifies whether a PodSecurityPolicy should be created pspEnabled: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: env: [] # - name: &lt;ENV_VAR_NAME&gt; # value: &lt;value&gt; # List of additional cli arguments to configure kafka-exporter # for example: --log.enable-sarama, --log.level=debug, etc. # all the possible args can be found here: https://github.com/danielqsj/kafka_exporter#flags extraArgs: [] service: type: ClusterIP port: 9308 labels: {} annotations: {} liveness: enabled: false probe: httpGet: path: / port: exporter-port readiness: enabled: false probe: httpGet: path: / port: exporter-port prometheus: serviceMonitor: enabled: false namespace: monitoring interval: \"30s\" # If serviceMonitor is enabled and you want prometheus to automatically register # target using serviceMonitor, add additionalLabels with prometheus release name # e.g. If you have deployed kube-prometheus-stack with release name kube-prometheus # then additionalLabels will be # additionalLabels: # release: kube-prometheus additionalLabels: {} targetLabels: [] resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} annotations: {} tolerations: [] affinity: {} # this allows the usage of tls connection to your kafka cluster based on a secret in tls format # mandatory keys: # ca.crt # tls.crt # tls.key tls: enabled: false insecureSkipVerify: false # The kafka server's name. Used to verify the hostname on the returned certificates unless tls.insecureSkipVerify is set to true. serverName: \"\" # mountPath: /secret/certs # secretName: &lt;name of an existing secret&gt; sasl: enabled: false handshake: true scram: enabled: false # mechanism: &lt;plain|scram-sha512|scram-sha256&gt; # add username and password # username: # password: # or use an existing secret # secretName: &lt;name of an existing secret&gt; kerberos: enabled: false # serviceName: # realm: # kerberosAuthType: &lt;keytabAuth|userAuth&gt; # mountPath: /secret/kerberos # secretName: &lt;name of an existing secret&gt; # This enables TLS for web server server: tls: enabled: false mutualAuthEnabled: false # mountPath: /secret/certs # secretName: &lt;name of an existing secret&gt; # If enabled Kafka dependency chart will be used. # This is only needed for the CI job so it should always be disabled. kafka: enabled: false # Set security context for the kafka-exporter pod. Useful when you need to adapt to an existing PSP securityContext: {} # fsGroup: 2000 # runAsGroup: 10002 # runAsUser: 10001 # seccompProfile: # type: RuntimeDefault # Set securityContext for the kafka-exporter container containerSecurityContext: {} # allowPrivilegeEscalation: false # capabilities: # drop: [\"all\"] # readOnlyRootFilesystem: true # runAsGroup: 10002 # runAsNonRoot: true # runAsUser: 10001 # seccompProfile: # type: RuntimeDefault . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-kafka-exporter#180",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-kafka-exporter#180"
  },"38": {
    "doc": "prometheus-mongodb-exporter",
    "title": "prometheus-mongodb-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-mongodb-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-mongodb-exporter"
  },"39": {
    "doc": "prometheus-mongodb-exporter",
    "title": "3.1.2",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . affinity: {} annotations: {} extraArgs: - --collect-all fullnameOverride: \"\" image: pullPolicy: IfNotPresent repository: percona/mongodb_exporter tag: \"\" imagePullSecrets: [] livenessProbe: httpGet: path: / port: metrics initialDelaySeconds: 10 # [mongodb[+srv]://][user:pass@]host1[:port1][,host2[:port2],...][/database][?options] mongodb: uri: \"mongodb://mongodb:27017\" # Name of an externally managed secret (in the same namespace) containing the connection uri as key `mongodb-uri`. # If this is provided, the value mongodb.uri is ignored. existingSecret: name: \"\" key: \"mongodb-uri\" nameOverride: \"\" nodeSelector: {} podAnnotations: {} # prometheus.io/scrape: \"true\" # prometheus.io/port: \"metrics\" podLabels: {} port: \"9216\" priorityClassName: \"\" readinessProbe: httpGet: path: / port: metrics initialDelaySeconds: 10 replicas: 1 resources: {} # limits: # cpu: 250m # memory: 192Mi # requests: # cpu: 100m # memory: 128Mi # Extra environment variables that will be passed into the exporter pod env: {} # Volumes that will be mounted into the exporter pod volumeMounts: [] # Volumes that will be attached to the exporter deployment volumes: [] securityContext: allowPrivilegeEscalation: false capabilities: drop: [\"all\"] readOnlyRootFilesystem: true runAsGroup: 10000 runAsNonRoot: true runAsUser: 10000 service: labels: {} annotations: {} port: 9216 type: ClusterIP portName: metrics serviceAccount: create: true # If create is true and name is not set, then a name is generated using the # fullname template. name: serviceMonitor: enabled: false interval: 30s scrapeTimeout: 10s namespace: additionalLabels: {} targetLabels: [] metricRelabelings: [] scheme: \"\" tlsConfig: {} tolerations: [] . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-mongodb-exporter#312",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-mongodb-exporter#312"
  },"40": {
    "doc": "prometheus-mysql-exporter",
    "title": "prometheus-mysql-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-mysql-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-mysql-exporter"
  },"41": {
    "doc": "prometheus-mysql-exporter",
    "title": "1.12.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . ## Default values for prometheus-mysql-exporter. ## This is a YAML-formatted file. ## Declare variables to be passed into your templates. ## override release name fullnameOverride: \"\" replicaCount: 1 image: repository: \"prom/mysqld-exporter\" ## if not set charts appVersion var is used tag: \"\" pullPolicy: \"IfNotPresent\" # imagePullSecrets: # - name: secret-name imagePullSecrets: [] service: labels: {} annotations: {} name: mysql-exporter type: ClusterIP externalPort: 9104 internalPort: 9104 serviceMonitor: # enabled should be set to true to enable prometheus-operator discovery of this service enabled: false # interval is the interval at which metrics should be scraped # interval: 30s # scrapeTimeout is the timeout after which the scrape is ended # scrapeTimeout: 10s # namespace: monitoring # additionalLabels is the set of additional labels to add to the ServiceMonitor additionalLabels: {} jobLabel: \"\" targetLabels: [] podTargetLabels: [] metricRelabelings: [] # Set relabel_configs as per https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config relabelings: [] serviceAccount: # Specifies whether a ServiceAccount should be created create: false # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: annotations: {} resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} tolerations: [] affinity: {} podLabels: {} # Extra Volume Mounts for the mysql exporter container extraVolumeMounts: [] # - name: example # mountPath: /example # Extra Volumes for the pod extraVolumes: [] # - name: example # configMap: # name: example podSecurityContext: {} # fsGroup: 65534 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 65534 annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/metrics\" prometheus.io/port: \"9104\" config: {} # Allow to set specifc options on the exporter # logLevel: info # logFormat: \"logger:stderr\" collectors: {} # auto_increment.columns: false # binlog_size: false # engine_innodb_status: false # engine_tokudb_status: false # global_status: true # global_variables: true # info_schema.clientstats: false # info_schema.innodb_metrics: false # info_schema.innodb_tablespaces: false # info_schema.innodb_cmp: false # info_schema.innodb_cmpmem: false # info_schema.processlist: false # info_schema.processlist.min_time: 0 # info_schema.query_response_time: false # info_schema.tables: true # info_schema.tables.databases: '*' # info_schema.tablestats: false # info_schema.schemastats: false # info_schema.userstats: false # perf_schema.eventsstatements: false # perf_schema.eventsstatements.digest_text_limit: 120 # perf_schema.eventsstatements.limit: false # perf_schema.eventsstatements.timelimit: 86400 # perf_schema.eventswaits: false # perf_schema.file_events: false # perf_schema.file_instances: false # perf_schema.indexiowaits: false # perf_schema.tableiowaits: false # perf_schema.tablelocks: false # perf_schema.replication_group_member_stats: false # slave_status: true # slave_hosts: false # heartbeat: false # heartbeat.database: heartbeat # heartbeat.table: heartbeat # mysql connection params which build the DATA_SOURCE_NAME env var of the docker container mysql: db: \"\" host: \"localhost\" param: \"\" pass: \"password\" port: 3306 protocol: \"\" user: \"exporter\" # secret with full DATA_SOURCE_NAME env var as stringdata existingSecret: \"\" # secret only containing the password existingPasswordSecret: name: \"\" key: \"\" # cloudsqlproxy https://cloud.google.com/sql/docs/mysql/sql-proxy cloudsqlproxy: enabled: false image: repo: \"gcr.io/cloudsql-docker/gce-proxy\" tag: \"1.33.0-alpine\" pullPolicy: \"IfNotPresent\" instanceConnectionName: \"project:us-central1:dbname\" ipAddressTypes: \"\" port: \"3306\" credentialsSecret: \"\" # service account json credentials: \"\" workloadIdentity: enabled: false serviceAccountEmail: \"\" . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-mysql-exporter#1121",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-mysql-exporter#1121"
  },"42": {
    "doc": "prometheus-nats-exporter",
    "title": "prometheus-nats-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-nats-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-nats-exporter"
  },"43": {
    "doc": "prometheus-nats-exporter",
    "title": "2.11.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-nats-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: natsio/prometheus-nats-exporter tag: \"\" pullPolicy: IfNotPresent service: labels: {} annotations: {} type: ClusterIP port: 80 targetPort: 7777 serviceMonitor: enabled: false additionalLabels: {} namespace: interval: scrapeTimeout: targetLabels: [] resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi config: nats: # See https://github.com/helm/charts/blob/master/stable/nats/templates/monitoring-svc.yaml service: nats-nats-monitoring namespace: default port: 8222 metrics: channelz: true connz: true jsz: true gatewayz: true leafz: true routez: true serverz: true subz: true varz: true nodeSelector: {} tolerations: [] affinity: {} annotations: {} extraContainers: | extraVolumes: | . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-nats-exporter#2110",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-nats-exporter#2110"
  },"44": {
    "doc": "prometheus-nginx-exporter",
    "title": "prometheus-nginx-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-nginx-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-nginx-exporter"
  },"45": {
    "doc": "prometheus-nginx-exporter",
    "title": "0.1.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Additional labels to add to all resources additionalLabels: {} # Additional annotations to add to all resources additionalAnnotations: {} # Number of instance replicaCount: 1 rbac: # Specifies whether RBAC resources should be created create: true serviceAccount: # Specifies whether a ServiceAccount should be created create: false # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: image: repository: nginx/nginx-prometheus-exporter pullPolicy: IfNotPresent # if not set appVersion field from Chart.yaml is used tag: \"\" nameOverride: \"\" fullnameOverride: \"\" # Add your nginx server details here nginxServer: \"http://{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local:8080/stub_status\" # nginxServer: \"http://frontend.default.svc.cluster.local:8080/stub_status\" # Arguments - https://github.com/nginxinc/nginx-prometheus-exporter#command-line-arguments options: {} # -nginx.plus # -nginx.retries int livenessProbe: httpGet: path: / port: http initialDelaySeconds: 30 timeoutSeconds: 10 readinessProbe: httpGet: path: / port: http initialDelaySeconds: 30 timeoutSeconds: 10 service: type: ClusterIP port: 9113 annotations: {} podAnnotations: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed namespace: \"\" # Set how frequently Prometheus should scrape interval: 30s # Set path to nginx-exporter telemtery-path path: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator additionalLabels: {} # Set timeout for scrape timeout: 10s # Set of labels to transfer on the Kubernetes Service onto the target. # targetLabels: [] # metricRelabelings: [] nodeSelector: {} tolerations: [] affinity: {} # Extra environment variables extraEnv: [] # Init Containers for Exporter Pod initContainers: [] # Extra containers for the exporter pod extraContainers: [] # Extra Volumes for the pod extraVolumes: [] # - name: example # configMap: # name: example # Extra Volume Mounts for the exporter container extraVolumeMounts: [] # - name: example # mountPath: /example # If enabled Nginx dependency chart will be used. # This is only needed for the CI job so it should always be disabled. nginx: enabled: false . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-nginx-exporter#010",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-nginx-exporter#010"
  },"46": {
    "doc": "prometheus-node-exporter",
    "title": "prometheus-node-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-node-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-node-exporter"
  },"47": {
    "doc": "prometheus-node-exporter",
    "title": "4.14.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-node-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. image: registry: quay.io repository: prometheus/node-exporter # Overrides the image tag whose default is {{ printf \"v%s\" .Chart.AppVersion }} tag: \"\" pullPolicy: IfNotPresent sha: \"\" imagePullSecrets: [] # - name: \"image-pull-secret\" global: # To help compatibility with other charts which use global.imagePullSecrets. # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style). # global: # imagePullSecrets: # - name: pullSecret1 # - name: pullSecret2 # or # global: # imagePullSecrets: # - pullSecret1 # - pullSecret2 imagePullSecrets: [] # # Allow parent charts to override registry hostname imageRegistry: \"\" # Configure kube-rbac-proxy. When enabled, creates a kube-rbac-proxy to protect the node-exporter http endpoint. # The requests are served through the same service but requests are HTTPS. kubeRBACProxy: enabled: false image: registry: quay.io repository: brancz/kube-rbac-proxy tag: v0.14.0 sha: \"\" pullPolicy: IfNotPresent # List of additional cli arguments to configure kube-rbac-prxy # for example: --tls-cipher-suites, --log-file, etc. # all the possible args can be found here: https://github.com/brancz/kube-rbac-proxy#usage extraArgs: [] ## Specify security settings for a Container ## Allows overrides and additional options compared to (Pod) securityContext ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 64Mi # requests: # cpu: 10m # memory: 32Mi service: type: ClusterIP port: 9100 targetPort: 9100 nodePort: portName: metrics listenOnAllInterfaces: true annotations: prometheus.io/scrape: \"true\" # Additional environment variables that will be passed to the daemonset env: {} ## env: ## VARIABLE: value prometheus: monitor: enabled: false additionalLabels: {} namespace: \"\" jobLabel: \"\" # List of pod labels to add to node exporter metrics # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor podTargetLabels: [] scheme: http basicAuth: {} bearerTokenFile: tlsConfig: {} ## proxyUrl: URL of a proxy that should be used for scraping. ## proxyUrl: \"\" ## Override serviceMonitor selector ## selectorOverride: {} relabelings: [] metricRelabelings: [] interval: \"\" scrapeTimeout: 10s ## prometheus.monitor.apiVersion ApiVersion for the serviceMonitor Resource(defaults to \"monitoring.coreos.com/v1\") apiVersion: \"\" ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. ## sampleLimit: 0 ## TargetLimit defines a limit on the number of scraped targets that will be accepted. ## targetLimit: 0 ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelLimit: 0 ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelNameLengthLimit: 0 ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer. ## labelValueLengthLimit: 0 # PodMonitor defines monitoring for a set of pods. # ref. https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.PodMonitor # Using a PodMonitor may be preferred in some environments where there is very large number # of Node Exporter endpoints (1000+) behind a single service. # The PodMonitor is disabled by default. When switching from ServiceMonitor to PodMonitor, # the time series resulting from the configuration through PodMonitor may have different labels. # For instance, there will not be the service label any longer which might # affect PromQL queries selecting that label. podMonitor: enabled: false # Namespace in which to deploy the pod monitor. Defaults to the release namespace. namespace: \"\" # Additional labels, e.g. setting a label for pod monitor selector as set in prometheus additionalLabels: {} # release: kube-prometheus-stack # PodTargetLabels transfers labels of the Kubernetes Pod onto the target. podTargetLabels: [] # apiVersion defaults to monitoring.coreos.com/v1. apiVersion: \"\" # Override pod selector to select pod objects. selectorOverride: {} # Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above. attachMetadata: node: false # The label to use to retrieve the job name from. Defaults to label app.kubernetes.io/name. jobLabel: \"\" # Scheme/protocol to use for scraping. scheme: \"http\" # Path to scrape metrics at. path: \"/metrics\" # BasicAuth allow an endpoint to authenticate over basic authentication. # More info: https://prometheus.io/docs/operating/configuration/#endpoint basicAuth: {} # Secret to mount to read bearer token for scraping targets. # The secret needs to be in the same namespace as the pod monitor and accessible by the Prometheus Operator. # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#secretkeyselector-v1-core bearerTokenSecret: {} # TLS configuration to use when scraping the endpoint. tlsConfig: {} # Authorization section for this endpoint. # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.SafeAuthorization authorization: {} # OAuth2 for the URL. Only valid in Prometheus versions 2.27.0 and newer. # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.OAuth2 oauth2: {} # ProxyURL eg http://proxyserver:2195. Directs scrapes through proxy to this endpoint. proxyUrl: \"\" # Interval at which endpoints should be scraped. If not specified Prometheus global scrape interval is used. interval: \"\" # Timeout after which the scrape is ended. If not specified, the Prometheus global scrape interval is used. scrapeTimeout: \"\" # HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data. honorTimestamps: true # HonorLabels chooses the metrics labels on collisions with target labels. honorLabels: true # Whether to enable HTTP2. Default false. enableHttp2: \"\" # Drop pods that are not running. (Failed, Succeeded). # Enabled by default. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase filterRunning: \"\" # FollowRedirects configures whether scrape requests follow HTTP 3xx redirects. Default false. followRedirects: \"\" # Optional HTTP URL parameters params: {} # RelabelConfigs to apply to samples before scraping. Prometheus Operator automatically adds # relabelings for a few standard Kubernetes fields. The original scrape jobs name # is available via the __tmp_prometheus_job_name label. # More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config relabelings: [] # MetricRelabelConfigs to apply to samples before ingestion. metricRelabelings: [] # SampleLimit defines per-scrape limit on number of scraped samples that will be accepted. sampleLimit: 0 # TargetLimit defines a limit on the number of scraped targets that will be accepted. targetLimit: 0 # Per-scrape limit on number of labels that will be accepted for a sample. # Only valid in Prometheus versions 2.27.0 and newer. labelLimit: 0 # Per-scrape limit on length of labels name that will be accepted for a sample. # Only valid in Prometheus versions 2.27.0 and newer. labelNameLengthLimit: 0 # Per-scrape limit on length of labels value that will be accepted for a sample. # Only valid in Prometheus versions 2.27.0 and newer. labelValueLengthLimit: 0 ## Customize the updateStrategy if set updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 200m # memory: 50Mi # requests: # cpu: 100m # memory: 30Mi serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: annotations: {} imagePullSecrets: [] automountServiceAccountToken: false securityContext: fsGroup: 65534 runAsGroup: 65534 runAsNonRoot: true runAsUser: 65534 containerSecurityContext: {} # capabilities: # add: # - SYS_TIME rbac: ## If true, create &amp; use RBAC resources ## create: true ## If true, create &amp; use Pod Security Policy resources ## https://kubernetes.io/docs/concepts/policy/pod-security-policy/ pspEnabled: true pspAnnotations: {} # for deployments that have node_exporter deployed outside of the cluster, list # their addresses here endpoints: [] # Expose the service to the host network hostNetwork: true # Share the host process ID namespace hostPID: true # Mount the node's root file system (/) at /host/root in the container hostRootFsMount: enabled: true # Defines how new mounts in existing mounts on the node or in the container # are propagated to the container or node, respectively. Possible values are # None, HostToContainer, and Bidirectional. If this field is omitted, then # None is used. More information on: # https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation mountPropagation: HostToContainer ## Assign a group of affinity scheduling rules ## affinity: {} # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchFields: # - key: metadata.name # operator: In # values: # - target-host-name # Annotations to be added to node exporter pods podAnnotations: # Fix for very slow GKE cluster upgrades cluster-autoscaler.kubernetes.io/safe-to-evict: \"true\" # Extra labels to be added to node exporter pods podLabels: {} # Annotations to be added to node exporter daemonset daemonsetAnnotations: {} ## set to true to add the release label so scraping of the servicemonitor with kube-prometheus-stack works out of the box releaseLabel: false # Custom DNS configuration to be added to prometheus-node-exporter pods dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 ## Assign a nodeSelector if operating a hybrid cluster ## nodeSelector: {} # beta.kubernetes.io/arch: amd64 # beta.kubernetes.io/os: linux tolerations: - effect: NoSchedule operator: Exists ## Assign a PriorityClassName to pods if set # priorityClassName: \"\" ## Additional container arguments ## extraArgs: [] # - --collector.diskstats.ignored-devices=^(ram|loop|fd|(h|s|v)d[a-z]|nvme\\\\d+n\\\\d+p)\\\\d+$ # - --collector.textfile.directory=/run/prometheus ## Additional mounts from the host to node-exporter container ## extraHostVolumeMounts: [] # - name: &lt;mountName&gt; # hostPath: &lt;hostPath&gt; # mountPath: &lt;mountPath&gt; # readOnly: true|false # mountPropagation: None|HostToContainer|Bidirectional ## Additional configmaps to be mounted. ## configmaps: [] # - name: &lt;configMapName&gt; # mountPath: &lt;mountPath&gt; secrets: [] # - name: &lt;secretName&gt; # mountPath: &lt;mountPatch&gt; ## Override the deployment namespace ## namespaceOverride: \"\" ## Additional containers for export metrics to text file ## sidecars: [] ## - name: nvidia-dcgm-exporter ## image: nvidia/dcgm-exporter:1.4.3 ## Volume for sidecar containers ## sidecarVolumeMount: [] ## - name: collector-textfiles ## mountPath: /run/prometheus ## readOnly: false ## Additional mounts from the host to sidecar containers ## sidecarHostVolumeMounts: [] # - name: &lt;mountName&gt; # hostPath: &lt;hostPath&gt; # mountPath: &lt;mountPath&gt; # readOnly: true|false # mountPropagation: None|HostToContainer|Bidirectional ## Additional InitContainers to initialize the pod ## extraInitContainers: [] ## Liveness probe ## livenessProbe: failureThreshold: 3 httpGet: httpHeaders: [] scheme: http initialDelaySeconds: 0 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ## Readiness probe ## readinessProbe: failureThreshold: 3 httpGet: httpHeaders: [] scheme: http initialDelaySeconds: 0 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 # Enable vertical pod autoscaler support for prometheus-node-exporter verticalPodAutoscaler: enabled: false # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory controlledResources: [] # Define the max allowed resources for the pod maxAllowed: {} # cpu: 200m # memory: 100Mi # Define the min allowed resources for the pod minAllowed: {} # cpu: 200m # memory: 100Mi # updatePolicy: # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\". # updateMode: Auto . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-node-exporter#4140",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-node-exporter#4140"
  },"48": {
    "doc": "prometheus-operator-crds",
    "title": "prometheus-operator-crds",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-operator-crds",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-operator-crds"
  },"49": {
    "doc": "prometheus-operator-crds",
    "title": "2.0.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . ## Annotations for CRDs ## annotations: {} . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-operator-crds#200",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-operator-crds#200"
  },"50": {
    "doc": "prometheus-pingdom-exporter",
    "title": "prometheus-pingdom-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-pingdom-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-pingdom-exporter"
  },"51": {
    "doc": "prometheus-pingdom-exporter",
    "title": "2.4.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-pingdom-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: # we use camptocamp/prometheus-pingdom-exporter image as giantswarm did not publish recent versions after 0.1.1 repository: camptocamp/prometheus-pingdom-exporter tag: 20190610-1 pullPolicy: IfNotPresent nameOverride: \"\" fullnameOverride: \"\" service: type: ClusterIP port: 9100 annotations: {} # prometheus.io/scrape: \"true\" # prometheus.io/port: \"9100\" resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} tolerations: [] affinity: {} # configuration of the pingdom credentials pingdom: # username of the pingdom account user: somebody@invalid # password of the pingdom account password: totallysecret # application id / api secret can be created on the pingdom website appId: alsototallysecret # account email of the account owner if using multiaccount / team accounts accountEmail: somebodyorelse@invalid # time (in seconds) between accessing the Pingdom API wait: 10 pod: annotations: {} # key: \"true\" # example: \"false\" existingSecret: name: \"\" secret: annotations: {} # key: \"true\" # example: \"false\" . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-pingdom-exporter#241",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-pingdom-exporter#241"
  },"52": {
    "doc": "prometheus-postgres-exporter",
    "title": "prometheus-postgres-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-postgres-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-postgres-exporter"
  },"53": {
    "doc": "prometheus-postgres-exporter",
    "title": "4.3.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-postgres-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: quay.io/prometheuscommunity/postgres-exporter tag: v0.11.1 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName command: [] service: type: ClusterIP port: 80 targetPort: 9187 name: http labels: {} annotations: {} serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false # Set the namespace the ServiceMonitor should be deployed # namespace: monitoring # Set how frequently Prometheus should scrape # interval: 30s # Set path to cloudwatch-exporter telemtery-path # telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator # labels: # Set timeout for scrape # timeout: 10s # Set of labels to transfer from the Kubernetes Service onto the target # targetLabels: [] # MetricRelabelConfigs to apply to samples before ingestion # metricRelabelings: [] # Set relabel_configs as per https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config # relabelings: [] prometheusRule: enabled: false additionalLabels: {} namespace: \"\" rules: [] ## These are just examples rules, please adapt them to your needs. ## Make sure to constraint the rules to the current prometheus-postgres-exporter service. # - alert: HugeReplicationLag # expr: pg_replication_lag{service=\"{{ template \"prometheus-postgres-exporter.fullname\" . }}\"} / 3600 &gt; 1 # for: 1m # labels: # severity: critical # annotations: # description: replication for {{ template \"prometheus-postgres-exporter.fullname\" . }} PostgreSQL is lagging by {{ \"{{ $value }}\" }} hour(s). # summary: PostgreSQL replication is lagging by {{ \"{{ $value }}\" }} hour(s). resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi rbac: # Specifies whether RBAC resources should be created create: true # Specifies whether a PodSecurityPolicy should be created pspEnabled: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: # Add annotations to the ServiceAccount, useful for EKS IAM Roles for Service Accounts or Google Workload Identity. annotations: {} # Add a default ingress to allow namespace access to service.targetPort # Helpful if other NetworkPolicies are configured in the namespace networkPolicy: # Specifies whether a NetworkPolicy should be created enabled: false # Set labels for the NetworkPolicy labels: {} # The securityContext of the pod. # See https://kubernetes.io/docs/concepts/policy/security-context/ for more. podSecurityContext: {} # runAsUser: 1001 # runAsGroup: 1001 # runAsNonRoot: true # seccompProfile: # type: RuntimeDefault # The securityContext of the container. # See https://kubernetes.io/docs/concepts/policy/security-context/ for more. securityContext: {} # runAsUser: 1001 # runAsGroup: 1001 # readOnlyRootFilesystem: true # runAsNonRoot: true # allowPrivilegeEscalation: false # capabilities: # drop: [\"ALL\"] # seccompProfile: # type: RuntimeDefault hostAliases: [] # Set Host Aliases as per https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/ # - ip: \"127.0.0.1\" # hostnames: # - \"foo.local\" # - \"bar.local\" config: datasource: # Specify one of both datasource or datasourceSecret host: user: postgres # Only one of password, passwordFile, passwordSecret and pgpassfile can be specified password: # Specify passwordFile if DB password is stored in a file. # For example, to use with vault-injector from Hashicorp passwordFile: '' # Specify passwordSecret if DB password is stored in secret. passwordSecret: {} # Secret name # name: # Password key inside secret # key: pgpassfile: '' # If pgpassfile is set, it is used to initialize the PGPASSFILE environment variable. # See https://www.postgresql.org/docs/14/libpq-pgpass.html for more info. port: \"5432\" database: '' sslmode: disable extraParams: '' datasourceSecret: {} # Specifies if datasource should be sourced from secret value in format: postgresql://login:password@hostname:port/dbname?sslmode=disable # Multiple Postgres databases can be configured by comma separated postgres connection strings # Secret name # name: # Connection string key inside secret # key: disableCollectorDatabase: false disableCollectorBgwriter: false disableDefaultMetrics: false disableSettingsMetrics: false autoDiscoverDatabases: false excludeDatabases: [] includeDatabases: [] constantLabels: {} # possible values debug, info, warn, error, fatal logLevel: \"\" # possible values logfmt, json logFormat: \"\" extraArgs: [] # Enable queries from an external configmap, enable it will disable inline queries below externalQueries: enabled: false configmap: postgresql-common-exporter-queries # These are the default queries that the exporter will run, extracted from: https://github.com/prometheus-community/postgres_exporter/blob/master/queries.yaml queries: |- pg_replication: query: \"SELECT CASE WHEN NOT pg_is_in_recovery() THEN 0 ELSE GREATEST (0, EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))) END AS lag\" master: true metrics: - lag: usage: \"GAUGE\" description: \"Replication lag behind master in seconds\" pg_postmaster: query: \"SELECT pg_postmaster_start_time as start_time_seconds from pg_postmaster_start_time()\" master: true metrics: - start_time_seconds: usage: \"GAUGE\" description: \"Time at which postmaster started\" pg_stat_user_tables: query: | SELECT current_database() datname, schemaname, relname, seq_scan, seq_tup_read, idx_scan, idx_tup_fetch, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd, n_live_tup, n_dead_tup, n_mod_since_analyze, COALESCE(last_vacuum, '1970-01-01Z') as last_vacuum, COALESCE(last_autovacuum, '1970-01-01Z') as last_autovacuum, COALESCE(last_analyze, '1970-01-01Z') as last_analyze, COALESCE(last_autoanalyze, '1970-01-01Z') as last_autoanalyze, vacuum_count, autovacuum_count, analyze_count, autoanalyze_count FROM pg_stat_user_tables metrics: - datname: usage: \"LABEL\" description: \"Name of current database\" - schemaname: usage: \"LABEL\" description: \"Name of the schema that this table is in\" - relname: usage: \"LABEL\" description: \"Name of this table\" - seq_scan: usage: \"COUNTER\" description: \"Number of sequential scans initiated on this table\" - seq_tup_read: usage: \"COUNTER\" description: \"Number of live rows fetched by sequential scans\" - idx_scan: usage: \"COUNTER\" description: \"Number of index scans initiated on this table\" - idx_tup_fetch: usage: \"COUNTER\" description: \"Number of live rows fetched by index scans\" - n_tup_ins: usage: \"COUNTER\" description: \"Number of rows inserted\" - n_tup_upd: usage: \"COUNTER\" description: \"Number of rows updated\" - n_tup_del: usage: \"COUNTER\" description: \"Number of rows deleted\" - n_tup_hot_upd: usage: \"COUNTER\" description: \"Number of rows HOT updated (i.e., with no separate index update required)\" - n_live_tup: usage: \"GAUGE\" description: \"Estimated number of live rows\" - n_dead_tup: usage: \"GAUGE\" description: \"Estimated number of dead rows\" - n_mod_since_analyze: usage: \"GAUGE\" description: \"Estimated number of rows changed since last analyze\" - last_vacuum: usage: \"GAUGE\" description: \"Last time at which this table was manually vacuumed (not counting VACUUM FULL)\" - last_autovacuum: usage: \"GAUGE\" description: \"Last time at which this table was vacuumed by the autovacuum daemon\" - last_analyze: usage: \"GAUGE\" description: \"Last time at which this table was manually analyzed\" - last_autoanalyze: usage: \"GAUGE\" description: \"Last time at which this table was analyzed by the autovacuum daemon\" - vacuum_count: usage: \"COUNTER\" description: \"Number of times this table has been manually vacuumed (not counting VACUUM FULL)\" - autovacuum_count: usage: \"COUNTER\" description: \"Number of times this table has been vacuumed by the autovacuum daemon\" - analyze_count: usage: \"COUNTER\" description: \"Number of times this table has been manually analyzed\" - autoanalyze_count: usage: \"COUNTER\" description: \"Number of times this table has been analyzed by the autovacuum daemon\" pg_statio_user_tables: query: \"SELECT current_database() datname, schemaname, relname, heap_blks_read, heap_blks_hit, idx_blks_read, idx_blks_hit, toast_blks_read, toast_blks_hit, tidx_blks_read, tidx_blks_hit FROM pg_statio_user_tables\" metrics: - datname: usage: \"LABEL\" description: \"Name of current database\" - schemaname: usage: \"LABEL\" description: \"Name of the schema that this table is in\" - relname: usage: \"LABEL\" description: \"Name of this table\" - heap_blks_read: usage: \"COUNTER\" description: \"Number of disk blocks read from this table\" - heap_blks_hit: usage: \"COUNTER\" description: \"Number of buffer hits in this table\" - idx_blks_read: usage: \"COUNTER\" description: \"Number of disk blocks read from all indexes on this table\" - idx_blks_hit: usage: \"COUNTER\" description: \"Number of buffer hits in all indexes on this table\" - toast_blks_read: usage: \"COUNTER\" description: \"Number of disk blocks read from this table's TOAST table (if any)\" - toast_blks_hit: usage: \"COUNTER\" description: \"Number of buffer hits in this table's TOAST table (if any)\" - tidx_blks_read: usage: \"COUNTER\" description: \"Number of disk blocks read from this table's TOAST table indexes (if any)\" - tidx_blks_hit: usage: \"COUNTER\" description: \"Number of buffer hits in this table's TOAST table indexes (if any)\" # WARNING: This set of metrics can be very expensive on a busy server as every unique query executed will create an additional time series pg_stat_statements: query: \"SELECT t2.rolname, t3.datname, queryid, calls, ( total_plan_time + total_exec_time ) / 1000 as total_time_seconds, ( min_plan_time + min_exec_time ) / 1000 as min_time_seconds, ( max_plan_time + max_exec_time ) / 1000 as max_time_seconds, ( mean_plan_time + mean_exec_time ) / 1000 as mean_time_seconds, ( stddev_plan_time + stddev_exec_time ) / 1000 as stddev_time_seconds, rows, shared_blks_hit, shared_blks_read, shared_blks_dirtied, shared_blks_written, local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written, temp_blks_read, temp_blks_written, blk_read_time / 1000 as blk_read_time_seconds, blk_write_time / 1000 as blk_write_time_seconds FROM pg_stat_statements t1 JOIN pg_roles t2 ON (t1.userid=t2.oid) JOIN pg_database t3 ON (t1.dbid=t3.oid) WHERE t2.rolname != 'rdsadmin' AND queryid IS NOT NULL\" master: true metrics: - rolname: usage: \"LABEL\" description: \"Name of user\" - datname: usage: \"LABEL\" description: \"Name of database\" - queryid: usage: \"LABEL\" description: \"Query ID\" - calls: usage: \"COUNTER\" description: \"Number of times executed\" - total_time_seconds: usage: \"COUNTER\" description: \"Total time spent in the statement, in milliseconds\" - min_time_seconds: usage: \"GAUGE\" description: \"Minimum time spent in the statement, in milliseconds\" - max_time_seconds: usage: \"GAUGE\" description: \"Maximum time spent in the statement, in milliseconds\" - mean_time_seconds: usage: \"GAUGE\" description: \"Mean time spent in the statement, in milliseconds\" - stddev_time_seconds: usage: \"GAUGE\" description: \"Population standard deviation of time spent in the statement, in milliseconds\" - rows: usage: \"COUNTER\" description: \"Total number of rows retrieved or affected by the statement\" - shared_blks_hit: usage: \"COUNTER\" description: \"Total number of shared block cache hits by the statement\" - shared_blks_read: usage: \"COUNTER\" description: \"Total number of shared blocks read by the statement\" - shared_blks_dirtied: usage: \"COUNTER\" description: \"Total number of shared blocks dirtied by the statement\" - shared_blks_written: usage: \"COUNTER\" description: \"Total number of shared blocks written by the statement\" - local_blks_hit: usage: \"COUNTER\" description: \"Total number of local block cache hits by the statement\" - local_blks_read: usage: \"COUNTER\" description: \"Total number of local blocks read by the statement\" - local_blks_dirtied: usage: \"COUNTER\" description: \"Total number of local blocks dirtied by the statement\" - local_blks_written: usage: \"COUNTER\" description: \"Total number of local blocks written by the statement\" - temp_blks_read: usage: \"COUNTER\" description: \"Total number of temp blocks read by the statement\" - temp_blks_written: usage: \"COUNTER\" description: \"Total number of temp blocks written by the statement\" - blk_read_time_seconds: usage: \"COUNTER\" description: \"Total time the statement spent reading blocks, in milliseconds (if track_io_timing is enabled, otherwise zero)\" - blk_write_time_seconds: usage: \"COUNTER\" description: \"Total time the statement spent writing blocks, in milliseconds (if track_io_timing is enabled, otherwise zero)\" pg_stat_activity_idle: query: | WITH metrics AS ( SELECT application_name, SUM(EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - state_change))::bigint)::float AS process_seconds_sum, COUNT(*) AS process_seconds_count FROM pg_stat_activity WHERE state = 'idle' GROUP BY application_name ), buckets AS ( SELECT application_name, le, SUM( CASE WHEN EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - state_change)) &lt;= le THEN 1 ELSE 0 END )::bigint AS bucket FROM pg_stat_activity, UNNEST(ARRAY[1, 2, 5, 15, 30, 60, 90, 120, 300]) AS le GROUP BY application_name, le ORDER BY application_name, le ) SELECT application_name, process_seconds_sum, process_seconds_count, ARRAY_AGG(le) AS process_seconds, ARRAY_AGG(bucket) AS process_seconds_bucket FROM metrics JOIN buckets USING (application_name) GROUP BY 1, 2, 3 metrics: - application_name: usage: \"LABEL\" description: \"Application Name\" - process_seconds: usage: \"HISTOGRAM\" description: \"Idle time of server processes\" # These are user-specified queries that are deep merged with the queries above userQueries: \"\" nodeSelector: {} tolerations: [] affinity: {} annotations: {} podLabels: {} # Configurable health checks livenessProbe: initialDelaySeconds: 0 timeoutSeconds: 1 readinessProbe: initialDelaySeconds: 0 timeoutSeconds: 1 # ExtraEnvs extraEnvs: [] # - name: EXTRA_ENV # value: value # - name: POD_NAMESPACE # valueFrom: # fieldRef: # fieldPath: metadata.namespace # Init containers, e. g. for secrets creation before the exporter initContainers: [] # - name: # image: # volumeMounts: # - name: creds # mountPath: /creds # Additional sidecar containers, e. g. for a database proxy, such as Google's cloudsql-proxy extraContainers: [] # Additional volumes, e. g. for secrets used in an extraContainer extraVolumes: [] # Uncomment for mounting custom ca-certificates # - name: ssl-certs # secret: # defaultMode: 420 # items: # - key: ca-certificates.crt # path: ca-certificates.crt # secretName: ssl-certs # Additional volume mounts extraVolumeMounts: [] # Uncomment for mounting custom ca-certificates file into container # - name: ssl-certs # mountPath: /etc/ssl/certs/ca-certificates.crt # subPath: ca-certificates.crt podDisruptionBudget: enabled: false maxUnavailable: 1 . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-postgres-exporter#430",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-postgres-exporter#430"
  },"54": {
    "doc": "prometheus-pushgateway",
    "title": "prometheus-pushgateway",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-pushgateway",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-pushgateway"
  },"55": {
    "doc": "prometheus-pushgateway",
    "title": "2.1.3",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-pushgateway. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Provide a name in place of prometheus-pushgateway for `app:` labels nameOverride: \"\" # Provide a name to substitute for the full names of resources fullnameOverride: \"\" # Provide a namespace to substitude for the namespace on resources namespaceOverride: \"\" image: repository: prom/pushgateway # if not set appVersion field from Chart.yaml is used tag: \"\" pullPolicy: IfNotPresent # Optional pod imagePullSecrets imagePullSecrets: [] service: type: ClusterIP port: 9091 targetPort: 9091 # nodePort: 32100 # Optional - Can be used for headless if value is \"None\" clusterIP: \"\" loadBalancerIP: \"\" loadBalancerSourceRanges: [] # Optional pod annotations podAnnotations: {} # Optional pod labels podLabels: {} # Optional service annotations serviceAnnotations: {} # Optional service labels serviceLabels: {} # Optional serviceAccount labels serviceAccountLabels: {} # Optional persistentVolume labels persistentVolumeLabels: {} # Optional additional environment variables extraVars: [] ## Additional pushgateway container arguments ## ## example: ## extraArgs: ## - --persistence.file=/data/pushgateway.data ## - --persistence.interval=5m extraArgs: [] ## Additional InitContainers to initialize the pod ## extraInitContainers: [] # Optional additional containers (sidecar) extraContainers: [] # - name: oAuth2-proxy # args: # - -https-address=:9092 # - -upstream=http://localhost:9091 # - -skip-auth-regex=^/metrics # - -openshift-delegate-urls={\"/\":{\"group\":\"monitoring.coreos.com\",\"resource\":\"prometheuses\",\"verb\":\"get\"}} # image: openshift/oauth-proxy:v1.1.0 # ports: # - containerPort: 9092 # name: proxy # resources: # limits: # memory: 16Mi # requests: # memory: 4Mi # cpu: 20m # volumeMounts: # - mountPath: /etc/prometheus/secrets/pushgateway-tls # name: secret-pushgateway-tls resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 200m # memory: 50Mi # requests: # cpu: 100m # memory: 30Mi liveness: enabled: true probe: httpGet: path: /-/healthy port: 9091 initialDelaySeconds: 10 timeoutSeconds: 10 readiness: enabled: true probe: httpGet: path: /-/ready port: 9091 initialDelaySeconds: 10 timeoutSeconds: 10 serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: ## Configure ingress resource that allow you to access the ## pushgateway installation. Set up the URL ## ref: http://kubernetes.io/docs/user-guide/ingress/ ## ingress: ## Enable Ingress. ## enabled: false # AWS ALB requires path of /* className: \"\" path: / pathType: ImplementationSpecific ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services. extraPaths: [] # - path: /* # backend: # serviceName: ssl-redirect # servicePort: use-annotation ## Annotations. ## # annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## Hostnames. ## Must be provided if Ingress is enabled. ## # hosts: # - pushgateway.domain.com ## TLS configuration. ## Secrets must be manually created in the namespace. ## # tls: # - secretName: pushgateway-tls # hosts: # - pushgateway.domain.com tolerations: {} # - effect: NoSchedule # operator: Exists ## Node labels for pushgateway pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} replicaCount: 1 ## When running more than one replica alongside with persistence, different volumes are needed ## per replica, since sharing a `persistence.file` across replicas does not keep metrics synced. ## For this purpose, you can enable the `runAsStatefulSet` to deploy the pushgateway as a ## StatefulSet instead of as a Deployment. runAsStatefulSet: false ## Security context to be added to push-gateway pods ## securityContext: fsGroup: 65534 runAsUser: 65534 runAsNonRoot: true ## Security context to be added to push-gateway containers ## Having a separate variable as securityContext differs for pods and containers. containerSecurityContext: {} # allowPrivilegeEscalation: false # readOnlyRootFilesystem: true # runAsUser: 65534 # runAsNonRoot: true ## Affinity for pod assignment ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: {} ## Topology spread constraints for pods ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ topologySpreadConstraints: [] # Enable this if you're using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: false namespace: monitoring # Fallback to the prometheus default unless specified # interval: 10s ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS. # scheme: \"\" ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS. ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig # tlsConfig: {} # bearerTokenFile: # Fallback to the prometheus default unless specified # scrapeTimeout: 30s ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec additionalLabels: {} # Retain the job and instance labels of the metrics pushed to the Pushgateway # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape) honorLabels: true ## Metric relabel configs to apply to samples before ingestion. ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs) metricRelabelings: [] # - action: keep # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+' # sourceLabels: [__name__] ## Relabel configs to apply to samples before ingestion. ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) relabelings: [] # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace # The values to set in the PodDisruptionBudget spec (minAvailable/maxUnavailable) # If not set then a PodDisruptionBudget will not be created podDisruptionBudget: {} priorityClassName: # Deployment Strategy type strategy: type: Recreate persistentVolume: ## If true, pushgateway will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: false ## pushgateway data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## pushgateway data Persistent Volume Claim annotations ## annotations: {} ## pushgateway data Persistent Volume existing claim name ## Requires pushgateway.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \"\" ## pushgateway data Persistent Volume mount root path ## mountPath: /data ## pushgateway data Persistent Volume size ## size: 2Gi ## pushgateway data Persistent Volume Storage Class ## If defined, storageClassName: &lt;storageClass&gt; ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS &amp; OpenStack) ## # storageClass: \"-\" ## Subdirectory of pushgateway data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \"\" extraVolumes: [] # - name: extra # emptyDir: {} extraVolumeMounts: [] # - name: extra # mountPath: /usr/share/extras # readOnly: true # Configuration for clusters with restrictive network policies in place: # - allowAll allows access to the PushGateway from any namespace # - customSelector is a list of pod/namespaceSelectors to allow access from # These options are mutually exclusive and the latter will take precedence. networkPolicy: {} # allowAll: true # customSelectors: # - namespaceSelector: # matchLabels: # type: admin # - podSelector: # matchLabels: # app: myapp . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-pushgateway#213",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-pushgateway#213"
  },"56": {
    "doc": "prometheus-rabbitmq-exporter",
    "title": "prometheus-rabbitmq-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-rabbitmq-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-rabbitmq-exporter"
  },"57": {
    "doc": "prometheus-rabbitmq-exporter",
    "title": "1.4.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-rabbitmq-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: kbudde/rabbitmq-exporter tag: v0.29.0 pullPolicy: IfNotPresent pullSecrets: [] service: type: ClusterIP externalPort: 9419 internalPort: 9419 resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi priorityClassName: \"\" nodeSelector: {} tolerations: [] affinity: {} loglevel: info rabbitmq: url: http://myrabbit:15672 user: guest password: guest # If existingPasswordSecret is set then password is ignored existingPasswordSecret: ~ existingPasswordSecretKey: password capabilities: bert,no_sort include_queues: \".*\" include_vhost: \".*\" skip_queues: \"^$\" skip_verify: \"false\" skip_vhost: \"^$\" exporters: \"exchange,node,overview,queue\" output_format: \"TTY\" timeout: 30 max_queues: 0 excludeMetrics: \"\" ## Additional labels to set in the Deployment object. Together with standard labels from ## the chart additionalLabels: {} podLabels: {} annotations: {} # prometheus.io/scrape: \"true\" # prometheus.io/path: \"/metrics\" # prometheus.io/port: 9419 prometheus: monitor: enabled: false additionalLabels: {} interval: 15s namespace: [] rules: enabled: false additionalLabels: {} serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: annotations: {} . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-rabbitmq-exporter#140",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-rabbitmq-exporter#140"
  },"58": {
    "doc": "prometheus-redis-exporter",
    "title": "prometheus-redis-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-redis-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-redis-exporter"
  },"59": {
    "doc": "prometheus-redis-exporter",
    "title": "5.3.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . rbac: # Specifies whether RBAC resources should be created create: true # Specifies whether a PodSecurityPolicy should be created pspEnabled: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: replicaCount: 1 image: repository: oliver006/redis_exporter pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" extraArgs: {} # global custom labels, applied to all resrouces customLabels: {} securityContext: {} # Additional Environment variables env: {} # - name: REDIS_PASSWORD # valueFrom: # secretKeyRef: # key: redis-password # name: redis-config-0.0.2 service: type: ClusterIP port: 9121 portName: redis-exporter annotations: {} labels: {} # prometheus.io/path: /metrics # prometheus.io/port: \"9121\" # prometheus.io/scrape: \"true\" resources: {} nodeSelector: {} tolerations: [] affinity: {} # If serviceMonitor.multipleTarget is enabled, this configuration is actually not used redisAddress: redis://myredis:6379 # deployment additional annotations and labels annotations: {} labels: {} # prometheus.io/path: /metrics # prometheus.io/port: \"9121\" # prometheus.io/scrape: \"true\" redisAddressConfig: # Use config from configmap enabled: false # Use existing configmap (ignores redisAddress) configmap: name: \"\" key: \"\" redisTlsConfig: # Use TLS configuration enabled: false # Whether to to skip TLS verification skipTlsVerification: false # All secrets key about TLS config will be mounted into this path mountPath: /tls # REDIS_EXPORTER_TLS_CA_CERT_FILE will be set to /tls/tls-ca-cert.crt caCertFile: secret: name: \"\" key: \"\" # REDIS_EXPORTER_TLS_CLIENT_KEY_FILE will be set to /tls/tls-client-key.key clientKeyFile: secret: name: \"\" key: \"\" # REDIS_EXPORTER_TLS_CLIENT_CERT_FILE will be set to /tls/tls-client-cert.crt clientCertFile: secret: name: \"\" key: \"\" # REDIS_EXPORTER_TLS_SERVER_KEY_FILE will be set to /tls/tls-server-key.key serverKeyFile: secret: name: \"\" key: \"\" # REDIS_EXPORTER_TLS_SERVER_CERT_FILE will be set to /tls/tls-server-cert.crt serverCertFile: secret: name: \"\" key: \"\" serviceMonitor: # When set true then use a ServiceMonitor to configure scraping enabled: false multipleTarget: false targets: [] # for every targets, url and name must be set, # an individual additionalRelabeling can be set for every target # - url: \"redis://myredis:6379\" # name: \"my-redis\" # - url: \"redis://my-redis-cluster:6379\" # name: \"bar\" # additionalRelabeling: # - sourceLabels: [type] # targetLabel: type # replacement: cluster # additionalMetricsRelabels: # type: cluster additionalMetricsRelabels: {} additionalRelabeling: [] # Set the namespace the ServiceMonitor should be deployed # namespace: monitoring # Set how frequently Prometheus should scrape # interval: 30s # Set path to redis-exporter telemtery-path # Please set telemetryPath to /scrape if you are using multiple targets # telemetryPath: /metrics # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator # labels: # Set timeout for scrape # timeout: 10s # Set relabel_configs as per https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config # relabelings: [] # Set of labels to transfer on the Kubernetes Service onto the target. # targetLabels: [] # metricRelabelings: [] # Set tls options # scheme: \"\" # tlsConfig: {} ## Custom PrometheusRules to be defined ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule: enabled: false additionalLabels: {} namespace: \"\" rules: [] ## These are just examples rules, please adapt them to your needs. ## Make sure to constraint the rules to the current service. # - alert: RedisDown # expr: redis_up{service=\"{{ template \"prometheus-redis-exporter.fullname\" . }}\"} == 0 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \"{{ $labels.instance }}\" }} down # description: Redis instance {{ \"{{ $labels.instance }}\" }} is down. # - alert: RedisMemoryHigh # expr: &gt; # redis_memory_used_bytes{service=\"{{ template \"prometheus-redis-exporter.fullname\" . }}\"} * 100 # / # redis_memory_max_bytes{service=\"{{ template \"prometheus-redis-exporter.fullname\" . }}\"} # &gt; 90 &lt;= 100 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \"{{ $labels.instance }}\" }} is using too much memory # description: | # Redis instance {{ \"{{ $labels.instance }}\" }} is using {{ \"{{ $value }}\" }}% of its available memory. # - alert: RedisKeyEviction # expr: | # increase(redis_evicted_keys_total{service=\"{{ template \"prometheus-redis-exporter.fullname\" . }}\"}[5m]) &gt; 0 # for: 1s # labels: # severity: error # annotations: # summary: Redis instance {{ \"{{ $labels.instance }}\" }} has evicted keys # description: | # Redis instance {{ \"{{ $labels.instance }}\" }} has evicted {{ \"{{ $value }}\" }} keys in the last 5 minutes. # Used to mount a LUA-Script via config map and use it for metrics-collection # script: # configmap: prometheus-redis-exporter-script # keyname: script auth: # Use password authentication enabled: false # Use existing secret (ignores redisPassword) secret: name: \"\" key: \"\" # Redis password (when not stored in a secret) redisPassword: \"\" # Redis user (version 6.X and above) redisUser: \"\" # Redis password file (e.g., https://github.com/oliver006/redis_exporter/blob/v1.27.0/contrib/sample-pwd-file.json) # secret (useful for multiple redis instances with different passwords). If secret name and key are set # this will ignore the single password auth.secret.* redisPasswordFile: # The secret key will be mounted into this path as a file # e.g., if secret key is pass.json, the env variable # REDIS_PASSWORD_FILE will be set to /auth/pass.json mountPath: /auth secret: name: \"\" key: \"\" . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-redis-exporter#530",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-redis-exporter#530"
  },"60": {
    "doc": "prometheus-smartctl-exporter",
    "title": "prometheus-smartctl-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-smartctl-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-smartctl-exporter"
  },"61": {
    "doc": "prometheus-smartctl-exporter",
    "title": "0.3.1",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . config: {} # devices: # - /dev/sda extraInstances: [] # - config: # devices: # - /dev/nvme0n1 # nodeSelector: # type: other common: config: bind_to: \"0.0.0.0:9633\" url_path: \"/metrics\" smartctl_location: /usr/sbin/smartctl collect_not_more_than_period: 120s serviceMonitor: enabled: false # Specify namespace to load the monitor if not in the same namespace # namespace: prometheus-operator # Add Extra labels if needed. Prometeus operator may need them to find it. extraLabels: {} # release: prometheus-operator prometheusRules: enabled: false # Specify namespace to load the monitor if not in the same namespace # namespace: prometheus-operator # Add Extra labels if needed. Prometeus operator may need them to find it. extraLabels: {} # release: prometheus-operator image: repository: quay.io/prometheuscommunity/smartctl-exporter pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" rbac: create: true podSecurityPolicy: unrestricted-psp resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} tolerations: - effect: NoSchedule operator: Exists affinity: {} service: type: ClusterIP port: 80 . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-smartctl-exporter#031",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-smartctl-exporter#031"
  },"62": {
    "doc": "prometheus-snmp-exporter",
    "title": "prometheus-snmp-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-snmp-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-snmp-exporter"
  },"63": {
    "doc": "prometheus-snmp-exporter",
    "title": "1.4.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . restartPolicy: Always kind: Deployment image: repository: prom/snmp-exporter # if not set appVersion field from Chart.yaml is used tag: \"\" pullPolicy: IfNotPresent imagePullSecrets: [] nodeSelector: {} tolerations: [] affinity: {} ## Security context to be added to snmp-exporter pods securityContext: {} # fsGroup: 1000 # runAsUser: 1000 # runAsNonRoot: true ## Security context to be added to snmp-exporter containers containerSecurityContext: runAsNonRoot: true runAsUser: 1000 readOnlyRootFilesystem: true ## Additional labels to add to all resources customLabels: {} # app: snmp-exporter # config: extraConfigmapMounts: [] # - name: snmp-exporter-configmap # mountPath: /run/secrets/snmp-exporter # subPath: snmp.yaml # (optional) # configMap: snmp-exporter-configmap-configmap # readOnly: true # defaultMode: 420 ## Additional secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /run/secrets/snmp-exporter # secretName: snmp-exporter-secret-files # readOnly: true # defaultMode: 420 ## For RBAC support: rbac: # Specifies whether RBAC resources should be created create: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: resources: {} # limits: # memory: 300Mi # requests: # memory: 50Mi livenessProbe: httpGet: path: /health port: http readinessProbe: httpGet: path: /health port: http service: annotations: {} type: ClusterIP port: 9116 ## An Ingress resource can provide name-based virtual hosting and TLS ## termination among other things for CouchDB deployments which are accessed ## from outside the Kubernetes cluster. ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/ ingress: enabled: false ## Class name can be set since version 1.18. className: \"\" ## Path type is required since version 1.18. Default: ImplementationSpecific. pathType: \"\" hosts: [] # - chart-example.local annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" tls: [] # Secrets must be manually created in the namespace. # - secretName: chart-example-tls # hosts: # - chart-example.local podAnnotations: {} extraArgs: [] # --history.limit=1000 replicas: 1 ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.5.0 pullPolicy: IfNotPresent ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # Enable this if you're using https://github.com/prometheus-operator/prometheus-operator # A service monitor will be created per each item in serviceMonitor.params[] serviceMonitor: enabled: false namespace: monitoring path: /snmp # Fall back to the prometheus default unless specified # interval: 10s scrapeTimeout: 10s module: - if_mib # Relabelings dynamically rewrite the label set of a target before it gets scraped. # Set if defined unless overriden by params.relabelings. # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig relabelings: [] # - sourceLabels: [__address__] # targetLabel: __param_target # - sourceLabels: [__param_target] # targetLabel: instance # Metric relabeling is applied to samples as the last step before ingestion. # Set if defined unless overriden by params.additionalMetricsRelabels. additionalMetricsRelabels: {} # Label for selecting service monitors as set in Prometheus CRD. # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.PrometheusSpec selector: prometheus: kube-prometheus # Retain the job and instance labels of the metrics retrieved by the snmp-exporter # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Endpoint honorLabels: true params: [] # Human readable URL that will appear in Prometheus / AlertManager # - name: localhost # The target that snmp will scrape # target: 127.0.0.1 # Module used for scraping. Overrides value set in serviceMonitor.module # module: # - if_mib # Map of labels for ServiceMonitor. Overrides value set in serviceMonitor.selector # labels: {} # release: kube-prometheus-stack # Scraping interval. Overrides value set in serviceMonitor.interval # interval: 30s # Scrape timeout. Overrides value set in serviceMonitor.scrapeTimeout # scrapeTimeout: 30s # Relabelings. Overrides value set in serviceMonitor.relabelings # relabelings: [] # Map of metric labels and values to add. Overrides value set in serviceMonitor.additionalMetricsRelabels # additionalMetricsRelabels: {} . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-snmp-exporter#140",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-snmp-exporter#140"
  },"64": {
    "doc": "prometheus-stackdriver-exporter",
    "title": "prometheus-stackdriver-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-stackdriver-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-stackdriver-exporter"
  },"65": {
    "doc": "prometheus-stackdriver-exporter",
    "title": "4.2.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Provide a name in place of prometheus-stackdriver-exporter for `app:` labels nameOverride: \"\" # Provide a name to substitute for the full names of resources fullnameOverride: \"\" # Number of exporters to run replicaCount: 1 # Restart policy for container restartPolicy: Always image: repository: prometheuscommunity/stackdriver-exporter tag: v0.13.0 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myDockerConfigJsonSecretName resources: {} # requests: # cpu: 100m # memory: 128Mi # limits: # cpu: 100m # memory: 128Mi securityContext: {} containerSecurityContext: {} service: type: ClusterIP httpPort: 9255 annotations: {} ## Additional labels to add to all resources customLabels: {} # app: prometheus-stackdriver-exporter secret: labels: {} stackdriver: # The Google Project ID to gather metrics for projectId: \"FALSE\" # An existing secret which contains credentials.json serviceAccountSecret: \"\" # Provide custom key for the existing secret to load credentials.json from serviceAccountSecretKey: \"\" # A service account key JSON file. Must be provided when no existing secret is used, in this case a new secret will be created holding this service account serviceAccountKey: \"\" # Max number of retries that should be attempted on 503 errors from Stackdriver maxRetries: 0 # How long should Stackdriver_exporter wait for a result from the Stackdriver API httpTimeout: 10s # Max time between each request in an exp backoff scenario maxBackoff: 5s # The amount of jitter to introduce in an exp backoff scenario backoffJitter: 1s # The HTTP statuses that should trigger a retry retryStatuses: 503 # Drop metrics from attached projects and fetch `project_id` only dropDelegatedProjects: false metrics: # The prefixes to gather metrics for, we default to just CPU metrics. typePrefixes: 'compute.googleapis.com/instance/cpu' # The filters to refine the metrics query by using Filter objects that Google provides. # Filter objects: project, group.id, resource.type, resource.labels.[KEY], metric.type, metric.labels.[KEY] # https://cloud.google.com/monitoring/api/v3/filters filters: [] # - 'pubsub.googleapis.com/subscription:resource.labels.subscription_id=monitoring.regex.full_match(\"us-west4.*my-team.*\")' # The frequency to request interval: '5m' # How far into the past to offset offset: '0s' # Offset for the Google Stackdriver Monitoring Metrics interval into the past by the ingest delay from the metric's metadata. ingestDelay: false # If enabled will treat all DELTA metrics as an in-memory counter instead of a gauge. aggregateDeltas: false # How long should a delta metric continue to be exported after GCP stops producing a metric aggregateDeltasTTL: '30m' web: # Port to listen on listenAddress: ':9255' # Path under which to expose metrics. path: /metrics ## Pod affinity ## affinity: {} annotations: {} ## Pod extra arguments ## extraArgs: {} ## Node labels for stackdriver-exporter pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Node tolerations for stackdriver-exporter scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \"key\" # operator: \"Equal|Exists\" # value: \"value\" # effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\" ## Service Account ## serviceAccount: # Specifies whether a ServiceAccount should be created create: false # The name of the ServiceAccount to use. # If not set and create is false, 'default' is used # If not set and create is true, a name is generated using the fullname template name: annotations: {} # Enable this if you're using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: false namespace: monitoring # additionalLabels is the set of additional labels to add to the ServiceMonitor additionalLabels: {} # How long until a scrape request times out. scrapeTimeout: '10s' # fallback to the prometheus default unless specified # interval: 10s ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr) honorLabels: true ## Whether Prometheus should use the timestamps of the metrics exposed by stackdriver-exporter honorTimestamps: true # MetricRelabelConfigs to apply to samples before ingestion https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig metricRelabelings: [] # RelabelConfigs to apply to samples before scraping. https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig relabelings: [] . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-stackdriver-exporter#420",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-stackdriver-exporter#420"
  },"66": {
    "doc": "prometheus-statsd-exporter",
    "title": "prometheus-statsd-exporter",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-statsd-exporter",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-statsd-exporter"
  },"67": {
    "doc": "prometheus-statsd-exporter",
    "title": "0.7.0",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . # Default values for prometheus-statsd-exporter. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 # deploymentRevisionHistoryLimit: 10 image: repository: prom/statsd-exporter pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \"\" imagePullSecrets: [] nameOverride: \"\" fullnameOverride: \"\" # Additional container environment variables extraEnv: {} # HTTP_PROXY: \"http://superproxy.com:3128\" # NO_PROXY: \"localhost,127.0.0.1\" # Additional arguments in the statsd_exporter command line extraArgs: [] # - --history.limit=1000 statsd: # The UDP port on which to receive statsd metric lines. udpPort: 9125 # The TCP port on which to receive statsd metric lines. tcpPort: 9125 # Maximum size of your metric mapping cache. # Relies on least recently used replacement policy if max size is reached. cacheSize: 1000 # Size of internal queue for processing events. eventQueueSize: 10000 # Number of events to hold in queue before flushing. eventFlushThreshold: 1000 # Time interval before flushing events in queue. eventFlushInterval: 200ms # Metric mapping ConfigMap # mappingConfigMapName: \"\" # Name of the key inside Metric mapping ConfigMap. Default is \"statsd.mappingConf\". # mappingConfigMapKey: \"\" # Metric mapping configuration # mappingConfig: |- livenessProbe: httpGet: path: /health port: http readinessProbe: httpGet: path: /health port: http prometheus: monitor: enabled: false additionalLabels: {} namespace: \"\" interval: 30s # ApiVersion for the podMonitor Resource(defaults to \"monitoring.googleapis.com/v1\") apiVersion: \"\" ## resource kind(defaults to \"PodMonitoring\") kind: \"\" # metrics exposing endpoint endpoint: \"/metrics\" serviceMonitor: enabled: false interval: 30s scrapeTimeout: 10s namespace: monitoring honorLabels: false additionalLabels: {} ## resource kind(defaults to \"ServiceMonitor\") kind: \"\" # MetricRelabelConfigs to apply to samples before ingestion metricRelabelings: [] # RelabelConfigs to apply to samples before scraping. # More info https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config relabelings: [] # ApiVersion for the podMonitor Resource(defaults to \"monitoring.coreos.com/v1\") apiVersion: \"\" # metrics exposing endpoint endpoint: \"/metrics\" serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP # The address on which to expose the web interface and generated Prometheus metrics. port: 9102 # Path under which to expose metrics. path: /metrics annotations: {} ingress: enabled: false className: \"\" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 80 # targetMemoryUtilizationPercentage: 80 nodeSelector: {} tolerations: [] affinity: {} . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-statsd-exporter#070",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-statsd-exporter#070"
  },"68": {
    "doc": "prometheus-to-sd",
    "title": "prometheus-to-sd",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-to-sd",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-to-sd"
  },"69": {
    "doc": "prometheus-to-sd",
    "title": "0.4.2",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . replicaCount: 1 image: repository: gcr.io/google-containers/prometheus-to-sd tag: v0.5.2 pullPolicy: IfNotPresent resources: {} port: 6060 metricsSources: kube-state-metrics: http://kube-state-metrics:8080 # The monitored resource types to use, either the legacy 'gke_container', or the new 'k8s' monitoredResourceTypes: gke_container nodeSelector: {} tolerations: [] . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus-to-sd#042",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus-to-sd#042"
  },"70": {
    "doc": "prometheus",
    "title": "prometheus",
    "content": " ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus"
  },"71": {
    "doc": "prometheus",
    "title": "19.7.2",
    "content": "Release date: 2023-03-04 . | [prometheus-blackbox-exporter] Update PSP-related templates (#3093) | . Default value changes . rbac: create: true podSecurityPolicy: enabled: false imagePullSecrets: [] # - name: \"image-pull-secret\" ## Define serviceAccount names for components. Defaults to component's fully qualified name. ## serviceAccounts: server: create: true name: \"\" annotations: {} ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: prometheus: ## If false, the configmap-reload container will not be deployed ## enabled: true ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.8.0 # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value). digest: \"\" pullPolicy: IfNotPresent # containerPort: 9533 ## Additional configmap-reload container arguments ## extraArgs: {} ## Additional configmap-reload volume directories ## extraVolumeDirs: [] ## Additional configmap-reload mounts ## extraConfigmapMounts: [] # - name: prometheus-alerts # mountPath: /etc/alerts.d # subPath: \"\" # configMap: prometheus-alerts # readOnly: true ## Security context to be added to configmap-reload container containerSecurityContext: {} ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} server: ## Prometheus server container name ## name: server ## Use a ClusterRole (and ClusterRoleBinding) ## - If set to false - we define a RoleBinding in the defined namespaces ONLY ## ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled. ## This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide. ## ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus. ## # useExistingClusterRoleName: nameofclusterrole ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges. # namespaces: # - yournamespace # sidecarContainers - add more containers to prometheus server # Key/Value where Key is the sidecar `- name: &lt;Key&gt;` # Example: # sidecarContainers: # webserver: # image: nginx sidecarContainers: {} # sidecarTemplateValues - context to be used in template for sidecarContainers # Example: # sidecarTemplateValues: *your-custom-globals # sidecarContainers: # webserver: |- # {{ include \"webserver-container-template\" . }} # Template for `webserver-container-template` might looks like this: # image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\" # ... # sidecarTemplateValues: {} ## Prometheus server container image ## image: repository: quay.io/prometheus/prometheus # if not set appVersion field from Chart.yaml is used tag: \"\" # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value). digest: \"\" pullPolicy: IfNotPresent ## prometheus server priorityClassName ## priorityClassName: \"\" ## EnableServiceLinks indicates whether information about services should be injected ## into pod's environment variables, matching the syntax of Docker links. ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0. ## enableServiceLinks: true ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \"\" ## External URL which can access prometheus ## Maybe same with Ingress host name baseURL: \"\" ## Additional server container environment variables ## ## You specify this manually like you would a raw deployment manifest. ## This means you can bind in environment variables from secrets. ## ## e.g. static environment variable: ## - name: DEMO_GREETING ## value: \"Hello from the environment\" ## ## e.g. secret environment variable: ## - name: USERNAME ## valueFrom: ## secretKeyRef: ## name: mysecret ## key: username env: [] # List of flags to override default parameters, e.g: # - --enable-feature=agent # - --storage.agent.retention.max-time=30m defaultFlagsOverride: [] extraFlags: - web.enable-lifecycle ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as ## deleting time series. This is disabled by default. # - web.enable-admin-api ## ## storage.tsdb.no-lockfile flag controls BD locking # - storage.tsdb.no-lockfile ## ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL) # - storage.tsdb.wal-compression ## Path to a configuration file on prometheus server container FS configPath: /etc/config/prometheus.yml ### The data directory used by prometheus to set --storage.tsdb.path ### When empty server.persistentVolume.mountPath is used instead storagePath: \"\" global: ## How frequently to scrape targets by default ## scrape_interval: 1m ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 1m ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write ## remoteWrite: [] ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read ## remoteRead: [] ## Custom HTTP headers for Liveness/Readiness/Startup Probe ## ## Useful for providing HTTP Basic Auth to healthchecks probeHeaders: [] # - name: \"Authorization\" # value: \"Bearer ABCDEabcde12345\" ## Additional Prometheus server container arguments ## extraArgs: {} ## Additional InitContainers to initialize the pod ## extraInitContainers: [] ## Additional Prometheus server Volume mounts ## extraVolumeMounts: [] ## Additional Prometheus server Volumes ## extraVolumes: [] ## Additional Prometheus server hostPath mounts ## extraHostPathMounts: [] # - name: certs-dir # mountPath: /etc/kubernetes/certs # subPath: \"\" # hostPath: /etc/kubernetes/certs # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # subPath: \"\" # configMap: certs-configmap # readOnly: true ## Additional Prometheus server Secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # subPath: \"\" # secretName: prom-secret-files # readOnly: true ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/server-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \"\" ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles) extraConfigmapLabels: {} ingress: ## If true, Prometheus server Ingress will be created ## enabled: false # For Kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressClassName # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress # ingressClassName: nginx ## Prometheus server Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## Prometheus server Ingress additional labels ## extraLabels: {} ## Prometheus server Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - prometheus.domain.com # - domain.com/prometheus path: / # pathType is only for k8s &gt;= 1.18 pathType: Prefix ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services. extraPaths: [] # - path: /* # backend: # serviceName: ssl-redirect # servicePort: use-annotation ## Prometheus server Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-server-tls # hosts: # - prometheus.domain.com ## Server Deployment Strategy type strategy: type: Recreate ## hostAliases allows adding entries to /etc/hosts inside the containers hostAliases: [] # - ip: \"127.0.0.1\" # hostnames: # - \"example.com\" ## Node tolerations for server scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \"key\" # operator: \"Equal|Exists\" # value: \"value\" # effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\" ## Node labels for Prometheus server pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## PodDisruptionBudget settings ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ ## podDisruptionBudget: enabled: false maxUnavailable: 1 ## Use an alternate scheduler, e.g. \"stork\". ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume labels ## labels: {} ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \"\" ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 8Gi ## Prometheus server data Persistent Volume Storage Class ## If defined, storageClassName: &lt;storageClass&gt; ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS &amp; OpenStack) ## # storageClass: \"-\" ## Prometheus server data Persistent Volume Binding Mode ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt; ## If undefined (the default) or set to null, no volumeBindingMode spec is ## set, choosing the default mode. ## # volumeBindingMode: \"\" ## Subdirectory of Prometheus server data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \"\" ## Persistent Volume Claim Selector ## Useful if Persistent Volumes have been provisioned in advance ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector ## # selector: # matchLabels: # release: \"stable\" # matchExpressions: # - { key: environment, operator: In, values: [ dev ] } ## Persistent Volume Name ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one ## # volumeName: \"\" emptyDir: ## Prometheus server emptyDir volume size limit ## sizeLimit: \"\" ## Annotations to be added to Prometheus server pods ## podAnnotations: {} # iam.amazonaws.com/role: prometheus ## Labels to be added to Prometheus server pods ## podLabels: {} ## Prometheus AlertManager configuration ## alertmanagers: [] ## Specify if a Pod Security Policy for node-exporter must be created ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: annotations: {} ## Specify pod annotations ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl ## # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*' # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below) ## replicaCount: 1 ## Annotations to be added to deployment ## deploymentAnnotations: {} statefulSet: ## If true, use a statefulset instead of a deployment for pod management. ## This allows to scale replicas to more than 1 pod ## enabled: false annotations: {} labels: {} podManagementPolicy: OrderedReady ## Alertmanager headless service to use for the statefulset ## headless: annotations: {} labels: {} servicePort: 80 ## Enable gRPC port on service to allow auto discovery with thanos-querier gRPC: enabled: false servicePort: 10901 # nodePort: 10901 ## Prometheus server readiness and liveness probe initial delay and timeout ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ ## tcpSocketProbeEnabled: false probeScheme: HTTP readinessProbeInitialDelay: 30 readinessProbePeriodSeconds: 5 readinessProbeTimeout: 4 readinessProbeFailureThreshold: 3 readinessProbeSuccessThreshold: 1 livenessProbeInitialDelay: 30 livenessProbePeriodSeconds: 15 livenessProbeTimeout: 10 livenessProbeFailureThreshold: 3 livenessProbeSuccessThreshold: 1 startupProbe: enabled: false periodSeconds: 5 failureThreshold: 30 timeoutSeconds: 10 ## Prometheus server resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 500m # memory: 512Mi # requests: # cpu: 500m # memory: 512Mi # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico), # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working ## hostNetwork: false # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically dnsPolicy: ClusterFirst # Use hostPort # hostPort: 9090 ## Vertical Pod Autoscaler config ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler verticalAutoscaler: ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs) enabled: false # updateMode: \"Auto\" # containerPolicies: # - containerName: 'prometheus-server' # Custom DNS configuration to be added to prometheus server pods dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 ## Security context to be added to server pods ## securityContext: runAsUser: 65534 runAsNonRoot: true runAsGroup: 65534 fsGroup: 65534 ## Security context to be added to server container ## containerSecurityContext: {} service: ## If false, no Service will be created for the Prometheus server ## enabled: true annotations: {} labels: {} clusterIP: \"\" ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \"\" loadBalancerSourceRanges: [] servicePort: 80 sessionAffinity: None type: ClusterIP ## Enable gRPC port on service to allow auto discovery with thanos-querier gRPC: enabled: false servicePort: 10901 # nodePort: 10901 ## If using a statefulSet (statefulSet.enabled=true), configure the ## service to connect to a specific replica to have a consistent view ## of the data. statefulsetReplica: enabled: false replica: 0 ## Prometheus server pod termination grace period ## terminationGracePeriodSeconds: 300 ## Prometheus data retention period (default if not specified is 15 days) ## retention: \"15d\" ## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation) ruleFiles: {} ## Prometheus server ConfigMap entries ## serverFiles: ## Alerts configuration ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/ alerting_rules.yml: {} # groups: # - name: Instances # rules: # - alert: InstanceDown # expr: up == 0 # for: 5m # labels: # severity: page # annotations: # description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' # summary: 'Instance {{ $labels.instance }} down' ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml alerts: {} ## Records configuration ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/ recording_rules.yml: {} ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml rules: {} prometheus.yml: rule_files: - /etc/config/recording_rules.yml - /etc/config/alerting_rules.yml ## Below two files are DEPRECATED will be removed from this default values file - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # A scrape configuration for running Prometheus on a Kubernetes cluster. # This uses separate scrape configs for cluster components (i.e. API server, node) # and services to allow each to use different authentication configs. # # Kubernetes labels will be added as Prometheus labels on metrics via the # `labelmap` relabeling action. # Scrape config for API servers. # # Kubernetes exposes API servers as endpoints to the default/kubernetes # service so this uses `endpoints` role and uses relabelling to only keep # the endpoints associated with the default/kubernetes service using the # default named port `https`. This works for single API server deployments as # well as HA API server deployments. - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS &amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery &amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # &lt;kubernetes_sd_config&gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token # Keep only the default/kubernetes service endpoints for the https port. This # will add targets for each API server which Kubernetes adds an endpoint to # the default/kubernetes service. relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS &amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery &amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # &lt;kubernetes_sd_config&gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$1/proxy/metrics - job_name: 'kubernetes-nodes-cadvisor' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS &amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery &amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # &lt;kubernetes_sd_config&gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node # This configuration will work only on kubelet 1.7.3+ # As the scrape endpoints for cAdvisor have changed # if you are using older version you need to change the replacement to # replacement: /api/v1/nodes/$1:4194/proxy/metrics # more info here https://github.com/coreos/prometheus-operator/issues/633 relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor # Metric relabel configs to apply to samples before ingestion. # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs) # metric_relabel_configs: # - action: labeldrop # regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone) # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well. # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` &amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. # * `prometheus.io/param_&lt;parameter&gt;`: If the metrics endpoint uses parameters # then you can set any parameter - job_name: 'kubernetes-service-endpoints' honor_labels: true kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow] action: drop regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+?)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: service - source_labels: [__meta_kubernetes_pod_node_name] action: replace target_label: node # Scrape config for slow service endpoints; same as above, but with a larger # timeout and a larger interval # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` &amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. # * `prometheus.io/param_&lt;parameter&gt;`: If the metrics endpoint uses parameters # then you can set any parameter - job_name: 'kubernetes-service-endpoints-slow' honor_labels: true scrape_interval: 5m scrape_timeout: 30s kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+?)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: service - source_labels: [__meta_kubernetes_pod_node_name] action: replace target_label: node - job_name: 'prometheus-pushgateway' honor_labels: true kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: pushgateway # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' honor_labels: true metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: namespace - source_labels: [__meta_kubernetes_service_name] target_label: service # Example scrape config for pods # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`, # except if `prometheus.io/scrape-slow` is set to `true` as well. # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` &amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' honor_labels: true kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow] action: drop regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme] action: replace regex: (https?) target_label: __scheme__ - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] action: replace regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4}) replacement: '[$2]:$1' target_label: __address__ - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] action: replace regex: (\\d+);((([0-9]+?)(\\.|$)){4}) replacement: $2:$1 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod - source_labels: [__meta_kubernetes_pod_phase] regex: Pending|Succeeded|Failed|Completed action: drop # Example Scrape config for pods which should be scraped slower. An useful example # would be stackriver-exporter which queries an API on every scrape of the pod # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` &amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods-slow' honor_labels: true scrape_interval: 5m scrape_timeout: 30s kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme] action: replace regex: (https?) target_label: __scheme__ - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] action: replace regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4}) replacement: '[$2]:$1' target_label: __address__ - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] action: replace regex: (\\d+);((([0-9]+?)(\\.|$)){4}) replacement: $2:$1 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod - source_labels: [__meta_kubernetes_pod_phase] regex: Pending|Succeeded|Failed|Completed action: drop # adds additional scrape configs to prometheus.yml # must be a string so you have to add a | after extraScrapeConfigs: # example adds prometheus-blackbox-exporter scrape config extraScrapeConfigs: \"\" # - job_name: 'prometheus-blackbox-exporter' # metrics_path: /probe # params: # module: [http_2xx] # static_configs: # - targets: # - https://example.com # relabel_configs: # - source_labels: [__address__] # target_label: __param_target # - source_labels: [__param_target] # target_label: instance # - target_label: __address__ # replacement: prometheus-blackbox-exporter:9115 # Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager # useful in H/A prometheus with different external labels but the same alerts alertRelabelConfigs: {} # alert_relabel_configs: # - source_labels: [dc] # regex: (.+)\\d+ # target_label: dc networkPolicy: ## Enable creation of NetworkPolicy resources. ## enabled: false # Force namespace of namespaced resources forceNamespace: \"\" # Extra manifests to deploy as an array extraManifests: [] # - apiVersion: v1 # kind: ConfigMap # metadata: # labels: # name: prometheus-extra # data: # extra-data: \"value\" # Configuration of subcharts defined in Chart.yaml ## alertmanager sub-chart configurable values ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager ## alertmanager: ## If false, alertmanager will not be installed ## enabled: true persistence: size: 2Gi podSecurityContext: runAsUser: 65534 runAsNonRoot: true runAsGroup: 65534 fsGroup: 65534 ## kube-state-metrics sub-chart configurable values ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics ## kube-state-metrics: ## If false, kube-state-metrics sub-chart will not be installed ## enabled: true ## promtheus-node-exporter sub-chart configurable values ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter ## prometheus-node-exporter: ## If false, node-exporter will not be installed ## enabled: true rbac: pspEnabled: false containerSecurityContext: allowPrivilegeEscalation: false ## pprometheus-pushgateway sub-chart configurable values ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway ## prometheus-pushgateway: ## If false, pushgateway will not be installed ## enabled: true # Optional service annotations serviceAnnotations: prometheus.io/probe: pushgateway . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/prometheus-community/helm-charts/prometheus#1972",
    
    "relUrl": "/prometheus-community/helm-charts/prometheus#1972"
  },"72": {
    "doc": "velero",
    "title": "velero",
    "content": " ",
    "url": "/changelogs/vmware-tanzu/helm-charts/velero",
    
    "relUrl": "/vmware-tanzu/helm-charts/velero"
  },"73": {
    "doc": "velero",
    "title": "3.1.2",
    "content": "Release date: 2023-01-30 . | [velero] fix: remove check on CRDs availability (#430) | . Default value changes . ## ## Configuration settings that directly affect the Velero deployment YAML. ## # Details of the container image to use in the Velero deployment &amp; daemonset (if # enabling node-agent). Required. image: repository: velero/velero tag: v1.10.0 # Digest value example: sha256:d238835e151cec91c6a811fe3a89a66d3231d9f64d09e5f3c49552672d271f38. # If used, it will take precedence over the image.tag. # digest: pullPolicy: IfNotPresent # One or more secrets to be used when pulling images imagePullSecrets: [] # - registrySecretName # Annotations to add to the Velero deployment's. Optional. # # If you are using reloader use the following annotation with your VELERO_SECRET_NAME annotations: {} # secret.reloader.stakater.com/reload: \"&lt;VELERO_SECRET_NAME&gt;\" # Labels to add to the Velero deployment's. Optional. labels: {} # Annotations to add to the Velero deployment's pod template. Optional. # # If using kube2iam or kiam, use the following annotation with your AWS_ACCOUNT_ID # and VELERO_ROLE_NAME filled in: podAnnotations: {} # iam.amazonaws.com/role: \"arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:role/&lt;VELERO_ROLE_NAME&gt;\" # Additional pod labels for Velero deployment's template. Optional # ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ podLabels: {} # Resource requests/limits to specify for the Velero deployment. # https://velero.io/docs/v1.6/customize-installation/#customize-resource-requests-and-limits resources: requests: cpu: 500m memory: 128Mi limits: cpu: 1000m memory: 512Mi # Configure the dnsPolicy of the Velero deployment # See: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy dnsPolicy: ClusterFirst # Init containers to add to the Velero deployment's pod spec. At least one plugin provider image is required. # If the value is a string then it is evaluated as a template. initContainers: # - name: velero-plugin-for-csi # image: velero/velero-plugin-for-csi:v0.3.2 # imagePullPolicy: IfNotPresent # volumeMounts: # - mountPath: /target # name: plugins # - name: velero-plugin-for-aws # image: velero/velero-plugin-for-aws:v1.5.2 # imagePullPolicy: IfNotPresent # volumeMounts: # - mountPath: /target # name: plugins # SecurityContext to use for the Velero deployment. Optional. # Set fsGroup for `AWS IAM Roles for Service Accounts` # see more informations at: https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html podSecurityContext: {} # fsGroup: 1337 # Container Level Security Context for the 'velero' container of the Velero deployment. Optional. # See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} # allowPrivilegeEscalation: false # capabilities: # drop: [\"ALL\"] # add: [] # readOnlyRootFilesystem: true # Container Lifecycle Hooks to use for the Velero deployment. Optional. lifecycle: {} # Pod priority class name to use for the Velero deployment. Optional. priorityClassName: \"\" # The number of seconds to allow for graceful termination of the pod. Optional. terminationGracePeriodSeconds: 3600 # Tolerations to use for the Velero deployment. Optional. tolerations: [] # Affinity to use for the Velero deployment. Optional. affinity: {} # Node selector to use for the Velero deployment. Optional. nodeSelector: {} # DNS configuration to use for the Velero deployment. Optional. dnsConfig: {} # Extra volumes for the Velero deployment. Optional. extraVolumes: [] # Extra volumeMounts for the Velero deployment. Optional. extraVolumeMounts: [] # Extra K8s manifests to deploy extraObjects: [] # - apiVersion: secrets-store.csi.x-k8s.io/v1 # kind: SecretProviderClass # metadata: # name: velero-secrets-store # spec: # provider: aws # parameters: # objects: | # - objectName: \"velero\" # objectType: \"secretsmanager\" # jmesPath: # - path: \"access_key\" # objectAlias: \"access_key\" # - path: \"secret_key\" # objectAlias: \"secret_key\" # secretObjects: # - data: # - key: access_key # objectName: client-id # - key: client-secret # objectName: client-secret # secretName: velero-secrets-store # type: Opaque # Settings for Velero's prometheus metrics. Enabled by default. metrics: enabled: true scrapeInterval: 30s scrapeTimeout: 10s # service metdata if metrics are enabled service: annotations: {} labels: {} # Pod annotations for Prometheus podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8085\" prometheus.io/path: \"/metrics\" serviceMonitor: autodetect: true enabled: false annotations: {} additionalLabels: {} # ServiceMonitor namespace. Default to Velero namespace. # namespace: # ServiceMonitor connection scheme. Defaults to HTTP. # scheme: \"\" # ServiceMonitor connection tlsConfig. Defaults to {}. # tlsConfig: {} prometheusRule: enabled: false # Additional labels to add to deployed PrometheusRule additionalLabels: {} # PrometheusRule namespace. Defaults to Velero namespace. # namespace: \"\" # Rules to be deployed spec: [] # - alert: VeleroBackupPartialFailures # annotations: # message: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups. # expr: |- # velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 # for: 15m # labels: # severity: warning # - alert: VeleroBackupFailures # annotations: # message: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed backups. # expr: |- # velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 # for: 15m # labels: # severity: warning kubectl: image: repository: docker.io/bitnami/kubectl # Digest value example: sha256:d238835e151cec91c6a811fe3a89a66d3231d9f64d09e5f3c49552672d271f38. # If used, it will take precedence over the kubectl.image.tag. # digest: # kubectl image tag. If used, it will take precedence over the cluster Kubernetes version. # tag: 1.16.15 # Container Level Security Context for the 'kubectl' container of the crd jobs. Optional. # See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} # Resource requests/limits to specify for the upgrade/cleanup job. Optional resources: {} # Annotations to set for the upgrade/cleanup job. Optional. annotations: {} # Labels to set for the upgrade/cleanup job. Optional. labels: {} # This job upgrades the CRDs. upgradeCRDs: true # This job is meant primarily for cleaning up CRDs on CI systems. # Using this on production systems, especially those that have multiple releases of Velero, will be destructive. cleanUpCRDs: false ## ## End of deployment-related settings. ## ## ## Parameters for the `default` BackupStorageLocation and VolumeSnapshotLocation, ## and additional server settings. ## configuration: # Cloud provider being used (e.g. aws, azure, gcp). provider: # Parameters for the `default` BackupStorageLocation. See # https://velero.io/docs/v1.6/api-types/backupstoragelocation/ backupStorageLocation: # name is the name of the backup storage location where backups should be stored. If a name is not provided, # a backup storage location will be created with the name \"default\". Optional. name: # provider is the name for the backup storage location provider. If omitted # `configuration.provider` will be used instead. provider: # bucket is the name of the bucket to store backups in. Required. bucket: # caCert defines a base64 encoded CA bundle to use when verifying TLS connections to the provider. Optional. caCert: # prefix is the directory under which all Velero data should be stored within the bucket. Optional. prefix: # default indicates this location is the default backup storage location. Optional. default: # accessMode determines if velero can write to this backup storage location. Optional. # default to ReadWrite, ReadOnly is used during migrations and restores. accessMode: ReadWrite # Additional provider-specific configuration. See link above # for details of required/optional fields for your provider. config: {} # region: # s3ForcePathStyle: # s3Url: # kmsKeyId: # resourceGroup: # The ID of the subscription containing the storage account, if different from the clusters subscription. (Azure only) # subscriptionId: # storageAccount: # publicUrl: # Name of the GCP service account to use for this backup storage location. Specify the # service account here if you want to use workload identity instead of providing the key file.(GCP only) # serviceAccount: # Option to skip certificate validation or not if insecureSkipTLSVerify is set to be true, the client side should set the # flag. For Velero client Command like velero backup describe, velero backup logs needs to add the flag --insecure-skip-tls-verify # insecureSkipTLSVerify: # Parameters for the `default` VolumeSnapshotLocation. See # https://velero.io/docs/v1.6/api-types/volumesnapshotlocation/ volumeSnapshotLocation: # name is the name of the volume snapshot location where snapshots are being taken. Required. name: # provider is the name for the volume snapshot provider. If omitted # `configuration.provider` will be used instead. provider: # Additional provider-specific configuration. See link above # for details of required/optional fields for your provider. config: {} # region: # apiTimeout: # resourceGroup: # The ID of the subscription where volume snapshots should be stored, if different from the clusters subscription. If specified, also requires `configuration.volumeSnapshotLocation.config.resourceGroup`to be set. (Azure only) # subscriptionId: # incremental: # snapshotLocation: # project: # These are server-level settings passed as CLI flags to the `velero server` command. Velero # uses default values if they're not passed in, so they only need to be explicitly specified # here if using a non-default value. The `velero server` default values are shown in the # comments below. # -------------------- # `velero server` default: restic uploaderType: # `velero server` default: 1m backupSyncPeriod: # `velero server` default: 4h fsBackupTimeout: # `velero server` default: 30 clientBurst: # `velero server` default: 500 clientPageSize: # `velero server` default: 20.0 clientQPS: # Name of the default backup storage location. Default: default defaultBackupStorageLocation: # How long to wait by default before backups can be garbage collected. Default: 72h defaultBackupTTL: # Name of the default volume snapshot location. defaultVolumeSnapshotLocations: # `velero server` default: empty disableControllers: # `velero server` default: 1h garbageCollectionFrequency: # Set log-format for Velero pod. Default: text. Other option: json. logFormat: # Set log-level for Velero pod. Default: info. Other options: debug, warning, error, fatal, panic. logLevel: # The address to expose prometheus metrics. Default: :8085 metricsAddress: # Directory containing Velero plugins. Default: /plugins pluginDir: # The address to expose the pprof profiler. Default: localhost:6060 profilerAddress: # `velero server` default: false restoreOnlyMode: # `velero server` default: customresourcedefinitions,namespaces,storageclasses,volumesnapshotclass.snapshot.storage.k8s.io,volumesnapshotcontents.snapshot.storage.k8s.io,volumesnapshots.snapshot.storage.k8s.io,persistentvolumes,persistentvolumeclaims,secrets,configmaps,serviceaccounts,limitranges,pods,replicasets.apps,clusterclasses.cluster.x-k8s.io,clusters.cluster.x-k8s.io,clusterresourcesets.addons.cluster.x-k8s.io restoreResourcePriorities: # `velero server` default: 1m storeValidationFrequency: # How long to wait on persistent volumes and namespaces to terminate during a restore before timing out. Default: 10m terminatingResourceTimeout: # Comma separated list of velero feature flags. default: empty # features: EnableCSI features: # `velero server` default: velero namespace: # additional key/value pairs to be used as environment variables such as \"AWS_CLUSTER_NAME: 'yourcluster.domain.tld'\" extraEnvVars: {} # Set true for backup all pod volumes without having to apply annotation on the pod when used file system backup Default: false. defaultVolumesToFsBackup: # How often repository maintain is run for repositories by default. defaultRepoMaintainFrequency: ## ## End of backup/snapshot location settings. ## ## ## Settings for additional Velero resources. ## rbac: # Whether to create the Velero role and role binding to give all permissions to the namespace to Velero. create: true # Whether to create the cluster role binding to give administrator permissions to Velero clusterAdministrator: true # Name of the ClusterRole. clusterAdministratorName: cluster-admin # Information about the Kubernetes service account Velero uses. serviceAccount: server: create: true name: annotations: labels: # Info about the secret to be used by the Velero deployment, which # should contain credentials for the cloud provider IAM account you've # set up for Velero. credentials: # Whether a secret should be used. Set to false if, for examples: # - using kube2iam or kiam to provide AWS IAM credentials instead of providing the key file. (AWS only) # - using workload identity instead of providing the key file. (GCP only) useSecret: true # Name of the secret to create if `useSecret` is true and `existingSecret` is empty name: # Name of a pre-existing secret (if any) in the Velero namespace # that should be used to get IAM account credentials. Optional. existingSecret: # Data to be stored in the Velero secret, if `useSecret` is true and `existingSecret` is empty. # As of the current Velero release, Velero only uses one secret key/value at a time. # The key must be named `cloud`, and the value corresponds to the entire content of your IAM credentials file. # Note that the format will be different for different providers, please check their documentation. # Here is a list of documentation for plugins maintained by the Velero team: # [AWS] https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/README.md # [GCP] https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/main/README.md # [Azure] https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/main/README.md secretContents: {} # cloud: | # [default] # aws_access_key_id=&lt;REDACTED&gt; # aws_secret_access_key=&lt;REDACTED&gt; # additional key/value pairs to be used as environment variables such as \"DIGITALOCEAN_TOKEN: &lt;your-key&gt;\". Values will be stored in the secret. extraEnvVars: {} # Name of a pre-existing secret (if any) in the Velero namespace # that will be used to load environment variables into velero and node-agent. # Secret should be in format - https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables extraSecretRef: \"\" # Whether to create backupstoragelocation crd, if false =&gt; do not create a default backup location backupsEnabled: true # Whether to create volumesnapshotlocation crd, if false =&gt; disable snapshot feature snapshotsEnabled: true # Whether to deploy the node-agent daemonset. deployNodeAgent: false nodeAgent: podVolumePath: /var/lib/kubelet/pods privileged: false # Pod priority class name to use for the node-agent daemonset. Optional. priorityClassName: \"\" # Resource requests/limits to specify for the node-agent daemonset deployment. Optional. # https://velero.io/docs/v1.6/customize-installation/#customize-resource-requests-and-limits resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1024Mi # Tolerations to use for the node-agent daemonset. Optional. tolerations: [] # Annotations to set for the node-agent daemonset. Optional. annotations: {} # labels to set for the node-agent daemonset. Optional. labels: {} # will map /scratch to emptyDir. Set to false and specify your own volume # via extraVolumes and extraVolumeMounts that maps to /scratch # if you don't want to use emptyDir. useScratchEmptyDir: true # Extra volumes for the node-agent daemonset. Optional. extraVolumes: [] # Extra volumeMounts for the node-agent daemonset. Optional. extraVolumeMounts: [] # Key/value pairs to be used as environment variables for the node-agent daemonset. Optional. extraEnvVars: {} # Configure the dnsPolicy of the node-agent daemonset # See: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy dnsPolicy: ClusterFirst # SecurityContext to use for the Velero deployment. Optional. # Set fsGroup for `AWS IAM Roles for Service Accounts` # see more informations at: https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html podSecurityContext: runAsUser: 0 # fsGroup: 1337 # Container Level Security Context for the 'node-agent' container of the node-agent daemonset. Optional. # See: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container containerSecurityContext: {} # Container Lifecycle Hooks to use for the node-agent daemonset. Optional. lifecycle: {} # Node selector to use for the node-agent daemonset. Optional. nodeSelector: {} # Affinity to use with node-agent daemonset. Optional. affinity: {} # DNS configuration to use for the node-agent daemonset. Optional. dnsConfig: {} # Backup schedules to create. # Eg: # schedules: # mybackup: # disabled: false # labels: # myenv: foo # annotations: # myenv: foo # schedule: \"0 0 * * *\" # useOwnerReferencesInBackup: false # template: # ttl: \"240h\" # includedNamespaces: # - foo schedules: {} # Velero ConfigMaps. # Eg: # configMaps: # fs-restore-action-config: # labels: # velero.io/plugin-config: \"\" # velero.io/pod-volume-restore: RestoreItemAction # data: # image: velero/velero-restore-helper:v1.10.0 configMaps: {} ## ## End of additional Velero resource settings. ## . Autogenerated from Helm Chart and git history using helm-changelog . ",
    "url": "/changelogs/vmware-tanzu/helm-charts/velero#312",
    
    "relUrl": "/vmware-tanzu/helm-charts/velero#312"
  }
}
